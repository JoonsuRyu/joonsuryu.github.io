<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-03T14:39:54+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">$n$-step Bootstrapping</title><link href="http://localhost:4000/studies/n-step-bootstrapping/" rel="alternate" type="text/html" title="$n$-step Bootstrapping" /><published>2022-04-06T00:00:00+09:00</published><updated>2022-04-06T00:00:00+09:00</updated><id>http://localhost:4000/studies/n-step-bootstrapping</id><content type="html" xml:base="http://localhost:4000/studies/n-step-bootstrapping/">&lt;p&gt;이번 장에서는 5장에서 배운 Monte Carlo Method과 6장에서 배운 Temporal Difference (TD)를 융합하여 만든 새로운 방법을 소개합니다. Monte Carlo Method는 항상 Episode가 끝난 후에야 학습이 가능했고, TD는 1단계만 관찰하면 학습이 가능했습니다. 그렇다면 그 사이의 단계인 $n$번째 단계까지 관찰한 다음 학습을 하게 되면 조금 더 일반화된 학습이 가능하지 않을까하는 아이디어가 떠오르게 됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 $n$개의 시간 단계 동안 관찰한 후 학습에 반영하는 것을 &lt;span style=&quot;color:red&quot;&gt;$n$-step Bootstrapping&lt;/span&gt;이라고 합니다. TD은 경험이 즉각적으로 학습에 반영되지만, 때때로 조금 더 장기적인 관점에서 바라봐야하는 문제가 있습니다. 이런 관점에서 $n$-step Bootstrapping은 12장에서 배울 Eligibility Traces의 기반이 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장 역시 이전 장들과 마찬가지로, 먼저 Prediction을 알아본 다음에 Control을 다루는 순서로 진행됩니다. 즉, 먼저 $n$-step Bootstrapping으로 $v_{\pi}$를 추정한 후, Optimal Policy를 찾기 위한 Control 방법을 논의할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-td-prediction&quot;&gt;$n$-step TD Prediction&lt;/h2&gt;

&lt;p&gt;Policy $\pi$를 사용하여 생성된 Sample Episode에서 $v_{\pi}$를 추정할 때, Monte Carlo Method는 Episode가 끝날 때까지 해당 State부터 관찰된 전체 Reward의 합인 Return을 기반으로 업데이트하고, 1-step TD는 1개의 Reward만 관찰한 후 업데이트합니다. 이번에는 이 두 극단적인 방법의 중간점으로 $n$개의 Reward를 관찰한 후 업데이트를 수행하는 $n$-step TD에 대해 알아보겠습니다. 1-step TD, $n$-step TD, 그리고 Monte Carlo Method의 차이는 아래의 Backup Diagram을 보시면 쉽게 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step 업데이트를 1-step TD와 마찬가지로 TD라고 부르는 이유는, 1-step TD처럼 이후의 추정이 어떻게 달라지는지에 따라 이전의 추정이 변하기 때문입니다. 다만 그 추정이 1-step 후가 아니라 $n$-step 후일 뿐입니다.&lt;/p&gt;

&lt;p&gt;이들간의 차이를 수식으로 비교해봅시다. 먼저 Monte Carlo Method에서 Return $G_t$를 계산하는 식은 다음과 같았습니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T\]

&lt;p&gt;위 식에서 $T$는 Episode의 마지막 시간 단계입니다. Monte Carlo Method에서는 Return을 각 시간 단계마다 받는 Reward에 Discount $\gamma$를 곱한 값의 합으로 계산합니다.&lt;/p&gt;

\[G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1})\]

&lt;p&gt;반면에 1-step TD에서는 첫 번째 Reward와, 다음 State의 추정 Value에 Discount를 곱한 값의 합으로 계산됩니다. $V_t$는 시간 단계 $t$에서 추정한 $v_{\pi}$를 의미하며, $G_{t:t+1}$는 시간 단계 $t$부터 $t+1$까지 얻은 수익을 의미합니다. 이 개념을 확장하면 2-step의 Return은 다음과 같음을 알 수 있습니다.&lt;/p&gt;

\[G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2})\]

&lt;p&gt;위와 같은 방법으로 $n$-step의 Return을 만들면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}) \tag{7.1}\]

&lt;p&gt;위 식에서 $n \ge 1$이고 $0 \le t &amp;lt; T - n$이라는 조건이 있습니다. 만약 $t + n \ge T$라면 $n$-step이 끝나기 전에 Episode가 끝난다는 이야기이므로 $T$ 이후의 항은 모두 0으로 처리하면 됩니다.&lt;/p&gt;

&lt;p&gt;1-step에서 시간 단계 $t+1$에 도달해야 $t$에서의 Value Function을 업데이트 할 수 있던 것처럼, $n$-step 또한 시간 단계 $t+n$에 도달해야만 $t$에서의 Value Function을 업데이트 할 수 있습니다. 그렇기 때문에 $n$-step에서 가장 처음 학습을 하는 시간 단계는 $t+n$이 됩니다. 이 점을 반영하여 $V$의 업데이트 식을 정의하면 다음과 같습니다.&lt;/p&gt;

\[V_{t+n} (S_t) \doteq V_{t+n-1} (S_t) + \alpha \left[ G_{t:t+n} - V_{t+n-1} (S_t) \right], \quad 0 \le t &amp;lt; T \tag{7.2}\]

&lt;p&gt;이 때, $S_t$ 이외의 State에서는 Value Function이 변하지 않습니다. 즉, 모든 $s \ne S_t$에 대해서 $V_{t+n} (s) = V_{t+n-1} (s)$입니다. 이것을 $n$-step TD라고 부릅니다. $n$-step TD의 모든 Episode에서는 처음 $n-1$ 시간 단계까지는 아무것도 변하지 않는데, 이를 보완하기 위해 각 Episode가 끝난 후 다음 Episode가 시작되기 전에 동일한 수의 추가적인 업데이트가 이루어집니다. 완전한 Pseudocode를 보시면 이것이 어떤 의미인지 이해가 되실 겁니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Return은 Value Function $V_{t+n-1}$를 사용하여 $R_{t+n}$ 이후에 누락된 Reward를 보완합니다. $n$-step Return의 장점은 최악의 상황에서 $V_{t+n-1}$보다 $v_{\pi}$ 추정값이 더 낫다는 것입니다. 다시말해, $n$-step Return에서 가장 큰 오차는 $V_{t+n-1}$의 가장 큰 오차보다 $\gamma^n$ 배 만큼 작거나 같습니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\max_s \left| \mathbb{E}_{\pi} \left[ G_{t:t+n} | S_t = s \right] - v_{\pi} (s) \right| \le \gamma^n \max_s \left| V_{t+n-1} (s) - v_{\pi} (s) \right| \tag{7.3}\]

&lt;p&gt;식 (7.3)은 모든 $n \ge 1$에 대해 성립합니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Error Reduction Property of $n$-step Returns&lt;/span&gt;라고 합니다. 이 수식 덕분에 모든 $n$-step TD 방법이 적절한 조건 하에 올바른 추정값으로 수렴한다는 것이 보장됩니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-sarsa&quot;&gt;$n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;이번에는 $n$-step과 Sarsa를 결합한 Control을 배우도록 하겠습니다. 새로 배우는 Sarsa와 구분하기 위해, 이전 장에서 배운 Sarsa를 1-step Sarsa, 또는 Sarsa(0)으로 표기하고, 이번 장에서 배우는 새로운 방법은 &lt;span style=&quot;color:red&quot;&gt;$n$-step Sarsa&lt;/span&gt;로 부르겠습니다. $n$-step Sarsa의 기본 개념은 아래의 Backup Diagram과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 기반으로 $n$-step Return을 재정의하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} \gamma^n Q_{t+n-1} (S_{t+n}, S_{t+n}), \quad n \ge 1, 0 \le t &amp;lt; T-n \tag{7.4}\]

&lt;p&gt;물론 이전 Section에서 배운 대로 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$입니다. 식 (7.4)를 알고리즘에 맞게 수정하면 다음과 같습니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right] \quad 0 \le t &amp;lt; T \tag{7.5}\]

&lt;p&gt;$n$-step Return처럼 식 (7.5)도 모든 $s \ne S_t$, $a \ne A_t$에 대해 $Q_{t+n} (s, a) = Q_{t+n-1}$입니다. 즉, 학습하고 있는 State-Action 쌍을 제외하고는 $Q$ 값이 변하지 않습니다. 그렇기 때문에 $n$-step Sarsa라고 부르는 것입니다. $n$-step Sarsa의 완전한 Pseudocode은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Sarsa의 장점은 1-step Sarsa보다 학습 속도가 빠르다는 점입니다. 아래 그림은 Grid World에서의 예시를 보여주고 있습니다. 첫 번째 그림과 같은 Episode에 대해, 1-step Sarsa는 가운데 그림처럼 마지막 State-Action 쌍에 대해서만 $Q$ 값의 업데이트가 일어납니다. 하지만 세 번째 그림을 보시면 10-step Sarsa는 Episode 뒤 10개의 State-Action 쌍이 모두 업데이트가 되는 장점이 있습니다. 즉, 하나의 Episode에서 더 많은 것을 학습할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step과 Expected Sarsa를 결합하는 방법도 동일합니다. 이에 대한 Backup Diagram은 $n$-step Sarsa의 Backup Diagram 맨 오른쪽에 나타나 있습니다. 주의할 점은 시간 단계 $t$부터 $t+n$까지 모두 평균값을 사용하는 것이 아니라, 마지막 단계에서만 평균값을 사용합니다. &lt;span style=&quot;color:red&quot;&gt;$n$-step Expected Sarsa&lt;/span&gt;의 Return 식은 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \bar{V}_{t+n-1} (S_{t+n}) \quad t+n &amp;lt; T \tag{7.7}\]

&lt;p&gt;마찬가지로 $t + n \ge T$인 경우에는 $G_{t:t+n} \doteq G_t$입니다. 또한 $\bar{V}_t (s)$는 State $s$의 &lt;strong&gt;Expected Approximate Value&lt;/strong&gt;이고, 다음과 같이 정의됩니다.&lt;/p&gt;

\[\bar{V}_t (s) \doteq \sum_a \pi (a|s) Q_t (s, a) \quad \text{for all } s \in \mathcal{S} \tag{7.8}\]

&lt;p&gt;식 (7.8)은 지금 뿐만이 아니라 다른 장에서도 사용되기 때문에 기억해두시는 것이 좋습니다. 또한 식 (7.8)에서 만약 State $s$가 마지막 State라면 $\bar{V}_t (s) $는 0으로 정의됩니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-off-policy-learning&quot;&gt;$n$-step Off-policy Learning&lt;/h2&gt;

&lt;p&gt;이번에는 Off-policy 학습의 $n$-step 버전을 배워보겠습니다. $n$-step 방법에서는 Return이 $n$ 단계에 걸쳐 생성됩니다. Off-policy에서 중요한 점은 Target Policy $\pi$와 Behavior Policy $b$가 구분되는 것인데, 이 때 두 Policy 간의 차이를 보정하기 위해 &lt;strong&gt;Importance Sampling&lt;/strong&gt;을 사용하였습니다. 그렇다면 $n$-step 버전에서 두 Policy 간의 Weight를 어떻게 처리하는지 다음 식을 통해 살펴보겠습니다.&lt;/p&gt;

\[V_{t+n} (S_t) \doteq V_{t+n-1} (S_t) + \alpha \rho_{t:t+n-1} \bigg[ G_{t:t+n} - V_{t+n-1} (S_t) \bigg] \quad 0 \le t &amp;lt; T \tag{7.9}\]

&lt;p&gt;식 (7.9)에서 $\rho_{t:t+n-1}$가 바로 Importance Sampling Ratio입니다. 5장에서 배운대로 Importance Sampling Ratio는 다음과 같이 계산합니다.&lt;/p&gt;

\[\rho_{t:h} \doteq \prod_{k=t}^{\min (h,T-1)} \frac{\pi (A_k | S_k)}{b (A_k | S_k)} \tag{7.10}\]

&lt;p&gt;만약 Policy $\pi$에 의해 선택되지 않는 Action의 경우(즉, $\pi (A_k \mid S_k) = 0$)에는 $n$-step Return에 Weight를 0으로 주고 완전히 무시해야 합니다. 반대로 Policy $\pi$가 Policy $b$보다 더 높은 확률로 선택되는 Action의 경우에는 Weight가 증가합니다. 만약 두 Policy $\pi$와 $b$가 동일한 경우 Importance Sampling Ratio는 정확히 1이 됩니다. 따라서 식 (7.9)와 같은 업데이트 식이 유도된 것입니다.&lt;/p&gt;

&lt;p&gt;이와 같은 방법으로, $n$-step Sarsa 또한 다음과 같이 Off-policy 방법으로 대체할 수 있습니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \rho_{t+1:t+n} \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right] \tag{7.11}\]

&lt;p&gt;여기서 Importance Sampling Ratio는 식 (7.9)의 $n$-step TD 보다 한 단계 늦게 시작하고 끝납니다. 왜냐면 Sarsa는 Q-learning과 달리 다음 Action을 선택한 후에 학습을 하기 때문입니다. 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Expected Sarsa의 Off-policy 버전은 Importance Sampling Ratio만 약간 변형시키면 됩니다. $\rho_{t+1:t+n-1}$ 대신 $\rho_{t+1:t+n}$로 바꾼 다음 나머지는 동일합니다. (물론 Return 식은 식 (7.7)을 사용합니다)&lt;/p&gt;

&lt;h2 id=&quot;per-decision-methods-with-control-variates&quot;&gt;Per-decision Methods with Control Variates&lt;/h2&gt;

&lt;p&gt;이전 Section에서 제시한 $n$-step Off-policy 방법은 간단하고 개념적으로 명확하지만, 그렇게 효율적인 방법은 아닙니다. 보다 정교하게 접근하는 방법은 Section 5.9에서 잠깐 소개한 Per-decision Importance Sampling을 사용해야 합니다. 이 접근 방식은 식 (7.1)과 같은 $n$-step Return을 다음과 같은 Recursive Form으로부터 시작합니다.&lt;/p&gt;

\[G_{t:h} = R_{t+1} + \gamma G_{t+1:h} \quad t&amp;lt;h&amp;lt;T \tag{7.12}\]

&lt;p&gt;이 때, $G_{h:h} \doteq V_{h-1}(S_h)$입니다. Importance Sampling Ratio를 고려할 때, 만약 Policy $\pi$에서 시간 $t$에 방문하지 않는 Action에 대해서 $\rho_t$는 0입니다. 이렇게 계산하면 $n$-step Return 또한 0이되고, Target Policy에 대해 Variance가 높아지는 문제가 있습니다. 그렇기 때문에 여기서는 이보다 정교한 새로운 방법을 제안하며, Off-policy의 $n$-step Return을 다음과 같이 재정의합니다.&lt;/p&gt;

\[G_{t:h} \doteq \rho_t (R_{t+1} + \gamma G_{t+1:h}) + (1 - \rho_t) V_{h-1} (S_t) \quad t&amp;lt;h&amp;lt;T \tag{7.13}\]

&lt;p&gt;식 (7.12)와 동일하게 $G_{h:h} \doteq V_{h-1}(S_h)$입니다. 식 (7.13)은 기존과 다르게 $\rho_t$가 0이라고 할지라도 Return이 0이 되지 않는 대신 기존 추정치인 $V_{h-1} (S_t)$와 동일해집니다. Importance Sampling Ratio가 0이라는 뜻은 Sample을 무시하라는 뜻이므로 추정치를 변경하지 않게 바꾼 것입니다. 이 때, 식 (7.13)에서 오른쪽 두 번째 항을 &lt;span style=&quot;color:red&quot;&gt;Control Variate&lt;/span&gt;라고 합니다. Importance Sampling Ratio의 기대값이 1이라면 추정값과 상관이 없으므로 Control Variate의 기대값은 0이 됩니다. 또한 식 (7.13)과 같은 Off-policy의 $n$-step Return 정의는 식 (7.1)과 같은 On-policy Return 정의를 엄격하게 일반화했다고 볼 수 있습니다. 실제로 식 (7.13)에서 $\rho_t$를 항상 1이라고 가정하면 식 (7.1)과 동일합니다. 식 (7.13)의 업데이트 식은 식 (7.2)를 그대로 사용하면 됩니다.&lt;/p&gt;

&lt;p&gt;Action-Value 버전에서는 첫 번째 Action이 Importance Sampling에서 영향을 끼치지 않기 때문에 $n$-step Return의 Off-policy 정의는 약간 다릅니다. 식 (7.7)과 같이 Expectation Form으로도, 식 (7.12)와 같은 Recursive Form으로도 표현할 수 있기 때문에 두 가지 표현 방법을 모두 보여드리도록 하겠습니다. 다음 식은 Control Variate를 포함하였습니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp;\doteq R_{t+1} + \gamma \left( \rho_{t+1} G_{t+1:h} + \bar{V}_{h-1} (S_{t+1}) - \rho_{t+1} Q_{h-1} (S_{t+1}, A_{t+1}) \right) \\ \\
&amp;amp;= R_{t+1} + \gamma \rho_{t+1} \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1}), t &amp;lt; h \le T \tag{7.14}
\end{align}\]

&lt;p&gt;Recursive Form에서 $h &amp;lt; T$인 경우 $G_{h:h} \doteq Q_{h-1} (S_h, A_h)$로, $h \ge T$라면 $G_{T-1:h} \doteq R_T$로 끝납니다. 식 (7.5)와 결합하여 추정 알고리즘을 만들면 Expected Sarsa와 유사한 형태가 나옵니다.&lt;/p&gt;

&lt;p&gt;지금까지 사용한 Importance Sampling은 Off-policy 학습을 가능하게 하지만 높은 Variance를 유발하기 때문에 Step-size parameter를 작게 설정해야 합니다. 다만 이로 인해 학습 속도는 느려질 수밖에 없습니다. 즉, Off-policy 방법은 On-policy 방법보다 학습 속도가 느립니다. 물론 이를 개선하기 위한 여러 연구가 진행되고 있습니다. 이번 Section에서 다룬 Control Variate가 그 예 중 하나이며, 이 외에도 &lt;strong&gt;Autostep&lt;/strong&gt; (Mahmood, Sutton, Degris and Pilarski, 2012), &lt;strong&gt;Tian&lt;/strong&gt; (Karampatziakis and Langford, 2010), &lt;strong&gt;Mahmood&lt;/strong&gt;(2017; Mahmood and Sutton, 2015) 등이 방법이 제안되었습니다. 다음 Section에서는 또 다른 방법인 Importance Sampling을 사용하지 않는 Off-policy 학습 방법에 대해 다루어 보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;off-policy-learning-without-importance-sampling--the-n-step-tree-backup-algorithm&quot;&gt;Off-policy Learning Without Importance Sampling : The n-step Tree Backup Algorithm&lt;/h2&gt;

&lt;p&gt;6장에서 1-step  Q-learning과 Expected Sarsa를 배울 때 Importance Sampling을 사용하지 않는 방법에 대해 배웠습니다. 이를 $n$-step으로 확장한 방법으로 Tree-backup Algorithm이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tree-backup Algorithm의 기본 아이디어는 위의 Backup Diagram과 같습니다. Diagram의 각 단계에는 3가지 Sample State와 Reward, 그리고 2개의 Sample Action이 있습니다. 이것은 초기 State, Action 쌍인 $(S_t, A_t)$ 이후에 발생하는 이벤트를 나타냅니다. 각 State에서 가운데 Action을 제외한 나머지 Action은 선택되지 않은 Action입니다. 선택되지 않은 Action은 Sample 데이터가 없기 때문에 Bootstrap하고 Target Policy의 추정값을 업데이트하는데 사용합니다. 이것이 Backup Diagram처럼 나무 모양과 비슷하기 때문에 &lt;span style=&quot;color:red&quot;&gt;Tree-backup&lt;/span&gt;이라고 불립니다.&lt;/p&gt;

&lt;p&gt;Tree-backup의 업데이트는 Tree의 Leaf Node로부터 추정된 Action-Value로부터 시작합니다. 각 Leaf Node는 Target Policy $\pi$로부터 선택될 확률에 비례하는 Weight를 기반으로 Target에 기여합니다. 따라서 첫 번째 Level의 Action은 $\pi (a \mid S_{t+1})$의 Weight로 기여하지만, 실제로 취한 Action $A_{t+1}$은 전혀 기여하지 않습니다. $\pi (A_{t+1} \mid S_{t+1})$은 두 번째 Level의 Action Value에 Weight를 부여하는데 사용됩니다. 즉, 선택되지 않은 두 번째 Level Action의 Weight는 $\pi (A_{t+1} \mid S_{t+1}) \pi (a’ \mid S_{t+2})$가 됩니다. 마찬가지로 세 번째 Level Action의 Weight는 $\pi (A_{t+1} \mid S_{t+1}) \pi (a’ \mid S_{t+2}) \pi (a’’ \mid S_{t+3})$이 됩니다. 쉽게 말해, Backup Diagram에서 각 Leaf Node가 의미하는 것은 Root State로부터 주어진 Policy $\pi$ 하에 해당 Action이 선택될 확률을 의미합니다.&lt;/p&gt;

&lt;p&gt;3-step Tree-backup 업데이트는 각 State별로 가능한 모든 Action을 고려하는 과정과, 주어진 Policy에 따라 Action을 수행하는 과정이 포함되기 때문에 6개의 단계로 나눌 수 있습니다. 이것을 일반적인 $n$-step Tree-backup Algorithm으로 만들어보겠습니다. 먼저, 1-step의 Return은 다음과 같이 Expected Sarsa와 같은 모양으로 표현할 수 있습니다.&lt;/p&gt;

\[G_{t:t+1} \doteq R_{t+1} + \gamma \sum_a \pi(a | S_{t+1}) Q_t (S_{t+1}, a), \quad t &amp;lt; T-1 \tag{7.15}\]

&lt;p&gt;같은 방식으로 2-step Tree-backup의 Return은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
G_{t:t+2} &amp;amp; \doteq R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+1} (S_{t+1}, a) \\ \\
&amp;amp;+ \gamma \pi (A_{t+1} | S_{t+1}) \left( R_{t+2} + \gamma \sum_a \pi (a | S_{t+2}) Q_{t+1} (S_{t+2}, a) \right) \\ \\
&amp;amp;= R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+1}(S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:t+2}, \quad t &amp;lt; T-2
\end{align}\]

&lt;p&gt;이를 반복하여 일반적인 $n$-step Tree-backup Return의 Recursive Form은 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+n-1} (S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:t+n}   \tag{7.16}\]

&lt;p&gt;단, $t &amp;lt; T - 1, n \ge 2$ 입니다. 만약 $n = 1$인 경우 식 (7.15)와 같으며, 예외적으로 $G_{T-1:t+n} \doteq R_T$입니다. 이것은 $n$-step Sarsa에서 다음과 같이 업데이트됩니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right], \quad - \ge t &amp;lt; T\]

&lt;p&gt;물론 학습에 사용되지 않는 모든 State $s \ne S_t$, 모든 Action $a \ne A_t$에 대해서는 Q 값이 변하지 않습니다. (즉, $Q_{t+n} (s, a) = Q_{t+n-1} (s, a)$) 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-unifying-algorithm-n-step-qsigma&quot;&gt;A Unifying Algorithm: $n$-step $Q(\sigma)$&lt;/h2&gt;

&lt;p&gt;지금까지 우리는 $n$-step Sarsa, $n$-step Tree-backup, 그리고 $n$-step Expected Sarsa에 대해 공부했습니다. 이 셋의 가장 큰 차이는 Sample로 인한 Importance Sampling의 여부입니다. $n$-step Sarsa는 매 단계마다 Importance Sampling Ratio를 보정해주어야 하고, $n$-step Expected Sarsa 또한 마지막을 제외한 모든 단계에서 Importance Sampling Ratio 보정이 필요합니다. $n$-step Tree-backup 알고리즘은 Importance Sampling이 필요없는 것이 특징입니다. 이번 Section에서는 이 3개의 알고리즘을 통합하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 위의 그림에서 네 번째 Backup Diagram과 같습니다. 매 단계 Sarsa 처럼 처리할 것인지, 아니면 Tree-backup 처럼 처리할 것인지의 여부를 따로 정하는 것입니다. Expected Sarsa 처럼 처리하기 위해서는 마지막 단계를 Tree-backup 처럼 처리하면 됩니다.&lt;/p&gt;

&lt;p&gt;단계 별로 어떻게 처리할 것인지는 $\sigma$의 값에 따라 달라집니다. $t$ 단계에서의 $\sigma$ 값을 $\sigma_t \in [0, 1]$라고 표기합니다. 만약  $\sigma = 1$인 경우 Sarsa와 동일해지고(=Full Sampling), $\sigma = 0$인 경우 Tree-backup과 동일해집니다(=Pure Expectation). 이 새로운 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;$n$-step $Q(\sigma)$&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;이제 $n$-step $Q(\sigma)$를 수식으로 표현해보겠습니다. 식 (7.16)의 Tree-backup $n$-step Return을 horizon $h = t + n$과 식 (7.8)의 $\bar{V}$로 표현합니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp;= R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{h-1} (S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:h} \\ \\
&amp;amp;= R_{t+1} + \gamma \bar{V}_{h-1} (S_{t+1}) - \gamma \pi (A_{t+1} | S_{t+1}) Q_{h-1}(S_{t+1}, A_{t+1}) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:h} \\ \\
&amp;amp;= R_{t+1} + \gamma \pi (A_{t+1} | S_{t+1}) \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1})
\end{align}\]

&lt;p&gt;그 이후로는 Importance Sampling Ratio $\rho_{t+1}$을 $\pi (A_{t+1} \mid S_{t+1})$로 대체한 것을 제외하고는 식 (7.14)와 같이 Control Variate가 있는 $n$-step Sarsa와 동일합니다. $Q(\sigma)$는 다음과 같이 표현할 수도 있습니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp; \doteq R_{t+1} + \gamma \left( \sigma_{t+1} \rho_{t+1} + (1 - \sigma_{t+1}) \pi (A_{t+1} | S_{t+1}) \right) \\ \\
&amp;amp;\times \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1}) \tag{7.17}
\end{align}\]

&lt;p&gt;이 때 $t &amp;lt; h \le T$ 입니다. Recursive Form에서 $h &amp;lt; T$인 경우 $G_{h:h} \doteq Q_{h-1} (S_h, A_h)$가 되고, $h = T$인 경우 $G_{T-1:T} \doteq R_T$가 됩니다. 그 후 식 (7.11) 대신 Importance Sampling Ratio가 없는 식 (7.5)를 사용하여 $n$-step Sarsa 업데이트를 사용합니다. Importance Sampling Ratio 자체가 $n$-step Return에 포함되기 때문입니다. $Q(\sigma)$의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 1-step TD와 Monte Carlo Method의 중간으로 볼 수 있는 &lt;strong&gt;$n$-step TD&lt;/strong&gt;에 대해 배웠습니다. 이렇게 극단적인 두 방법을 적절히 조절하여 중간 정도의 방법을 사용하는 것은 때때로 좋은 성능을 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Section 7.6에서 사용했던 그림을 다시 가져와보면 이번 장에서 배운 내용이 요약되어 있습니다. &lt;strong&gt;$n$-step Sarsa&lt;/strong&gt;나 &lt;strong&gt;Expected Sarsa&lt;/strong&gt;는 기본적으로 Return을 계산할 때 Importance Sampling Ratio를 반드시 고려해야하며, 그 방법을 회피하기 위해 &lt;strong&gt;$n$-step Tree-backup&lt;/strong&gt;을 고안하였습니다. 마지막으로는 이것들을 일반화할 수 있는 &lt;strong&gt;$n$-step $Q(\sigma)$&lt;/strong&gt;를 제안하였습니다.&lt;/p&gt;

&lt;p&gt;이러한 $n$-step 방법의 단점은 지난 장에서 배운 1-step 방법보다 각 시간 단계당 더 많은 계산이 필요할 뿐만 아니라 더 많은 메모리가 필요하다는 것입니다. 12장에서는 이 단점을 최소한으로 줄여 $n$-step TD를 구현하지만, 아무리 줄여도 항상 1-step보다는 계산량과 필요 메모리량이 많다는 한계가 있습니다. 하지만 이러한 단점을 감안하더라도 $n$-step 방법이 가지는 장점이(대표적으로 빠른 학습) 있기 때문에 고려할만한 가치는 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy $n$-step은 추후 배울 Eligibility Traces보다 복잡하지만 개념적으로 명확하다는 장점이 있습니다. 이 장에서는 Off-policy $n$-step TD에 대해 2가지 방법으로 접근하였습니다. 첫 번째 방법은 기존에 배운 &lt;strong&gt;Importance Sampling&lt;/strong&gt;을 이용하는 방법이었습니다. 이것은 개념적으로 간단하지만 Variance가 크다는 단점이 있습니다. 따라서 만약 Target Policy와 Behavior Policy가 매우 큰 차이가 나는 경우라면 이 방법은 적합하지 않습니다. 두 번째 방법으로 &lt;strong&gt;Tree-backup&lt;/strong&gt; 방법을 제안하였습니다. 이것은 Stochastic Target Policy를 갖는 Q-learning을 $n$-step으로 확장한 개념입니다. Importance Sampling을 포함하지 않지만, Target Policy와 Behavior Policy의 차이가 클 경우 $n$이 크더라도 Bootstrapping이 간단하다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;7장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서는 5장에서 배운 Monte Carlo Method과 6장에서 배운 Temporal Difference (TD)를 융합하여 만든 새로운 방법을 소개합니다. Monte Carlo Method는 항상 Episode가 끝난 후에야 학습이 가능했고, TD는 1단계만 관찰하면 학습이 가능했습니다. 그렇다면 그 사이의 단계인 $n$번째 단계까지 관찰한 다음 학습을 하게 되면 조금 더 일반화된 학습이 가능하지 않을까하는 아이디어가 떠오르게 됩니다.</summary></entry><entry><title type="html">Temporal-Difference Learning</title><link href="http://localhost:4000/studies/temporal-difference-learning/" rel="alternate" type="text/html" title="Temporal-Difference Learning" /><published>2022-03-30T00:00:00+09:00</published><updated>2022-03-30T00:00:00+09:00</updated><id>http://localhost:4000/studies/temporal-difference-learning</id><content type="html" xml:base="http://localhost:4000/studies/temporal-difference-learning/">&lt;p&gt;이번 장은 강화학습의 핵심 아이디어인 &lt;span style=&quot;color:red&quot;&gt;Temporal-Difference (TD) Learning&lt;/span&gt;을 다루게 됩니다. TD Learning은 Environment에 대한 정확한 Model 없이 경험을 통해 학습한다는 Monte Carlo의 아이디어와 Bootstrap 하지 않고 학습된 다른 추정치를 기반으로 추정치를 업데이트한다는 Dynamic Programming 아이디어를 결합하여 만들어졌습니다. 이번 장의 시작은 이전 장들과 같이 주어진 Policy $\pi$에 대한 Value Function $v_{\pi}$를 추정하는 문제로부터 시작하며, Optimal Policy을 찾는 Control 문제에서도 이전과 같이 GPI를 변형하여 접근합니다. 이전 장들과의 주요 차이점은 Prediction 문제에 대한 접근 방식입니다.&lt;/p&gt;

&lt;p&gt;이번 장 이후로 나오는 대부분의 주제는 Dynamic Programming, Monte Carlo, TD Learning과 관련이 있으며, 특히 7장에서는 TD Learning과 Monte Carlo 방법의 중간 개념임 $n$-step Bootstrapping을 소개하고, 12장에서는 이것들을 매끄럽게 통합할 수 있는 TD($\lambda$)를 소개합니다.&lt;/p&gt;

&lt;h2 id=&quot;td-prediction&quot;&gt;TD Prediction&lt;/h2&gt;

&lt;p&gt;TD Learning과 Monte Carlo Method는 모두 경험을 사용하여 Value Function을 추정합니다. Monte Carlo 방법은 State를 방문 후, Return의 값을 알 수 있을 때까지 기다린 다음 그 값을 토대로 $V(S_t)$를 추정합니다. 간단한 Every-visit Monte Carlo Method의 $V(S_t)$ 업데이트 식은 이전 장에서 배운 대로 다음과 같습니다.&lt;/p&gt;

\[V(S_t) \gets V(S_t) + \alpha \left[ G_t - V(S_t) \right] \tag{6.1}\]

&lt;p&gt;이전에 배운 대로 Return $G_t$는 시간 $t$ 이후에 얻는 기대 Reward이고, $\alpha$는 Step-size parameter 입니다. 만약 $\alpha$가 상수라면 식 (6.1)을 &lt;span style=&quot;color:red&quot;&gt;Constant-$\alpha$ Monte Carlo&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서는 $G_t$를 얻기 위해 Episode가 끝날 때까지 기다려야 하지만, TD Learning에서는 다음 시간 단계까지만 기다리면 된다는 차이가 있습니다. 시간 $t+1$에서 얻은 Reward $R_{t+1}$과 추정된 Value Function $V(S_{t+1})$을 사용하여 $V(S_t)$를 업데이트 할 수 있습니다. 가장 간단한 TD의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[V(S_t) \gets V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right] \tag{6.2}\]

&lt;p&gt;식 (6.2)와 같은 업데이트 식을 $TD(0)$, 또는 1-step TD 라고 부릅니다. 이렇게 부르는 이유는 추후 12장에서 다룰 $TD(\lambda)$와 7장에서 다룰 $n$-step TD의 특수한 형태이기 때문입니다. 아래는 $TD(0)$ 업데이트를 사용한 Value Function 추정 방법의 완전한 Pseudocode입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TD(0) 알고리즘은 부분적으로 기존 추정값을 기반으로 업데이트 하기 때문에 DP와 같은 Bootstrapping 방법이라고 부를 수 있습니다. 또한 3장에서 다루었던 Value Function의 추정 식을 다시 가져와보면,&lt;/p&gt;

\[\begin{align}
v_{\pi} &amp;amp; \doteq \mathbb{E}_{pi} \left[ G_t | S_t = s \right] \tag{6.3} \\ \\
&amp;amp;= \mathbb{E}_{pi} \left[ R_{t+1} + \gamma G_{t+1} | S_t = s \right] \tag{from (3.9)} \\ \\
&amp;amp;= \mathbb{E}_{pi} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s \right] \tag{6.4}
\end{align}\]

&lt;p&gt;식 (6.3)은 Monte Carlo Method가 Target으로 하는 추정값이고, 식 (6.4)는 DP가 Target으로 하는 추정값입니다. 두 식이 추정값인 이유는 Monte Carlo Method에서는 실제 Expected Return이 아닌 Sample Return이 사용되기 때문이고, DP에서는 $v_{\pi}(S_{t+1})$가 알려져 있지 않아 $V(S_{t+1})$를 대신 사용하기 때문입니다. TD의 Target은 Sample Return을 사용하며 역시 $V(S_{t+1})$를 사용하기 때문에 추정값이 됩니다. 즉, TD는 Monte Carlo Method와 DP를 결합한 것으로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 TD(0)에 대한 Backup Diagram입니다. 맨 위의 흰 점은 State 노드를 의미하며, 이에 대한 추정값은 바로 다음 State만 사용하기 때문에 Backup Diagram이 간단하게 표현됩니다. TD와 Monte Carlo 방법은 Sample Update라고 부르는데, 그 이유는 현재 State에서 이어지는 Action과 Reward를 사용하여 원래의 State(또는 State-Action 쌍)를 업데이트하기 때문입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 TD(0)에서 0이 의미하는 것은 $S_t$의 추정값과 $R_{t+1} + \gamma V(S_{t+1})$의 추정값 사이의 차이입니다. 이것을 TD-error라고 하는데, 추후 배울 강화학습에서 다양한 형태로 만나보실 수 있습니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6.5}\]

&lt;p&gt;각 시간 단계의 TD-error는 그 당시에 추정했던 값의 오차입니다. TD-error는 다음 State와 다음 Reward에 따라 달라지기 때문에 실제로는 다음 단계 전까지 사용할 수 없습니다. 즉, $\delta_t$는 $V(S_t)$에서 발생하는 오차이지만, $t+1$ 시간이 되어야 알 수 있습니다. 만약 $V$가 Episode가 끝나기 전까지 변경되지 않는다면 Monte Carlo의 오차 또한 TD-error의 합으로 표현할 수 있습니다. (다행히 Monte Carlo Method에서는 Episode가 끝나기 전까지 $V$가 변하지 않습니다)&lt;/p&gt;

\[\begin{align}
G_t - V(S_t) &amp;amp;= R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \tag{from (3.9)} \\ \\
&amp;amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+2} - V(S_{t+2})) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (G_T - V(S_T)) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (0 - 0) \\ \\
&amp;amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k \tag{6.6}
\end{align}\]

&lt;p&gt;만약 $V$가 Episode 도중에 업데이트 되는 경우 위의 과정은 정확하지 않지만, Step-size가 작으면 대략적으로 근접할 수는 있습니다. 이 과정은 TD에서의 이론과 알고리즘에서 중요한 역할을 합니다.&lt;/p&gt;

&lt;h2 id=&quot;advantage-of-td-prediction-methods&quot;&gt;Advantage of TD Prediction Methods&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 TD가 Monte Carlo Method나 DP에 비해 어떤 이점이 있는지 알아보도록 하겠습니다. 가장 먼저 생각할 수 있는 TD의 장점은 DP와 달리 Environment의 Model이 필요하지 않다는 점입니다. 물론 이것은 Monte Carlo Method도 가지고 있는 장점이긴 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 Monte Carlo Method에 비해 TD가 가지는 장점은 Incremental로 구현이 가능하다는 것입니다. Monte Carlo Method의 가장 치명적인 단점은 Episode가 끝날 때까지 기다려야 한다는 것입니다. Episode의 길이가 길다면 그만큼 학습하기 위해 대기해야하는 시간 또한 길어지게 됩니다. 지난 장에서 Episode를 일부 무시하거나 줄이는 방법들을 잠깐 소개하였으나, 이것들은 실험적인 방법이기 때문에 문제가 해결된다고 볼 수는 없습니다.&lt;/p&gt;

&lt;p&gt;다만 TD에서 하나의 Sample만을 가지고 학습하는 것이 과연 올바른 값으로 수렴함을 보장하는지를 따져봐야 합니다. 다행히도 고정된 Policy $\pi$에 대해  Step-size parameter $\alpha$가 충분히 작고, 조건 (2.7)을 만족한다면 TD(0)은 확률 1로 $v_{\pi}$에 수렴합니다. 일반적인 상황에 대해서는 9.4에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;TD와 Monte Carlo Method가 모두 수렴하는 것이 보장된다면, 다음으로 논의할 것은 어떤 것이 더 빨리 수렴하는가입니다. 안타깝게도 어떤 방법이 더 빠르게 수렴하는지는 수학적으로 증명되지 않았지만, 일반적으로는 TD가 Monte Carlo Method보다 빠르게 수렴한다고 합니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method와 TD의 장단점을 요약하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo has &lt;span style=&quot;color:red&quot;&gt;high variance&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;zero bias&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good convergence properties (even with function approximation)&lt;/li&gt;
  &lt;li&gt;Not very sensitive to initial value&lt;/li&gt;
  &lt;li&gt;Very simple to understand and use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Temporal-Difference has &lt;span style=&quot;color:red&quot;&gt;low variance&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;some bias&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Usually more efficient than Monte Carlo&lt;/li&gt;
  &lt;li&gt;TD(0) converges to $v_{\pi}(s)$ (but not always with function approximation)&lt;/li&gt;
  &lt;li&gt;More sensitive to initial value&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimality-of-td0&quot;&gt;Optimality of TD(0)&lt;/h2&gt;

&lt;p&gt;만약 10개의 Episode, 또는 100개의 시간 단계와 같이 제한된 숫자의 경험만 사용할 수 있다고 가정해봅시다. 이 경우 Incremental 학습 방법의 접근 방식은 수렴될 때까지 경험을 반복적으로 학습하는 것입니다. 문제는 Value Function $V$가 Episode 당 단 한번만 변경된다는 것입니다. (TD(0) 알고리즘 참고) 이렇게 전체 데이터를 사용하여 학습하고 수렴하도록 만드는 방법을 &lt;span style=&quot;color:red&quot;&gt;Batch Updating&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;배치 업데이트에서 TD(0)는 Step-size parameter $\alpha$가 충분히 작다는 조건 하에 답에 수렴합니다. Constant-$\alpha$ Monte Carlo Method도 동일한 조건에서는 수렴하지만, 다른 답으로 수렴합니다. 이것은 설명으로만 이해하기 어렵기 때문에, 교재에 나와있는 예제를 통해 보충하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.4) You are the Predictor&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;알 수 없는 MDP 문제에 대해 다음 8개의 Episode가 관찰되었다고 가정해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A, 0, B, 0&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;첫 번째 Episode는 State A에서 시작하여 0의 Reward를 받고 State B로 이동한 다음, 0의 Reward를 받고 종료된 것을 의미합니다. 나머지 Episode 중 6개는 State B에서 시작하여 1의 Reward를 받고 종료되고, 마지막 Episode는 State B에서 시작하여 0의 Reward를 받고 종료됩니다.&lt;/p&gt;

&lt;p&gt;위의 8개의 Episode를 기반으로 Value $V(A)$, $V(B)$를 추정해봅시다. 먼저 State B를 보면 8개의 Episode 중 2개의 Episode가 0의 Reward를 받고, 6개의 Episode가 1의 Reward를 받으므로 $V(B) = 0.75$라고 쉽게 추정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;문제는 $V(A)$입니다. 주어진 Episode에서 State A가 언급된 것은 첫 번째 Episode 하나인데, 이것만으로 유추하자면 State A는 100% 확률로 0의 Reward를 받고 State B로 이동하는 것처럼 보입니다. 이것을 그림으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, State A의 Value는 $V(B)$와 동일하게 $V(A) = 0.75$라고 말할 수 있습니다. 이 결과는 Batch TD(0)로 계산해도 동일한 결과가 나옵니다.&lt;/p&gt;

&lt;p&gt;또 다른 추정 방법은 8개의 Episode 중 State A는 단 한번 등장했고, 그 때의 Reward는 0이었으므로 $V(A) = 0$으로 추정하는 것입니다. 이 결과는 Batch MC와 동일한 결과입니다. 이렇게 추정하면 실제로 데이터에 대해서는 RMS Error가 0이 됩니다. (0으로 추정했는데 표본이 0이므로) 하지만 생각해봤을 때, $V(A) = 0$으로 추정하는 것 보다 $V(A) = 0.75$로 추정하는 것이 더 합리적으로 보입니다. 이를 토대로 결론을 내려보면, Monte Carlo 방법은 현재 주어진 데이터에서 더 우수한 성능을 보일 수 있지만, 미래에 얻게 될 데이터까지 고려한다면 TD(0)가 더 우수한 성능을 보일 것이라고 기대할 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Example 6.4를 보시면 Batch TD(0)와 Batch MC로 찾은 추정치가 어떻게 다른지 알 수 있습니다. Batch MC는 항상 Training Data에서 Root Mean Square (RMS) Error를 최소화하는 추정치를 찾지만, Batch TD(0)는 항상 Maximum-likelihood Model에 맞는 추정치를 찾습니다. Batch TD(0)와 같이 추정하는 것을 &lt;span style=&quot;color:red&quot;&gt;Certainty-equivalence Estimate&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;Certainty-equivalence Estimate은 확실히 최적의 해법처럼 보이지만, 일반적인 상황에서는 위의 Example 6.4와 같이 직접 Value Function를 계산하는 것이 현실적으로 어렵다는 문제점이 있습니다. $n$을 State의 수라고 하면(즉, $n = \lvert \mathcal{S} \rvert$), Maximum-likelihood Model을 추정하는데 $n^2$의 메모리가 필요하고, Value Function를 계산하는 데 $n^3$ 만큼의 시간 단계가 필요합니다. 이러한 측면에서 보면 TD(0)는 Example 6.4와 같이 직접 계산하지 않고도 훨씬 적은 메모리와 시간을 소모하여 이와 동일한 답으로 수렴하기 때문에 우수한 방법임을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;sarsa--on-policy-td-control&quot;&gt;Sarsa : On-policy TD Control&lt;/h2&gt;

&lt;p&gt;이제는 TD를 사용한 Control 방법에 대해 알아보겠습니다. 이전 장들과 마찬가지로 Control은 Generalized Policy Iteration (GPI)를 기반으로 하며, Evaluation과 Improvement에 TD를 사용합니다. Monte Carlo Method와 마찬가지로 Exploration과 Exploitation 사이의 Trade-off가 있으며 그 때와 동일하게 On-policy와 Off-policy라는 두 가지 방법으로 나누어 접근합니다. 이번 Section에서는 먼저 On-policy를 사용한 TD Control 방법을 다룹니다.&lt;/p&gt;

&lt;p&gt;첫 번째 단계로는 State-Value Function가 아닌 Action-Value Function를 배우는 것입니다. 즉, 모든 State $s$와 Action $a$에 대하여, Policy $\pi$를 기반으로 $q_{\pi} (s, a)$를 추정합니다. 각각의 Episode는 다음과 같이 State와 State-Action이 반복적으로 이어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 계속 State에 대한 Value Function만을 고려했으나, 이번 Section부터는 State-Action 쌍에 대한 Value를 측정하기 때문에 $V(S)$가 아니라 $Q(S, A)$를 사용합니다. 이 부분을 제외하고는 식 자체가 크게 다르지 않고, 수렴 역시 동일하게 보장됩니다. TD(0)에서의 Q 값을 업데이트 하는 식은 다음과 같습니다.&lt;/p&gt;

\[Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right] \tag{6.7}\]

&lt;p&gt;이 업데이트는 마지막 State가 아닌 모든 State $S_t$마다 수행됩니다. 만약 $S_{t+1}$이 마지막 State라면 $Q(S_{t+1}, A_{t+1})$는 0으로 정의됩니다. 이 업데이트를 사용하기 위해서 필요한 요소는 $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ 5가지입니다. 그렇기 때문에 이 업데이트를 사용한 TD 제어를 &lt;span style=&quot;color:red&quot;&gt;Sarsa&lt;/span&gt;라고 부릅니다. Sarsa의 Backup Diagram은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa는 이전에 배운 On-policy 방법과 마찬가지로 Policy $\pi$에 대해 $q_{\pi}$를 추정하고, $q_{\pi}$에 대해 greedy하게 $\pi$를 변경하는 과정을 반복합니다. 전체 Sarsa 알고리즘의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa의 수렴성은 Q에 대한 Policy의 조건이 어떤 지에 따라 다릅니다. $\epsilon$-greedy을 사용하는 것과 상관 없이, Step-size에 대한 조건인 식 (2.7)을 만족하고, 모든 State-Action 쌍이 무한한 횟수로 방문되는 조건 하에 확률 1로 Optimal Policy 및 Action-Value Function으로 수렴합니다.&lt;/p&gt;

&lt;h2 id=&quot;q-learning--off-policy-td-control&quot;&gt;Q-learning : Off-policy TD Control&lt;/h2&gt;

&lt;p&gt;Off-policy TD Control 학습 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;Q-learning&lt;/span&gt;이라는 이름으로 불립니다. Q-learning은 강화학습 중에서도 가장 유명한 학습 방법이며, 1989년에 Chris Watkins 교수님이 처음 제안한 방법입니다. Q-learning의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] \tag{6.8}\]

&lt;p&gt;식 (6.8)은 Sarsa의 업데이트 규칙인 식 (6.7)과 대부분 유사합니다. 딱 한부분만 다른데, 다음 State와 Action을 사용했던 Sarsa와 달리 Q-learning은 다음 State에서 가능한 Action 중 Q값이 가장 큰 Action을 현재의 Q 값에 반영한다는 것입니다. Off-policy는 Behavior Policy와 Target Policy가 구분되기 때문이라고 이해하시면 되겠습니다. Q-learning의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.6) Cliff Walking&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa와 Q-learning의 차이를 한 눈에 알 수 있는 예제로 Cliff Walking이라는 문제가 있습니다. 위와 같이 Gridworld로 구성된 Environment에서 Episode는 S라는 위치에서 시작하고, G에 도달하는 것이 목표입니다. Cliff State의 Reward는 -100로, 나머지 모든 State에서의 Reward는 -1로 정의되어 있습니다. 만약 Agent가 Cliff나 G에 도달하면 Episode가 종료됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 $\epsilon = 0.1$으로 설정한 $\epsilon$-greedy Policy를 사용했을 때 Sarsa와 Q-learning의 성능을 비교한 그림입니다.  Q-learning은 Cliff의 가장자리를 따라 이동하는 Policy로 수렴하지만, 때때로 $\epsilon$-greedy로 인해 Cliff State에 진입하는 경우가 발생합니다. 반면에 Sarsa는 Cliff에 최대한 접근하지 않도록 안전한 경로로 학습하기 때문에, Cliff에 떨어지는 경우는 발생하지 않지만 최적의 경로보다 우회한 경로로 수렴하게 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;expected-sarsa&quot;&gt;Expected Sarsa&lt;/h2&gt;

&lt;p&gt;이번 Section부터는 Sarsa와 Q-learning으로부터 파생된 방법을 하나씩 다룰 예정입니다. 먼저 Sarsa를 변형한 Expected Sarsa 방법이 있습니다. Expected Sarsa는 이름과 같이 (다음 State Q 값의) 평균을 사용한 방법입니다. Expected Sarsa의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
Q(S_t, A_t) &amp;amp; \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \mathbb{E}_{\pi} \left[ Q ( S_{t+1}, A_{t+1} ) | S_{t+1} \right] - Q(S_t, A_t) \right] \\ \\
&amp;amp;= Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_a \pi (a | S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right] \tag{6.9}
\end{align}\]

&lt;p&gt;교재에서는 다음 State $S_{t+1}$이 주어졌을 때, Sarsa의 Expectation대로 이동하기 때문에 Expected Sarsa라는 이름이 붙었다고 합니다. 어차피 비슷한 의미이니 다음 State의 평균 Q 값을 사용하기 때문에 Expected Sarsa라는 이름이 붙었다고 이해해도 괜찮을 것 같습니다.&lt;/p&gt;

&lt;p&gt;Expected Sarsa는 Sarsa보다 계산이 더 복잡하지만, Sarsa에서 $A_{t+1}$을 무작위로 선택하기 때문에 발생하는 Variance가 없어진다는 장점이 있습니다. 이로 인해 동일한 학습량을 놓고 비교했을 때, Expected Sarsa는 Sarsa보다 약간 더 나은 성능을 보입니다. 아래의 그래프는 Cliff Walking 예제에서 Sarsa, Q-learning, Expected Sarsa 간의 성능을 비교한 그림입니다. Interim Performance에서 Expected Sarsa는 다른 두 방법보다 더 우수한 성능을 보일 뿐만 아니라, Asymptotic Performance에서 성능이 하락하는 Sarsa에 비해 성능 저하 없이 우수한 성능을 유지하는 모습을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cliff Walking 예제에서는 Expected Sarsa가 On-policy로 구현되었지만, Q-learning처럼 Behavior Policy와 Target Policy를 분리하여 Off-policy로 구현할 수도 있습니다. 이 경우 Expected Sarsa는 Q-learning과 거의 동일해지며, 추가적인 계산량만 감당할 수 있다면 대부분의 TD 알고리즘보다 더 우수한 성능을 보인다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Q-learning와 Expected Sarsa의 Backup Diagram을 비교하면 위와 같습니다. 두 방법 모두 다음 State의 모든 Action을 고려하는 것은 같으나, Q-learning은 다음 State에서 가능한 Action 중 가장 Q 값이 높은 Action만을 찾아 학습에 반영하는 반면에, Expected Sarsa는 다음 State에서 가능한 Action을 모두 고려하여 Q 값의 평균을 계산한 다음 학습에 반영한다는 차이점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;maximization-bias-and-double-learning&quot;&gt;Maximization Bias and Double Learning&lt;/h2&gt;

&lt;p&gt;지금까지 배운 Control 방법들은 Target Policy를 최적화하는데 중점을 두었습니다. 만약 추정한 값보다 최대값이 큰 경우, 이 값은 최대값의 추정값으로 사용될 수 있으며, 이로 인해 상당한 Bias를 초래할 수 있습니다. 예를 들어, 실제 $q(s, a)$ 값은 모두 0이라고 가정한 상황에서, 추정값 $Q(s,a)$가 불확실하므로 0보다 클 수도 있고 0보다 작을 수도 있습니다. 이 경우 실제 $q(s, a)$ 값의 최대값은 0이지만, 추정값 $Q(s,a)$의 최대값은 양수입니다. 즉, 양의 방향으로 Bias되었다고 볼 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Maximization Bias&lt;/span&gt;라고 합니다. 구체적인 예시를 통해 이것이 어떻게 발생하는지 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.7) Maximization Bias Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 Maximization Bias가 TD Control에서 어떻게 문제를 발생시키는지 알 수 있는 간단한 예제입니다. 예제의 MDP는 항상 State A에서 Episode가 시작됩니다. 왼쪽 끝, 또는 오른쪽 끝 State에 도달하면 Episode가 종료됩니다. State A에서 오른쪽으로 갈 때는 0의 Reward를 받지만, B에서 왼쪽으로 갈 때는 정규분포 $N(-0.1, 1)$에 따른 Reward를 받습니다. 왼쪽으로 갈 때의 평균 Reward는 -0.1이기 때문에, Reward가 0으로 고정된 오른쪽에 비해서 좋지 않은 방향임을 쉽게 알 수 있습니다. 문제는 왼쪽의 Reward가 정규분포를 따르기 때문에, 0보다 큰 Reward를 받을 확률이 있다는 것입니다. 그렇기 때문에 학습량이 적은 초기에는 왼쪽으로 가는 것을 더 좋은 Action으로 학습하게 되고, 그것이 위의 그래프에 나타나 있습니다. (Q-learning이 초기에 왼쪽을 선택학 확률이 매우 높음)&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Maximization Bias를 피하기 위한 대표적인 방법으로 &lt;span style=&quot;color:red&quot;&gt;Double Q-learning&lt;/span&gt;이 있습니다. Agent를 2개로 나누어 각각 따로 학습하는 것입니다. 즉, Q 값을 2개로 분리하여 각각 $Q_1(a)$와 $Q_2(a)$로 나누는 것입니다. 두 개의 Q 값은 모두 $q(a)$의 추정치가 됩니다. 재밌는 것은 각각의 Agent가 Action을 선택할 때 상대의 추정값을 사용한다는 것입니다. 즉, $Q_2(A^{*}) = Q_2(\underset{a}{\operatorname{argmax}} Q_1(a))$가 되는 것입니다. 이렇게 추정값을 계산하게 되면 $\mathbb{E} \left[ Q_2 (A^{*}) \right] = q(A^{*})$라는 의미가 되어 Bias되지 않습니다. 마찬가지로 $Q_1$의 최적의 Action 선택 방식은 $Q_1(A^*) = Q_1(\underset{a}{\operatorname{argmax}} Q_2(a))$이 됩니다. 주의할 점은, 두 개의 추정을 따로 학습하지만, 각각의 Action마다 $Q_1$ 또는 $Q_2$ 중 하나의 추정치만 업데이트한다는 것입니다. 따라서 Double Q-learning은 공간 복잡도를 2배로 늘리지만, 시간 복잡도를 늘리지는 않습니다. 즉, 새로운 Action 샘플이 들어오면, 50% 확률로 $Q_1$을 학습하고, 50% 확률로 $Q_2$를 학습하는 것입니다. Double Q-learning의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[Q_1(S_t, A_t) \gets Q_1(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q_2 (S_{t+1}, \underset{a}{\operatorname{argmax}} Q_1(S_{t+1}, a)) - Q_1(S_t, A_t) \right] \tag{6.10}\]

&lt;p&gt;식 (6.10)에서 $Q_2$를 학습할 때는 $Q_1$과 $Q_2$의 위치를 서로 바꾸면 됩니다. Double Q-learning에 대한 완전한 Pseudocode는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Double Q-learning과 Q-learning에 대한 성능 비교는 예제 6.7의 그래프를 확인해보시면 됩니다. 기존 Q-learning과 비교했을 때, 우연히 발생한 양의 Reward 쪽으로 Bias되지 않는 것을 볼 수 있습니다. 여기서는 Double Q-learning만 소개했으나, Sarsa와 Expected Sarsa를 응용해서 구현할 수도 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;games-afterstates-and-other-special-cases&quot;&gt;Games, Afterstates, and Other Special Cases&lt;/h2&gt;

&lt;p&gt;1장에서 잠깐 다루었던 Tic-Tac-Toe 예제를 다시 떠올려봅시다. 그 당시에도 TD에 대해 잠깐 소개했었는데, 다시 그 부분을 확인해보시면 Tic-Tac-Toe에서는 Agent가 이동한 후 보드의 State를 평가했었습니다. 이렇게 Action을 먼저 한 후 State의 Value를 확인하는 것을 &lt;span style=&quot;color:red&quot;&gt;Afterstates&lt;/span&gt;라고 하고, 이 때의 Value Function를 &lt;span style=&quot;color:red&quot;&gt;Afterstate Value Function&lt;/span&gt;이라고 합니다. Afterstates는 Environment에 대한 초기 지식이 있지만, 전체 지식은 없고 일부분에 대한 지식만 있을 때 유용합니다. 예를 들어, 체스 같은 게임에서는 말을 움직였을 때 어떤 효과가 일어나는지는 바로 알 수 있지만, 상대방이 어떻게 행동할지는 모릅니다. Afterstates는 이런 종류의 지식을 활용하여 보다 효율적인 학습을 하는 방법이라고 이해하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;Afterstate가 더 효율적인 것을 보이기 위해 다시 Tic-Tac-Toe 예제를 사용해보겠습니다. 기존의 Action-Value Function는 현재 O/X가 체크된 State를 토대로 Value를 추정합니다. 하지만 다음 그림을 보시면 현재 State가 될 수 있는 이전 State는 한 가지가 아닐 수 있습니다. 그러나 두 State는 모두 같은 후속 State를 만들기 때문에 동일하게 평가해야 합니다. 기존의 Action-Value Function는 두 State를 개별적으로 평가하지만, Afterstate Value Function는 두 State를 동등하게 평가하는 차이점이 있습니다. 즉, 아래 그림에서 왼쪽에서 발생하는 모든 학습은, 오른쪽 State에도 즉시 적용됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Afterstate는 게임에서만 사용되는 것이 아닙니다. 예를 들어, Queuing 문제에서는 Queue에 있는 작업을 서버에 할당하는지/거부할 것인지 등을 계속 판단해야 하는데, 동일한 작업에 대해서 동일하게 판단해야하므로 Afterstates를 적용할 수 있습니다. 이 외에도 Afterstates를 사용하는 여러 문제가 있지만, 여기서 모두 소개하기에는 불가능합니다. Afterstates는 동일한 방식으로 상호 작용하는 State 및 Policy에 대해서는 동일하게 추정해야하는 문제에 적용하는 것이라고 이해하시면 되겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 새로운 학습 방법인 &lt;strong&gt;Temporal Difference (TD)&lt;/strong&gt;를 소개하였습니다. 이전 장들과 마찬가지로 예측 문제와 제어 문제로 나누어 소개하였으며, TD 또한 &lt;strong&gt;Generalized Policy Iteration&lt;/strong&gt; 아이디어에서 기반하였습니다.&lt;/p&gt;

&lt;p&gt;TD의 Control 방법 또한 On-policy와 Off-policy 방법으로 나뉘며, On-policy TD는 &lt;strong&gt;Sarsa&lt;/strong&gt;, Off-policy TD는 &lt;strong&gt;Q-learning&lt;/strong&gt;이라는 대표적인 방법을 소개하였습니다. 그 외에 &lt;strong&gt;Expected Sarsa&lt;/strong&gt;나 &lt;strong&gt;Double Q-learning&lt;/strong&gt;등의 변형 버전도 소개하였습니다. 이번 장에서 소개하지 않은 방법으로 &lt;strong&gt;Actor-Critic&lt;/strong&gt;이 있는데, 이는 13장에서 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;TD는 지금까지 배운 강화학습 방법 중에 가장 널리 사용되는 방법입니다. Dynamic Programming이나 Monte Carlo Method 등과 비교해서 단순할 뿐만 아니라 계산량 또한 적기 때문입니다. 이 뒤에 이어지는 내용들은 대부분 TD를 심도 있게 확장하여 더 강력하게 만든 알고리즘을 소개할 예정입니다. 특히 Part II 부터는 TD와 기존의 전통적인 기계학습 방법들을 융합한 새로운 학습 방법들을 소개합니다.&lt;/p&gt;

&lt;p&gt;6장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장은 강화학습의 핵심 아이디어인 Temporal-Difference (TD) Learning을 다루게 됩니다. TD Learning은 Environment에 대한 정확한 Model 없이 경험을 통해 학습한다는 Monte Carlo의 아이디어와 Bootstrap 하지 않고 학습된 다른 추정치를 기반으로 추정치를 업데이트한다는 Dynamic Programming 아이디어를 결합하여 만들어졌습니다. 이번 장의 시작은 이전 장들과 같이 주어진 Policy $\pi$에 대한 Value Function $v_{\pi}$를 추정하는 문제로부터 시작하며, Optimal Policy을 찾는 Control 문제에서도 이전과 같이 GPI를 변형하여 접근합니다. 이전 장들과의 주요 차이점은 Prediction 문제에 대한 접근 방식입니다.</summary></entry><entry><title type="html">Monte Carlo Methods</title><link href="http://localhost:4000/studies/monte-carlo-methods/" rel="alternate" type="text/html" title="Monte Carlo Methods" /><published>2022-03-11T00:00:00+09:00</published><updated>2022-03-11T00:00:00+09:00</updated><id>http://localhost:4000/studies/monte-carlo-methods</id><content type="html" xml:base="http://localhost:4000/studies/monte-carlo-methods/">&lt;p&gt;이번 장에서는 지난 장과 마찬가지로 Value Function을 추정하고 Optimal Policy를 찾기 위한 방법을 다루지만, 지난 장과는 달리 MDP에 대한 완전한 정보를 알고 있다고 가정하지 않습니다. &lt;span style=&quot;color:red&quot;&gt;Monte Carlo Method&lt;/span&gt;는 Environment와의 상호 작용을 통해 얻은 경험을 기반으로 Optimal Policy를 찾는 방법입니다. 이 때 Environment와의 상호작용은 실제로 이루어지는 경험 뿐만이 아니라 시뮬레이션된 경험이라도 상관 없습니다.&lt;/p&gt;

&lt;p&gt;3장에서 MDP를 끝이 존재하는 Episodic Task와 끝이 없는 Continuing Task로 분류하였는데, 이번 장에서는 일단 Episodic Task 상황만 가정하도록 하겠습니다. Monte Carlo Method 또한 지난 장에서 배운 Generalized Policy Iteration (GPI)의 아이디어를 기반으로 하지만, 그 때와 달리 MDP에서 직접 Value Function을 계산하지 않고 Sample로부터 Value Function을 계산합니다. 물론 여전히 그 때처럼 Optimal Value Function에 수렴하기 위해 상호작용하는 것은 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-prediction&quot;&gt;Monte Carlo Prediction&lt;/h2&gt;

&lt;p&gt;가장 먼저 Policy가 주어졌을 때 State-Value Function을 학습하기 위해 Monte Carlo Method를 고려해보겠습니다. State의 Value는 해당 State에서 시작했을 때 얻을 수 있는 기대 수익이므로, 경험으로 이를 추정하기 위해서는 해당 State를 여러 번 방문한 후 얻은 수익의 평균을 구하면 됩니다. 당연히 많이 방문할수록 평균값이 정확한 State의 Value에 가까워집니다. 이것이 바로 Monte Carlo 방법의 기본 아이디어입니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method는 두 가지 방법으로 나눌 수 있습니다. 만약 Policy가 $\pi$로 주어지고 이 Policy를 따랐을 때 어떤 Epiosde에서 State $s$를 방문했다고 가정해봅시다. 그런데 한 Episode에서 State $s$를 꼭 한 번만 방문한다는 보장은 없습니다. 한 Episode에서 State $s$를 여러 번 방문했을 때 $v_{\pi}(s)$를 어떻게 구해야 할까요?&lt;/p&gt;

&lt;p&gt;Monte Carlo 방법에는 &lt;span style=&quot;color:red&quot;&gt;First-visit MC Method&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;Every-visit MC Method&lt;/span&gt;가 있습니다. 두 방법 모두 State $s$를 방문한 후, 얻은 Reward의 평균을 추정하는 것은 같습니다. 하지만 First-visit MC Method는 가장 처음에 방문한 것만 계산에 사용하고, Every-visit MC Method는 방문한 모든 것을 계산에 사용한다는 차이가 있습니다. 두 방법 모두 State $s$를 무한히 방문했을 때 $v_{\pi}(s)$에 수렴한다는 것은 같습니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 First-visit MC Method를 위주로 살펴볼 예정이며, Every-visit MC Method는 9장과 12장에서 다시 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;First-visit MC Method의 Pseudocode는 다음과 같습니다. 이 알고리즘을 Every-visit MC Method로 수정하려면 Unless ~ 로 시작하는 조건문을 삭제하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 5.1) Blackjack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Blackjack 게임은 카지노에서 자주 보이는 카드 게임 중 하나로써, 딜러와 플레이어가 승부하는 게임입니다. 플레이어가 가지고 있는 카드의 합이 딜러가 가지고 있는 카드의 합보다 높거나 플레이어 카드의 합이 21일 경우 무조건 승리하지만, 21을 초과할 경우에는 무조건 패배합니다. 이 때, 특수한 카드인 Jack (J), Queen (Q), King (K)은 10으로 취급합니다. Ace (A)는 플레이어의 선택에 따라 1로 취급할 수도 있고 11로 취급할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;게임은 플레이어와 딜러가 각각 무작위한 2장의 카드를 받고 시작합니다. 이 때 플레이어의 카드는 모두 오픈하지만, 딜러는 1개만 오픈하고 1개는 뒤집어 놓습니다.&lt;/p&gt;

&lt;p&gt;플레이어가 선택할 수 있는 Action은 &lt;strong&gt;Hit&lt;/strong&gt;과 &lt;strong&gt;Stand&lt;/strong&gt;입니다. Hit을 선택하면 무작위 카드를 한 장 더 받을 수 있고, Stand를 선택하면 카드를 더 받지 않습니다. 만약 Hit을 선택했을 때, 새로 받은 카드를 포함하여 합이 21을 초과할 경우 플레이어는 즉시 패배합니다. 이것을 &lt;strong&gt;Bust&lt;/strong&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;플레이어가 Stand를 선택하고 카드의 합이 21을 넘지 않는다면, 딜러가 게임을 시작합니다. 이 때 딜러는 자신의 의지대로 플레이할 수 없고, 카드의 합이 17보다 작으면 무조건 Hit을 해야하고, 그렇지 않으면 무조건 Stand를 해야합니다. 딜러가 Bust가 된다면 플레이어의 승리입니다.&lt;/p&gt;

&lt;p&gt;만약 플레이어와 딜러 두 명 모두 Bust가 아니라면, 그 때 카드의 합을 비교하여 높은 쪽이 승리합니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Solution)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;먼저 Blackjack 게임의 State와 Action, 그리고 Reward를 정의해봅시다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;초기에 플레이어가 받은 카드 2장의 합 : 최소 12 - 최대 21
딜러가 가지고 있는 카드 중 공개된 1장의 숫자 : 최소 1 (Ace) - 최대 10
내가 Ace를 가지고 있는지에 대한 여부 : 예 or 아니오&lt;/p&gt;

&lt;p&gt;따라서 State의 총 갯수는 10 * 10 * 2 = 200개입니다.&lt;/p&gt;

&lt;p&gt;State는 (플레이어 카드의 합, 딜러의 카드, Ace 보유 여부) 로 표기합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;플레이어의 Action만 고려하면 되므로 Hit 또는 Stand로 2개입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;플레이어가 게임에서 승리하면 +1, 패배하면, -1, 무승부일시 0으로 정의합니다.&lt;/p&gt;

&lt;p&gt;다음으로 Policy를 세팅해보겠습니다. 교재에서는 현재 플레이어가 가지고 있는 카드의 합이 20, 21이면 Stand를 하고, 그 이외는 Hit을 선택하도록 Policy를 정의하였습니다. 이것을 가지고 Python을 사용하여 구현하면 다음과 같습니다.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Blackjack-v1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500000&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;total_return&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;N&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;value&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;total_return&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;N&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;위의 코드는 First-visit MC Method로 Blackjack 게임에서 각 State의 Value를 추정한 프로그램입니다. (프로그램을 실행하기 위해서는 gym과 pandas 라이브러리를 설치하셔야 합니다) 만약 이 프로그램을 Every-visit MC Method로 바꾸고 싶다면, 32번째 Line의 if 문을 삭제하시면 됩니다.&lt;/p&gt;

&lt;p&gt;프로그램을 실행하게 되면 많이 방문한 State 순으로 10개를 보여주고, 얻은 총 Return과 방문 횟수, 그리고 추정한 State의 Value를 출력합니다. 이 프로그램은 단순히 State의 Value만 추정하는 것이기 때문에 Policy를 변경하지는 않습니다. 이러한 방법으로 1만개와 50만개의 Episode를 경험한 후, State의 Value를 도식화하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과를 보면 State의 Value가 높은 경우는 카드의 합이 20과 21일 경우밖에 없습니다. 어떻게 보면 당연한게, 저희가 설정한 Policy는 20과 21에서만 Stand를 하기 때문입니다. Blackjack 게임에 대해서 잘 아는 것은 아니지만 18이나 19에서 Hit을 한다면 Bust할 확률이 높기 때문에 승률이 낮을 수밖에 없습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;예제로 보여드린 Blackjack 게임은 Environment에 대해 완전한 정보를 가지고 있습니다. 하지만 지난 장에서 배운 DP 방법으로 Blackjack 게임의 Value Function을 계산하는 것은 쉽지 않습니다. 가장 어려운 점은 Transition Probability를 계산하는 것입니다. 예를 들어, 현재 플레이어가 가지고 있는 카드의 합이 14이고 플레이어가 Stand를 선택했다고 가정했을 때, 플레이어가 이길 확률을 계산하면 얼마일까요? 이것부터가 쉽지 않은데 모든 State에 대해 이것을 계산해야 한다고 생각하면 막막할 따름입니다. 그렇기 때문에 Environment에 대한 지식을 알고있는지 여부에 상관없이 Monte Carlo Method는 DP보다 간단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서 중요한 점은 각 State에 대해 추정한 Value가 독립적이라는 것입니다. 이것은 어떤 State에 대해 추정한 Value가 다른 State에 대해 추정한 Value에 영향을 끼치지 않는다는 뜻이므로, Bootstrap하지 않는다고 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 Monte Carlo Method는 Episode에 기반하기 때문에, 특정(단일) State의 Value를 추정할 때의 계산 비용은 State의 수가 매우 많더라도 간단하게 계산할 수도 있습니다. 지난 장에서 DP를 사용할 때는 State의 Value를 추정하기 위해서 모든 State를 고려한 것을 생각해보면 확실히 낫다고 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-estimation-of-action-values&quot;&gt;Monte Carlo Estimation of Action Values&lt;/h2&gt;

&lt;p&gt;Model을 사용할 수 없는 경우, 즉 Environment에 대한 완전한 정보가 없는 경우에는 State-Value를 사용하는 것 보다 Action-Value를 사용하는 것이 더 좋습니다. Value를 기반으로 Policy를 만들 때는 각 Action에 대한 Value를 명시적으로 추정해야하기 때문입니다. 따라서 Monte Carlo Method의 주요 목표 중 하나는 $q_{*}$를 추정하는 것입니다. 그 목표를 이루기 위해서는 먼저 Action-Value에 대한 Policy Evaluation을 고려해야 합니다.&lt;/p&gt;

&lt;p&gt;Action-Value에 대한 Policy Evaluation은 State $s$에서 시작하여 Action $a$를 선택한 후, Policy $\pi$를 따를 때의 기대 Reward인 $q_{\pi} (s, a)$를 추정하는 것입니다. 추정 방법은 State-Value를 추정하는 방법과 거의 유사합니다.&lt;/p&gt;

&lt;p&gt;Action-Value에 대한 Policy Evaluation을 할 때 유일한 문제는 많은 State-Action 쌍이 방문되지 않을 수 있다는 것입니다. 특히 $\pi$가 Deterministic Policy인 경우, 각 State에 대해서는 하나의 Action만 선택하기 때문에 Policy가 선택하는 Action 외에는 모두 방문하지 않기 때문입니다. 2장에서 언급했듯이 Policy에 대한 평가가 제대로 작동하기 위해서는 지속적인 탐색이 보장되어야 합니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해서는 모든 State-Action 쌍이 선택될 확률을 0보다 크게 만드는 것입니다. 아무리 작은 확률이라도 무한한 샘플을 얻게 되면 모든 State-Action 쌍을 많이 방문하게 되므로 제대로 Policy Evaluation을 할 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Assumption of Exploring Starts&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Assumption of Exploring Starts는 때때로 유용하지만, Environment와 실제 상호 작용을 하며 직접 학습할 때는 일반적으로 신뢰할 수 없습니다. 모든 State-Action 쌍을 방문할 수 있게 만들기 위해서는 차라리 Stochastic Policy를 고려하는 것이 낫습니다. 이번 장 뒷부분에서는 그렇게 접근하는 2가지 방법에 대해 설명하겠지만, 일단 지금은 Assumption of Exploring Starts를 유지하면서 Monte Carlo Control 방법을 소개하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-control&quot;&gt;Monte Carlo Control&lt;/h2&gt;

&lt;p&gt;Monte Carlo Prediction의 기본 아이디어는 지난 장에서 배웠던 &lt;strong&gt;Generalized Policy Iteration (GPI)&lt;/strong&gt;과 동일합니다. 교재에서는 Monte Carlo Method가 DP와 달리 Environment에 대한 지식 없이 샘플 Episode 만으로 Optimal Policy와 Value Function을 찾는 데 사용할 수 있다는 것을 보여주지만, 굳이 이 부분까지 설명할 필요는 없을 것 같아서 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method로 수렴한다는 보장을 얻기 위해서 저희는 2가지 가정을 했습니다. 하나는 Assumption of Exploring Starts이고, 다른 하나는 무한한 수의 Episode가 있다는 가정입니다. 하지만 두 가정 모두 현실적이지 않기 때문에 Monte Carlo 알고리즘을 만들기 위해서는 이 두 가정을 모두 제거할 필요가 있습니다. 첫 번째 가정은 나중에 고려하도록 하고, 우선 무한한 수의 Episode에 대해서만 생각해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;첫 번째로 생각할 수 있는 해결 방법은 각각의 Policy Evaluation이 $q_{\pi_k}$를 근사화한다는 아이디어를 유지하는 것입니다. 즉, 어느 정도 근사치까지 정확한 수렴을 보장한다는 의미이지만, 문제의 크기가 조금만 커져도 실제와 가깝게 근사화하기 위해서는 너무 많은 Episode가 필요하다는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;두 번째로 생각할 수 있는 해결 방법은 Policy Improvement를 하기 전에 Policy Evaluation을 끝내지 않는 것입니다. Policy Evaluation은 매 단계마다 Value Function을 $q_{\pi_k}$에 가까워지게 하는 시도를 하지만, 어차피 후반 몇몇 단계를 제외하고는 실제로 그렇게 가깝지 않습니다. 지난 장에서 GPI를 처음 소개할 때 잠깐 소개했었는데, 극단적으로는 Policy Evaluation을 한 번만 수행하는 것도 가능하다고 했었습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서의 Policy Iteration은 각 Episode별로 Evaluation과 Improvement를 번갈아 수행합니다. 각 Episode에서 발생한 Reward는 Policy Evaluation에 사용되며, Episode에서 방문한 모든 State에서 Policy가 Improve 됩니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Monte Carlo with Exploring Starts&lt;/span&gt;라고 하며 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Monte Carlo with Exploring Starts는 초기 Policy에 상관없이 모든 State-Action 쌍에 대한 평균적인 Reward를 얻을 수 있습니다. 이전에 Monte Carlo Prediction에서는 Policy에 대한 평가만 했다면, Monte Carlo with Exploring Starts는 Policy를 업데이트하는 기능이 추가되며, Optimal Policy로 수렴할 것이라고 생각됩니다. &lt;strong&gt;생각됩니다&lt;/strong&gt; 라고 쓰는 이유는 아직 공식적으로 증명되지 않았기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 5.3) Solving Blackjack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Example 5.1에서 다루었던 Blackjack 게임을 다시 언급해보겠습니다. Example 5.1에서는 Policy에 대한 Value만을 계산했지만, Monte Carlo with Exploring Starts을 적용하게 되면 Optimal Policy를 구할 수 있습니다. 이를 도식화하면 아래 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-control-without-exploring-starts&quot;&gt;Monte Carlo Control &lt;em&gt;without&lt;/em&gt; Exploring Starts&lt;/h2&gt;

&lt;p&gt;이번에는 Assumption of Exploring Starts를 제거해보도록 하겠습니다. Assumption of Exploring Starts 없이 모든 Action이 선택될 수 있게 하는 유일한 방법은 Agent로 하여금 모든 State를 골고루 선택하게 설계하는 것입니다. 이를 보장하는 방법이 2가지 있는데, 하나는 On-policy 방법이고 다른 하나는 Off-policy 방법입니다. On-policy 방법은 Agent가 실제로 사용한 Policy를 Evaluation하거나 Improvement하는 방법이고, Off-policy 방법은 실제로 사용한 Policy와 다른 Policy를 Evaluation하거나 Improvement하는 방법입니다. 직전에 다룬 Monte Carlo with Exploring Starts는 On-policy 방법의 한 종류라고 볼 수 있습니다. 먼저 On-policy에 대해 알아본 다음, Off-policy 방법은 다음 섹션에서 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;On-policy 방법은 모든 State $s \in \mathcal{S}$와 모든 Action $a \in \mathcal{A} (s)$에 대해 $\pi (a \mid s) &amp;gt; 0$으로 시작하지만, 점차적으로 (Optimal) Deterministic Policy에 가까워집니다. 이것을 유도하는 방법은 여러 개가 있지만, 우선 가장 많이 쓰이는 &lt;span style=&quot;color:red&quot;&gt;$\epsilon$-Greedy Policy&lt;/span&gt;를 먼저 소개하도록 하겠습니다. $\epsilon$-Greedy Policy는 일반적으로 Value가 가장 높은 Action을 선택하지만, 낮은 확률로 무작위 Action을 선택하는 방법입니다. 조금 더 구체적으로 설명하자면 $\epsilon &amp;gt; 0$의 확률로 무작위 Action을 선택하고 $1 - \epsilon$의 확률로 Value가 가장 높은 Action을 선택하는 것입니다.&lt;/p&gt;

&lt;p&gt;$\epsilon$-Greedy Policy에서 $\epsilon$의 값은 고정된 값이 아닙니다. Value가 제대로 추정되지 않은 초기에는 $\epsilon$을 크게 만들어서 여러 Action을 선택하게 유도하고, 어느 정도 추정이 끝난 다음에는 $\epsilon$를 작게 만들어 최선의 선택을 하도록 유도하는 것입니다.&lt;/p&gt;

&lt;p&gt;On-policy Monte Carlo Control에서도  현재 Policy에 대한 Action-Value Function을 추정하기 위해 First-visit MC Method를 사용합니다. Monte Carlo with Exploring Starts와 달리 Assumption of Exploring Starts를 하지 않고, 대신 $\epsilon$-Greedy Policy를 사용합니다. $\epsilon$-Greedy와 같은 Policy를 &lt;span style=&quot;color:red&quot;&gt;$\epsilon$-soft&lt;/span&gt;라고도 부릅니다. &lt;span style=&quot;color:red&quot;&gt;On-policy Monte Carlo Control&lt;/span&gt;의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\epsilon$-Greedy Policy로부터 $q_{\pi}$를 개선하는 것은 Policy Improvement Theorem에 의해 보장됩니다. 이에 대한 유도는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
q_{\pi} (s, \pi&apos; (s)) &amp;amp;= \sum_a \pi&apos; (a | s) q_{\pi} (s, a) = \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + (1 - \epsilon) \max_a q_{\pi} (s, a) \tag{5.2} \\ \\
&amp;amp;\ge \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + (1 - \epsilon) \sum_a \frac{\pi (a | s) - \frac{\epsilon}{|\mathcal{A}(s)|}}{1-\epsilon} \\ \\
&amp;amp;= \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) - \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + \sum_a \pi (a | s) q_{\pi} (s, a) = v_{\pi} (s)
\end{align}\]

&lt;p&gt;위의 전개식은 Policy Improvement Theorem에 의해 $\pi^{\prime} \ge \pi$임을 보여주고 있습니다. 두 번째로 증명해야할 부분은 두 Policy $\pi^{\prime}$와 $\pi$가 최적의  $\epsilon$-soft Policy일 때, 다른 $\epsilon$-soft Policy보다 좋거나 같아야 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;이것을 증명하기 위해 먼저 Policy가 $\epsilon$-soft 로 이동되어야 한다는 조건을 제외하고 원래 Environment과 동일한 새 Environment를 가정해보겠습니다. 새 Environment에서는 원래 Environment와 동일한 State와 Action 집합이 설정되어 있습니다. 새 Environment에서는 State $s$에서 Action $a$를 선택했을 때 $1 - \epsilon$ 확률로 원래 Environment와 동일하게 동작하며, $\epsilon$ 확률로 무작위 Action을 선택한 다음 새로운 Action으로 이전 Environment처럼 동작합니다. 이런 새로운 Environment에서 할 수 있는 최선의 Policy는 원래 Environment에서 할 수 있는 최선의 Policy와 동일합니다. $\tilde{v}_{*}$와 $\tilde{q}_{*}$를 새 Environment에서 Optimal Value Function라고 정의하면, Policy $\pi$는 $v_{\pi} = \tilde{v}_{*}$인 경우에만 $\epsilon$-soft Policy 중 최선의 Policy입니다. 먼저 Transition Probability가 변경되었을 때 $\tilde{v}_{*}$는 Bellman Optimality Equation (식 3.19)에 대한 고유한 해법임을 다음을 통해 알 수 있습니다.&lt;/p&gt;

\[\begin{align}
\tilde{v}_* &amp;amp;= \max_a \sum_{s&apos;, r} \left[ (1 - \epsilon) p (s&apos;, r | s, a) + \sum_{a&apos;} \frac{\epsilon}{|\mathcal{A}(s)|} p (s&apos;, r | s, a&apos;) \right] \left[ r + \gamma \tilde{v}_* (s&apos;) \right] \\ \\
&amp;amp;= (1 - \epsilon) \max_a \sum_{s&apos;, r} p(s&apos;, r | s, a) \left[ r + \gamma \tilde{v}_*(s&apos;) \right] + \frac{\epsilon}{|\mathcal{A(s)}|} \sum_a \sum_{s&apos;, r} p (s&apos;, r |s, a) \left[ r + \gamma \tilde{v}_*(s&apos;) \right]
\end{align}\]

&lt;p&gt;다음으로 $\epsilon$-soft Policy $\pi$가 더 이상 개선되지 않고, $v_{\pi} = \tilde{v}_*$가 성립하면 식 (5.2)에 의하여 다음이 성립합니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp;= (1 - \epsilon) \max_a q_{\pi} (s, a) + \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) \\ \\
&amp;amp;= (1 - \epsilon) \max_a \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right] + \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a \sum_{s&apos;, r} p (s&apos;, r |s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right]
\end{align}\]

&lt;p&gt;그러나 이 방정식은 $\tilde{v}_{*}$를 $v_{\pi}$로 대입한 것을 제외하고는 앞의 방정식과 동일합니다. 여기서 $\tilde{v}_{*}$는 유일한 해법이므로 $v_{\pi} = \tilde{v}_{*}$일 수밖에 없습니다.&lt;/p&gt;

&lt;p&gt;결과적으로 $\epsilon$-soft Policy에서 Policy Iteration은 정상적으로 작동한다는 것을 알 수 있습니다. 또한 이제는 Assumption of Exploring Starts 없이 $\epsilon$-soft Policy에 대한 Improvement만으로 Optimal Policy를 찾을 수 있다는 것을 보장할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;off-policy-prediction-via-importance-sampling&quot;&gt;Off-policy Prediction via Importance Sampling&lt;/h2&gt;

&lt;p&gt;이전 섹션에서 배운 On-policy 방법의 가장 큰 단점은 Policy를 평가하기 위해서 실제로 경험해봐야 한다는 것입니다. 이 말은 최적의 Action을 찾기 위해 그렇지 않은 Action을 반드시 해봐야한다는 의미입니다. 이것을 해결하는 방법은 Target Policy와 Behavior Policy를 분리하는 것입니다. 이렇게 두 Policy를 분리하여 학습하는 방법을 &lt;span style=&quot;color:red&quot;&gt;Off-policy&lt;/span&gt; 방법이라고 합니다. 이렇게 이름을 붙이는 이유는 Target Policy에 Off 된다는 의미이기 때문이라고 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법은 On-policy 방법에 비해 일반적으로 많이 사용합니다. 특히 인간 전문가로부터 생성되는 데이터나, 비학습적인 컨트롤러로부터 생성되는 데이터를 학습하는데 우수한 성능을 가지고 있습니다. 그러나 Off-policy 방법은 On-policy 방법에 비해 추가적인 개념이 필요하기 때문에 더 복잡하고, 학습 데이터가 다르게 주어지기 때문에 분산이 더 크며, 수렴 속도 또한 더 느리다는 단점이 있습니다. 그렇기 때문에 On-policy 방법과 Off-policy 방법은 서로 우열을 가리기 어려우며, 두 방법 모두 상황에 따라 잘 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;이번 섹션에서 다룰 Off-policy 방법에서 Target Policy는 $\pi$, Behavior Policy는 $b$로 표기하며, 두 Policy 모두 고정되어 주어진 것으로 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;Policy $b$로부터 생성된 Episode를 사용하여 Policy $\pi$를 추정하기 위해서는 $\pi$에서 수행한 모든 Action이 적어도 가끔은 $b$에서도 수행되어야 합니다. 즉, 여기에는 $\pi (a \mid s) &amp;gt; 0$일 경우 반드시 $b(a \mid s) &amp;gt; 0$라는 것을 의미합니다. (이것을 &lt;span style=&quot;color:red&quot;&gt;Assumption of Coverage&lt;/span&gt;라고 합니다) 또한 Policy $\pi$는 Deterministic일 수 있으나, Policy $b$는 Stochastic이어야 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법에서 또 한가지 중요한 것은 바로 &lt;span style=&quot;color:red&quot;&gt;Importance Sampling&lt;/span&gt;입니다. 처음 강화학습을 공부했을 때 Importance Sampling에 대한 개념이 난해해서 이해하기 힘들었는데, 좋은 포스팅을 발견하여 여기에 잠깐 소개하도록 하겠습니다. &lt;a href=&quot;https://blog.naver.com/kwonpub/221143316307&quot;&gt;(원본 포스트 링크)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example of Importance Sampling)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;어느 섬에 키다리족과 난장이족이 있습니다. 문제를 쉽게 하기 위해 이 섬의 사람은 키가 160cm 아니면 180cm 둘 중 하나라고 가정하도록 하겠습니다. 오랜 시간에 걸쳐 키다리족과 난장이족의 인구 분포를 조사한 결과, 다음과 같은 키의 분포가 나왔습니다. 이 때, 확률 변수 X를 키다리족, 확률 변수 Y를 난장이족이라고 정의하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 이번에는 두 부족의 평균 키를 구하려고 합니다. 먼저 키다리족의 사람 중 무작위로 10명의 표본을 뽑았더니, 다음과 같은 키의 분포가 나왔습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180 180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이 분포를 토대로 표본집단의 평균을 구하는 것은 간단합니다. 표본집단의 평균은 모집단의 평균과 유사하니 완벽하게 일치하지는 않겠지만 모집단의 평균을 대략적으로 유추할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번에는 난장이족의 평균 키를 구하려고 합니다. 난장이족도 마찬가지로 무작위 표본을 뽑아 평균을 구하면 간단하지만, 모종의 문제로 난장이족의 표본을 구할 수 없다고 가정해봅시다. 대신 우리는 키다리족의 표본을 구했었고, 난장이족과 키다리족의 키의 분포를 알고 있습니다.&lt;/p&gt;

&lt;p&gt;이 키다리족의 표본을 토대로 난장이족의 키의 표본을 생성해봅시다. 먼저, 키가 160cm인 사람의 비율은 난장이족이 키다리족보다 2배가 많습니다. 그렇기 때문에 키가 160cm인 사람의 비율을 2배로 늘립니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160 160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180 180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다음으로 키가 180cm인 사람의 비율을 보면 키다리족이 난장이족보다 2배가 많습니다. 그래서 키가 180cm인 사람의 비율을 절반으로 줄입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160 160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이렇게 구한 표본을 토대로 평균을 구한다면, 이것은 난장이족의 평균 키라고 말할 수 있습니다. 이것이 바로 Importance Sampling의 한 예입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이번에는 강화학습에서의 Importance Sampling이 무엇인지 알아보겠습니다. 위의 예제에서 키다리족의 표본으로 난장이족의 데이터를 추정했듯이, Off-policy에서도 Behavior Policy $b$로부터 생성되는 데이터를 토대로 Target Policy $\pi$를 추정해야 합니다. 예제에서는 키다리족과 난장이족의 키의 분포가 미리 주어졌으나, 여기에서는 문제마다 다르기 때문에 직접 계산해야합니다. 이렇게 두 데이터 사이의 비율을 &lt;span style=&quot;color:red&quot;&gt;Importance Sampling Ratio&lt;/span&gt;라고 합니다. 이것을 수식으로 표현하기 위해, 먼저 State $S_t$에서 시작하여 State-Action Trajactory를 얻을 확률을 구하도록 하겠습니다.&lt;/p&gt;

\[\begin{align}
&amp;amp;Pr \left\{ A_t, S_{t+1}, A_{t+1}, ... , S_T | S_t, A_{t:T-1} ~ \pi \right\} \\ \\
&amp;amp;= \pi (A_t | S_t ) p (S_{t+1} | S_t, A_t) \pi (A_{t+1} | S_{t+1}) \cdots p (S_T | S_{T-1}, A_{T-1}) \\ \\
&amp;amp;= \prod_{k=t}^{T-1} \pi (A_k | S_k) p ( S_{k+1} | S_k, A_k)
\end{align}\]

&lt;p&gt;위 식에서 $p$는 식 (3.4)에서 정의한 &lt;strong&gt;State Transition Probability&lt;/strong&gt;입니다. 위 식을 토대로 Target Policy와 Behavior Policy 간의 Importance Sampling Ratio를 구하면 다음과 같습니다.&lt;/p&gt;

\[\rho_{t:T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi (A_k | S_k) p ( S_{k+1} | S_k, A_k)}{\prod_{k=t}^{T-1} b (A_k | S_k) p ( S_{k+1} | S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{b (A_k | S_k)} \tag{5.3}\]

&lt;p&gt;$p$는 MDP에서 주어지지 않는 한 구하기 가장 어려운 데이터지만, 운이 좋게도 분자와 분모 모두 있기 때문에 약분이 가능합니다. 즉, 두 Policy $\pi$와 $b$의 Importance Sampling Ratio는 두 Policy가 각 State에서 특정 Action을 선택할 확률이 얼마인지만 알면 구할 수 있습니다. 즉, Importance Sampling Ratio는 MDP에 독립적입니다.&lt;/p&gt;

&lt;p&gt;Target Policy에서의 기대 Reward를 추정할 때 주의할 점은, Behavior Policy로 인한 Return인 $G_t$만 주어졌다는 것입니다. 이것으로는 $\mathbb{E} \left[ G_t \mid S_t = s \right] = v_b (s)$을 구할 수 있기 때문에 $v_{\pi}$를 얻을 수 없습니다. 그렇기 때문에 Importance Sampling이 필요한 것입니다. Importance Sampling Ratio $\rho_{t:T-1}$를 추가하면 원하는 $v_{\pi}$를 얻을 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} G_t | S_t = s \right] = v_{\pi} (s) \tag{5.4}\]

&lt;p&gt;이로써 Behavior Policy $b$로 생성된 Episode를 토대로 $v_{\pi} (s)$를 추정하는 Monte Carlo 알고리즘을 만들 준비가 되었습니다. 하나 추가할 점은 앞으로 각 Episode에서의 시간 단계는 이어지게 표현하겠습니다. 예를 들어, 첫 번째 Episode가 $t = 100$에서 종료되었다면, 다음 Episode는 $t = 101$에서 시작하는 방식입니다. 이를 통해 특정 Episode의 특정 단계를 참조하기 위해 시간 단계를 사용할 수 있고, $\mathcal{T}(s)$라는 표기로 State $s$를 방문하는 모든 시간 단계 집합을 정의할 수 있습니다. 이 표기가 유용한 이유는 Every-visit MC 알고리즘을 사용할 때 표현이 간단해지기 때문입니다. 또한 $T(s)$라는 표기를 통해 시간 단계 $t$ 이후 처음으로 Episode가 종료되는 시점을 나타나게 하겠습니다. 이렇게 되면 $G_t$가 시간 단계 $t$ 이후에서 $T(s)$까지의 Return을 의미하게 됩니다. 또한 $\{ G_t \}_{t \in \mathcal{T}(s)}$는 State $s$에 대한 Return을 의미하고, $\{ \rho_{t:T(t)-1} \}_{t \in \mathcal{T}(s)}$는 Importance Sampling Ratio를 의미합니다. 여러 표기가 갑자기 나와서 헷갈리실 수 있는데, 어쨌든 중요한 것은 이게 다 $v_{\pi} (s)$를 간단하게 표현하기 위함입니다. 방금 새로 정의한 표기를 사용하여 $V(s)$를 표현하면 다음과 같습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t}{|\mathcal{T}(s)|} \tag{5.5}\]

&lt;p&gt;식 (5.5)와 같이 Importance Sampling을 단순한 평균으로 계산하는 것을 &lt;span style=&quot;color:red&quot;&gt;Ordinary Importance Sampling&lt;/span&gt;이라고 합니다. 단순한 평균 대신 Weighted Average를 사용할 수도 있는데, 이 때는 &lt;span style=&quot;color:red&quot;&gt;Weighted Importance Sampling&lt;/span&gt;이라고 하며, $V(s)$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t}{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1}} \tag{5.6}\]

&lt;p&gt;식 (5.6)에서 만약 분모가 0인 경우에는 $V(s)$를 그냥 0으로 정의합니다.&lt;/p&gt;

&lt;p&gt;이 두 가지 Importance Sampling를 비교하기 위해 State $s$에서 단 하나의 Return을 관찰한 후 First-visit MC Method를 사용한다고 가정해보겠습니다. Weighted Importance Sampling에서는 ($\Sigma$ 기호가 없어지기 때문에) $\rho_{t:T(t)-1}$가 약분되므로 $V(s)$가 Return $G_t$와 동일해집니다. 단 하나의 Return만 관찰되었기 때문에 이것이 합리적으로 보일 수 있지만, 이것은 $v_b (s)$의 기대값이기 때문에 통계적으로 Bias되었다고 볼 수 있습니다. 반대로 Ordinary Importance Sampling에서는 단 하나의 Return이더라도 $v_{\pi} (s)$의 기대값이기 때문에 Bias되지는 않지만, 값 자체는 극단적일 수 있습니다. 예를 들어 Importance Sampling Ratio가 10이라면 관찰된 Trajactory가 Behavior Policy보다 Target Policy에서 10배 더 가치가 있다고 판단되고, 해당 Episode Trajactory가 Policy를 훌륭하게 대표한다고 가정해도 실제로 관측되는 Reward와는 크게 차이나는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;따라서 First-visit MC Method에서 두 가지 Importance Sampling에 대한 차이는 Bias와 Variance의 차이라고 볼 수 있습니다. Ordinary Importance Sampling은 Bias되지 않지만 Variance이 무한하게 커질 수 있으며, Weighted Importance Sampling은 극단적인 경우에도 Variance가 1이지만(위의 예제) Bias라는 문제가 있습니다. 두 방법 모두 장단점이 있지만, 일반적으로는 Bounded Return 환경에서 Variance가 0으로 수렴하는 Weighted Importance Sampling을 더 선호하는 편입니다.&lt;/p&gt;

&lt;p&gt;Every-visit MC Method에서는 Ordinary Importance Sampling과 Weighted Importance Sampling 모두 Bias되지만 Sample의 수가 증가할수록 Bias가 0에 가까워집니다. 그렇기 때문에 Off-policy에서는 Every-visit MC Method가 선호됩니다. Weighted Importance Sampling을 사용한 Off-policy Every-visit MC 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;incremental-implementation&quot;&gt;Incremental Implementation&lt;/h2&gt;

&lt;p&gt;이번 Section의 이름은 2.4와 동일합니다. 그 때와 마찬가지로 On-policy Monte Carlo Method를 Recurrence Relation으로 표현하는 방법에 대해 알아보겠습니다. Off-policy Monte Carlo Method는 Importance Sampling을 어떻게 구현하는지에 따라 달라집니다. Ordinary Importance Sampling에서의 Return은 식 (5.3)처럼 $\rho_{t:T(t)-1}$의 비율로 조정되고 식 (5.5)처럼 평균화됩니다. 이렇게하면 Section 2.4에서 사용했던 방법을 그대로 사용할 수 있지만, Weighted Importance Sampling일 때 사용하기 위해서는 약간 다른 알고리즘이 필요합니다.&lt;/p&gt;

&lt;p&gt;Return의 Sequence $G_1, G_2, \ldots, G_{n-1}$이 있다고 가정해봅시다. 이 Return들은 모두 동일한 State에서 시작하고 각각 해당하는 무작위 Weight $W_i$를 가집니다. 그렇다면 이것의 Value Function을 다음과 같이 추정할 수 있습니다.&lt;/p&gt;

\[V_n \doteq \frac{\sum_{k=1}^{n-1} W_k G_k}{\sum_{k=1}^{n-1} W_k}, \quad n \ge 2 \tag{5.7}\]

&lt;p&gt;식 (5.7)을 그대로 사용하면 Return $G_n$을 얻을 때마다 $V_n$을 다시 계산해야하는 불편함이 있습니다. Weighted Expectation을 구하기 위해선 Section 2.4에서 다루었던 Recurrence Relation을 약간 변경하여, Cumulative Sum $C_n$를 추가합니다. 먼저 $V_n$의 Recurrence Relation은 다음과 같습니다.&lt;/p&gt;

\[V_{n+1} \doteq V_n + \frac{W_n}{C_n} \left[ G_n - V_n \right] , \quad n \ge 1, \tag{5.8}\]

&lt;p&gt;그리고 $C_n$의 Recurrence Relation은 다음과 같습니다.&lt;/p&gt;

\[C_{n+1} \doteq C_n + W_{n+1}\]

&lt;p&gt;이 때, $C_0 \doteq 0$으로 정의합니다. 이것을 반영한 Off-policy Monte Carlo Prediction 알고리즘은 다음과 같습니다. 아래 알고리즘은 Weighted Importance Sampling으로 구현되었습니다. 만약 On-policy로 바꾸고 싶다면, Target Policy와 Behavior Policy가 같다는 뜻이므로 ($\pi = b$), $W = 1$로 설정하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;off-policy-monte-carlo-control&quot;&gt;Off-policy Monte Carlo Control&lt;/h2&gt;

&lt;p&gt;이제 Off-policy Monte Carlo Prediction을 완성했으니, Control 방법을 제시할 준비가 되었습니다. On-policy에서는 Control를 위해 Policy의 Value를 추정하면서 Policy를 사용했으나, Off-policy는 두 기능이 분리된다는 차이점이 있습니다. 그렇기 때문에 Behavior Policy는 Action을 선택하는데만 사용하고, Target Policy는 Policy를 Evaluation 및 Improvement하는데 이 둘이 전혀 연관이 없을 수 있습니다. 이렇게 구분하게 되면 각 Action의 Value에 상관 없이 모든 Action을 계속 Sampling할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy Monte Carlo Control 방법은 앞에 두 Section에서 제안한 기법 중 하나를 사용합니다. 이 때, Behavior Policy는 모든 State의 모든 Action에 대해서 선택할 확률이 0보다 커야합니다. (=Soft)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 의사 코드는 $\pi_{*}$와 $q_{*}$를 추정하기 위해 GPI와 Weighted Importance Sampling을 기반으로 하는 Off-policy Monte Carlo Control 알고리즘입니다. Target Policy $\pi \approx \pi_{*}$는 $q_{\pi}$의 추정치인 Q에 대해 Greedy한 Policy이고, Behavior Policy $b$는 $\epsilon$-soft로 선택함으로써 $\pi$의 수렴을 Optimal Policy로 보장하였습니다. 따라서 Policy $\pi$는 Policy $b$에 따라 Action을 선택하더라도 최적으로 수렴하는 것이 보장됩니다.&lt;/p&gt;

&lt;p&gt;이 Control 방법의 문제는 Episode의 나머지 Action이 Greedy일 때 Episode의 마지막 부분에서만 학습이 된다는 것입니다. 만약 대부분의 Action이 greedy가 아니라면 학습 속도는 그만큼 느릴 수밖에 없습니다. 특히 Episode의 길이가 길수록 이 문제는 더욱 심각해집니다. 이것을 해결하기 위한 방법은 다음 장에서 주요하게 다룰 예정입니다. 지금은 $\gamma$가 1보다 작은 특수한 경우, 이것을 해결할 수 있는 간단한 방법을 다음 두 Section에 걸쳐 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;discounting-aware-importance-sampling&quot;&gt;Discounting-aware Importance Sampling&lt;/h2&gt;

&lt;p&gt;지금까지 논의했던 Off-policy 방법은 Discount와 같은 Return의 구조를 고려하기 보단, Importance Sampling Ratio를 계산하는 것에 중점을 두었습니다. 이제는 이것을 토대로 Off-policy Prediction에서의 Variance를 줄이기 위한 아이디어를 논의해 보겠습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, Episode의 길이가 100단계이고 $\gamma = 0$인 상황이라고 가정하겠습니다. 그렇다면 $t=0$에서의 Return은 $G_0 = R_1$이지만, Importance Sampling Ratio는 $\frac{\pi (A_0 \mid S_0)}{b (A_0 \mid S_0)} \frac{\pi (A_1 \mid S_1)}{b (A_1 \mid S_1)} \cdots \frac{\pi (A_99 \mid S_99)}{b (A_99 \mid S_99)}$ 와 같이 100 항의 곱셉으로 이루어져 있습니다. Ordinary Importance Sampling에서의 Return은 실제로 첫 번째 항인 $\frac{\pi (A_0 \mid S_0)}{b (A_0 \mid S_0)}$외에는 독립적이지만, 대신 Variance가 엄청나게 커질 수 있습니다. 지금부터 이 Variance를 피하기 위한 아이디어를 논의하겠습니다.&lt;/p&gt;

&lt;p&gt;아이디어의 핵심은 Discounting을 Episode가 종료될 확률이나 부분적으로 종료될 확률로 생각하는 것입니다. 임의의 $\gamma \in \left[ 0, 1 \right)$에 대해, Episode는 $t=1$에서 $1 - \gamma$ 확률로 부분 종료될 수 있습니다. 이 때의 Return은 $R_1$이 됩니다. 마찬가지로 $t=2$에서 Episode가 부분 종료될 확률은 $(1 - \gamma) \gamma$이며, 이 때의 Return은 $R_1 + R_2$가 되는 방식입니다. 이런 방식으로 얻은 Return을 &lt;span style=&quot;color:red&quot;&gt;Flat Partial Return&lt;/span&gt;이라고 합니다. 임의의 시간 $t$부터 $h$까지의 Flat Partial Return은 다음과 같이 정의됩니다.&lt;/p&gt;

\[\bar{G}_{t:h} \doteq R_{t+1} + R_{t+2} + \cdots + R_{h}, \quad 0 \le t &amp;lt; h \le T\]

&lt;p&gt;Flat Partial Return에서 &lt;strong&gt;Flat&lt;/strong&gt;은 Discounting이 없다는 것을 의미하고, &lt;strong&gt;Partial&lt;/strong&gt;은 Return이 Episode가 종료되기 전인 Horizon $h$에서 중지된다는 것을 의미합니다. 기존의 전체 Return $G_t$는 다음과 같이 Flat Partial Return의 합계로 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp; \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \gamma^{T-t-1} R_T \\ \\
&amp;amp;= (1 - \gamma) R_{t+1} + (1 - \gamma) \gamma (R_{t+1} + R_{t+2}) + (1 - \gamma) \gamma^2 (R_{t+1} + R_{t+2} + R_{t+3}) \\ \\
&amp;amp; \cdots + (1 - \gamma) \gamma^{T-t-2} (R_{t+1} + R_{t+2} + \cdots +  R_{T-1}) + \gamma^{T-t-1} (R_{t+1} + R_{t+2} + \cdots +  R_{T}) \\ \\
&amp;amp;= (1 - \gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \bar{G}_{t:h} + \gamma^{T-t-1} \bar{G}_{t:T}
\end{align}\]

&lt;p&gt;Flat Partial Return을 제대로 사용하기 위해서는 Importance Sampling Ratio 또한 적절하게 조절해야 합니다. Return $\bar{G}_{t:h}$는 $h$까지의 Reward만 포함하기 때문에 $h-1$까지의 Ratio를 구하면 됩니다. 식 (5.5)와 유사하게, Ordinary Importance Sampling은 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} \bar{G}_{t:h} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \bar{G}_{t:T(t)} \right)}{|\mathcal{T}(s)|} \tag{5.9}\]

&lt;p&gt;Weighted Importance Sampling의 경우에는 식 (5.6)과 유사하게 다음처럼 정의할 수 있습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} \bar{G}_{t:h} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \bar{G}_{t:T(t)} \right)}{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \right)} \tag{5.10}\]

&lt;p&gt;이렇게 $V(s)$를 추정하는 것을 &lt;span style=&quot;color:red&quot;&gt;Discounting-aware Importance Sampling Estimator&lt;/span&gt;라고 부릅니다. 가장 큰 특징은 이 추정 방법은 Discount Factor를 고려하지만 $\gamma = 1$인 경우는 효과가 없다는 것입니다. (Section 5.5의 Off-policy 추정과 동일함)&lt;/p&gt;

&lt;h2 id=&quot;per-decision-importance-sampling&quot;&gt;Per-decision Importance Sampling&lt;/h2&gt;

&lt;p&gt;Off-policy Importance Sampling에서 Discounting이 없는 경우(즉, $\gamma = 1$)에도 Variance를 줄일 수 있는 또 다른 방법이 있습니다. Off-Policy를 나타낸 식 (5.5)와 (5.6)에서 분자에 있는 합계의 각 항은, 각각이 Reward에 대한 Return으로 표현될 수 있습니다.&lt;/p&gt;

\[\begin{align}
\rho_{t:T-1} G_t &amp;amp;= \rho_{t:T-1} \left( R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t-1} R_T \right) \\ \\
&amp;amp;= \rho_{t:T-1} R_{t+1} + \gamma \rho_{t:T-1} R_{t+2} + \cdots + \gamma^{T-t-1} \rho_{t:T-1} R_T \tag{5.11}
\end{align}\]

&lt;p&gt;위의 식 (5.11)을 보시면 각각의 항은 Return과 Importance Sampling Ratio의 곱으로 이루어져 있습니다. 즉, 이것은 Importance Sampling Ratio 비율을 가중치로 사용한 Weighted Average라고 볼 수도 있습니다. 이것을 더 간단한 방법으로 표현하기 위해서는, 식 (5.3)을 이용할 수 있습니다. 예를 들어, 식 (5.3)을 사용하면 첫 번째 항을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\rho_{t:T-1} R_{t+1} = \frac{\pi (A_t | S_t)}{b (A_t | S_t)} \frac{\pi (A_{t+1} | S_{t+1})}{b (A_{t+1} | S_{t+1})} \frac{\pi (A_{t+2} | S_{t+2})}{b (A_{t+2} | S_{t+2})} \cdots \frac{\pi (A_{T-1} | S_{T-1})}{b (A_{T-1} | S_{T-1})} R_{t+1} \tag{5.12}\]

&lt;p&gt;식 (5.12)는 매우 길지만, 사실 이렇게 많은 항의 곱셈 중에서 첫 번째 항과 마지막 항(=$R_{t+1}$)만 유효합니다. 왜냐하면 나머지는 어차피 모두 Return을 얻은 후에 발생한 이벤트이기 때문입니다. 나머지 항에 대한 기대값은 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \frac{\pi (A_k | S_k}{b (A_k | S_k} \right] \doteq \sum_a b (a|S_k) \frac{\pi (a|S_k)}{b (a|S_k)} = \sum_a \pi (a|S_k) = 1 \tag{5.13}\]

&lt;p&gt;이런 식으로 몇 가지 단계를 더 거치면 왜 첫 번째 항과 마지막 항 이외에는 유효하지 않은지 알 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} R_{t+1} \right] = \mathbb{E} \left[ \rho_{t:t} R_{t+1} \right] \tag{5.14}\]

&lt;p&gt;식 (5.11)의 $k$번째 항에 대해 이 과정을 반복하면 다음을 얻습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} R_{t+k} \right] = \mathbb{E} \left[ \rho_{t:t+k-1} R_{t+k} \right]\]

&lt;p&gt;따라서 식 (5.11)의 기대값은 다음과 같이 간단하게 표현 가능합니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} G_t \right] = \mathbb{E} \left[ \tilde{G}_k \right],\]

\[\text{where  } \tilde{G}_t = \rho_{t:t} R_{t+1} + \gamma \rho_{t:t+1} R_{t+2} + \gamma^2 \rho_{t:t+2} R_{t+3} + \cdots + \gamma^{T-t-1} \rho_{t:T-1} R_T\]

&lt;p&gt;위와 같은 식을 &lt;span style=&quot;color:red&quot;&gt;Per-decision Importance Sampling&lt;/span&gt;이라고 부릅니다. 이렇게 $\bar{G}_t$를 사용하면 식 (5.5)와 같이 Bias가 없어지는 $V(s)$의 추정이 나오는데, 식은 다음과 같습니다. (First-visit 이라고 가정)&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \bar{G}_t }{|\mathcal{T}(s)|} \tag{5.15}\]

&lt;p&gt;식 (5.15)는 Ordinary Importance Sampling으로부터 유도된 Per-decision Importance Sampling입니다. 이와 비슷한 방법으로 Weighted Importance Sampling으로부터 Per-decision Importance Sampling을 유도할 수 있을 것 같지만, 대부분의 경우 일관성이 없기 때문에 (무한 개의 데이터가 주어졌을 때, 올바른 값으로 수렴하지 않음) 존재가 명확하지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서 다룬 Monte Carlo Method는 Episode라는 경험을 통해 Value Function과 Optimal Policy를 학습합니다. 이것이 DP보다 우수한 장점으로는 ① Environment에 대한 Model이 없어도 Environment와의 상호 작용을 통해 최적의 Action을 학습할 수 있고, ② 시뮬레이션이나 Sample Model과 함께 사용할 수 있으며, ③ State의 작은 부분 집합에 집중하는 것이 쉽고 효율적이라는 것입니다. (8장 참고)&lt;/p&gt;

&lt;p&gt;이번 장에서 다루지는 않았지만, 추후 소개할 4번째 장점으로는 Markov Property를 위반했을 때 문제가 작다는 것입니다. 이것은 후속 State의 추정한 Value를 기반으로 현재의 Value를 추정하지 않기 때문입니다. (=Bootstrapping하지 않기 때문입니다)&lt;/p&gt;

&lt;p&gt;Monte Carlo Control은 4장에서 소개했던 Generalized Policy Iteration (GPI)를 기반으로 설계하였습니다. GPI는 Policy Evaluation과 Policy Improvement의 상호 작용 과정을 포함합니다. Monte Carlo Method는 Model을 사용하는 대신 해당 State에서 시작했을 때 얻을 수 있는 수익을 평균화함으로써 대안적인 Policy Evaluation 방법을 제공합니다. 또한 Environment의 Transition Probability를 필요로 하지 않고 Policy를 개선할 수 있기 때문에 Action-Value Function을 추정하는데 유리합니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Control에서는 충분한 탐색을 할 수 있도록 유지하는 것이 중요한 이슈입니다. 왜냐하면 현재 시점에서 가장 좋아보이는 Action이 궁극적으로 최선의 Action이 아닐 수 있기 때문입니다. 이것을 해결하기 위해 처음으로 생각할 수 있는 방법은 Episode가 무작위 State-Action에서 시작한다고 가정하는 것입니다. 하지만 이러한 Assumption of Exploring Starts는 실제 상황에서 적용하기 어렵습니다. 따라서 이에 대한 대안으로 Episode를 탐색하며 Optimal Policy를 찾는 On-policy 방법과, Target Policy와 Behavior Policy를 구분하여 Optimal Policy를 찾는 Off-policy 방법이 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법은 Behavior Policy에 의해 생성된 데이터로부터 Target Policy의 Value Function을 학습합니다. 이 학습 방법은 다른 표본(Behavior Policy)으로부터 생성된 데이터를 학습(Target Policy)에 사용하므로 Importance Sampling을 사용하여 Return에 가중치를 두는 방식으로 값을 보정합니다. 이 때 사용하는 방법은 두 가지가 있는데, Ordinary Importance Sampling은 단순한 평균을 사용하지만, Weighted Importance Sampling은 Weighted Avaerage를 사용한다는 차이가 있습니다. Ordinary Importance Sampling은 추정값이 Bias되지 않는다는 장점이 있지만, Variance가 크고 심지어 무한할 수도 있는 단점이 있는 반면, Weighted Importance Sampling은 추정값이 Bias될지언정 항상 유한한 Variance를 가진다는 장점이 있습니다. Off-policy 방법은 개념적으로 단순하지만, Prediction과 Control 모두 불안정하며 발전 가능성이 있기 때문에 지속적으로 연구되고 있습니다.&lt;/p&gt;

&lt;p&gt;5장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서는 지난 장과 마찬가지로 Value Function을 추정하고 Optimal Policy를 찾기 위한 방법을 다루지만, 지난 장과는 달리 MDP에 대한 완전한 정보를 알고 있다고 가정하지 않습니다. Monte Carlo Method는 Environment와의 상호 작용을 통해 얻은 경험을 기반으로 Optimal Policy를 찾는 방법입니다. 이 때 Environment와의 상호작용은 실제로 이루어지는 경험 뿐만이 아니라 시뮬레이션된 경험이라도 상관 없습니다.</summary></entry><entry><title type="html">Dynamic Programming</title><link href="http://localhost:4000/studies/dynamic-programming/" rel="alternate" type="text/html" title="Dynamic Programming" /><published>2022-02-22T00:00:00+09:00</published><updated>2022-02-22T00:00:00+09:00</updated><id>http://localhost:4000/studies/dynamic-programming</id><content type="html" xml:base="http://localhost:4000/studies/dynamic-programming/">&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Dynamic Programming&lt;/span&gt;은 Markov Decision Process (MDP)와 같이 완벽한 Environment Model이 주어졌을 때 Optimal Policy을 계산할 수 있는 알고리즘입니다. Dynamic Programming은 학부 알고리즘 수업에서도 다루는 중요한 알고리즘이지만, 완벽한 Model이 주어져야 한다는 가정과 막대한 계산 비용으로 인해 강화학습에 직접적으로 적용하기는 힘든 단점이 있습니다. 다만 Dynamic Programming으로 강화학습 문제를 해결하는 과정은 다른 강화학습 해결 방법을 이해하는데 큰 도움이 되기 때문에 반드시 짚고 넘어가야 합니다.&lt;/p&gt;

&lt;p&gt;Dynamic Programming을 시작하기 전에, 일단 주어진 Environment가 Finite MDP라고 가정합시다. 지난 장에서 배운 것처럼 Finite MDP는 State, Action, Reward이 유한하고 모든 State $s$와 Action $a$에 대해 Transition Probability $p ( s’, r \mid s, a )$가 제공된 Environment를 말합니다.&lt;/p&gt;

&lt;p&gt;강화학습 문제를 Dynamic Programming으로 해결하는 핵심 아이디어는 지난 장에서 배운 Value Function를 계산하여 좋은 Policy을 찾는 것입니다. 즉, Bellman Optimality Equation을 만족하는 최적의 Value Function $v_{*}$ 또는 $q_{*}$를 Dynamic Programming을 사용해 계산하는 것을 보여드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-evaluation-prediction&quot;&gt;Policy Evaluation (Prediction)&lt;/h2&gt;

&lt;p&gt;먼저 임의의 Policy $\pi$에 대해, State-Value Function $v_{\pi}$를 계산하는 방법을 생각해봅시다. Dynamic Programming에서는 이것을 &lt;span style=&quot;color:red&quot;&gt;Policy Evaluation&lt;/span&gt;이라고 합니다. 또 다른 말로는 &lt;span style=&quot;color:red&quot;&gt;Prediction Problem&lt;/span&gt;라고도 합니다. 지난 장에서 State-Value Function는 다음과 같이 전개했었습니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} &amp;amp; \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \tag{from (3.9)} \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) \mid S_t = s \right] \tag{4.3} \\ \\
&amp;amp;= \sum_a \pi (a \mid s) \sum_{s&apos;, r} p (s&apos;, r \mid s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right] \tag{4.4}
\end{align}\]

&lt;p&gt;식 (4.4)에서 $\pi (a \mid s)$는 Policy $\pi$에 따라 State $s$에서 Action $a$를 선택할 확률입니다. 기대값 $\mathbb{E}$에서 $\pi$를 붙이는 이유는 Policy $\pi$를 따르기 때문입니다. 만약 $\gamma &amp;lt; 1$이거나 끝이 보장된다면 $v_{\pi}$ 또한 Policy $\pi$에 대해 유일하게 존재한다는 것이 보장됩니다.&lt;/p&gt;

&lt;p&gt;Environment를 완벽하게 알 수 있다면 식 (4.4)는 $v_{\pi}$에 대한 연립 일차방정식으로 해결할 수 있습니다. 여기서는 $v_{\pi}$에 대한 근사값을 반복적으로 계산합니다. 초기 근사값 $v_0$를 임의로 정의한 후, 다음과 같이 Bellman Equation의 업데이트 규칙을 사용하여 정확한 $v_{\pi}$에 수렴하게 만들 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{k+1} (s) &amp;amp; \doteq \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k (S_{t+1}) | S_t = s \right] \\ \\
&amp;amp;= \sum_a \pi (a | s) \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_k (s&apos;) \right] \tag{4.5}
\end{align}\]

&lt;p&gt;Sequence $\{ v_k \}$는 $k \to \infty$일 때 $v_{\pi}$에 수렴합니다. 식 (4.5)와 같은 방법을 &lt;span style=&quot;color:red&quot;&gt;Iterative Policy Evaluation&lt;/span&gt;이라고 합니다. 구체적인 Iterative Policy Evaluation의 알고리즘은 아래의 Pseudocode를 읽어주시기 바랍니다. 유의하실 점으로 실제 Iterative Policy Evaluation 알고리즘은 이론과 달리 무한히 반복하지 않습니다. 대신 $v_{k+1} (s) - v_{k} (s)$가 임의의 작은 실수 $\theta &amp;gt; 0$보다 작으면 끝나게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;policy-improvement&quot;&gt;Policy Improvement&lt;/h2&gt;

&lt;p&gt;4.1절에서 Policy에 대한 Value Function을 계산했던 이유는 더 나은 Policy를 찾기 위함입니다. 임의의 Deterministic Policy $\pi$에 대해 Value Function $v_{\pi}$를 구했다면, 이제는 Policy를 변경할 것인지를 고민해봐야합니다. 간단하게 기존 Policy에 대한 Value를 평가했었기 때문에, Policy를 변경했을 때 Value가 높아지는지 그렇지 않은지를 확인해보면 됩니다. 현재 State $s$에서 Action을 $a$로 변경했을 때 Value는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
q_{\pi} (s, a) &amp;amp; \doteq \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = a \right] \\ \\
&amp;amp;= \sum_{s&apos;, r} p( s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s^{\prime}) \right] \tag{4.6}
\end{align}\]

&lt;p&gt;식 (4.6)에서의 핵심은 이것이 과연 $v_{\pi} (s)$ 보다 큰가입니다. 만약 식 (4.6)이 더 크다면 State $s$에 도달할 때마다 Action $a$를 선택하면 기존 Policy보다 더 낫기 때문입니다. 결론부터 말씀드리면 이것은 &lt;span style=&quot;color:red&quot;&gt;Policy Improvement Theorem&lt;/span&gt;라는 특별한 상황을 만족시킬 경우 참이 됩니다. Policy Improvement Theorem란 모든 State $s \in \mathcal{S}$에 대해, 서로 다른 Deterministic Policy $\pi$와 $\pi^{\prime}$가 주어졌을 때&lt;/p&gt;

\[q_{\pi} (s, \pi^{\prime}(s)) \ge v_{\pi}(s) \tag{4.7}\]

&lt;p&gt;를 만족한다면 Policy $\pi^{\prime}$는 Policy $\pi$보다 좋거나 같다는 정리입니다. 즉, 다음과 같이 모든 State $s \in \mathcal{S}$에 대해 기대 Reward가 좋거나 같아야 합니다.&lt;/p&gt;

\[v_{\pi^{\prime}}(s) \ge v_{\pi}(s) \tag{4.8}\]

&lt;p&gt;만약 식 (4.7)에서 $\ge$가 아니라 $\gt$를 만족한다면, 식 (4.8) 또한 $\gt$를 만족합니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement Theorem는 두 Policy $\pi$와 $\pi^{\prime}$에서 State $s$를 제외한 상황에서 동일한 경우에도 적용됩니다. 식 (4.7)은 양쪽 항이 같을 경우에도 만족하기 때문입니다. 따라서 정확히 하나의 State $s$에서만 $q_{\pi} (s, a) &amp;gt; v_{\pi}(s)$를 만족한다고 해도, Policy $\pi^{\prime}$가 Policy $\pi$보다 낫다고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement Theorem는 다음과 같이 전개함으로써 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Policy Improvement Theorem)&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp; \le q_{\pi} (s, \pi &apos;(s)) \\ \\
&amp;amp;= \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = \pi &apos;(s) \right] \tag{by (4.6)} \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s \right] \\ \\
&amp;amp;\le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma q_{\pi} (S_{t+1}, \pi &apos; (S_{t+1})) | S_t = s \right] \tag{by (4.7)} \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma \mathbb{E} \left[ R_{t+2} + \gamma v_{\pi} (S_{t+2}) | S_{t+1}, A_{t+1} = \pi &apos; (S_{t+1}) \right] | S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi} (S_{t+2}) | S_t = s \right] \\ \\
&amp;amp;\le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) | S_t = s \right] \\
&amp;amp;\qquad \qquad \qquad \qquad \qquad \vdots \\
&amp;amp; \le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots | S_t = s \right] \\ \\
&amp;amp;= v_{\pi &apos;} (s)
\end{align}\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이렇게 Policy와 Value Function이 주어지면 특정한 단일 State에서 Policy를 변경했을 때 그 Value를 쉽게 평가할 수 있는 방법을 알게 되었습니다. 이 개념을 확장하여 각각의 State에서 $q_{\pi}(s, a)$에 따라 가장 좋은 Action을 선택하는 Greedy한 Policy를 $\pi^{\prime}$이라고 했을 때, 새로운 Greedy Policy $\pi^{\prime}$는 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\pi &apos; (s) &amp;amp; \doteq \underset{a}{\operatorname{argmax}} q_{\pi} (s, a) \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = a \right] \tag{4.9} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right]
\end{align}\]

&lt;p&gt;이러한 Greedy Policy는 $v_{\pi}$에 따라 단기적으로 가장 좋은 Action을 선택하는 방식으로 Policy Improvement Theorem (4.7)의 조건을 만족합니다. 이렇게 기존 Policy의 Value Function을 사용하여 기존 Policy를 개선하는 새로운 Policy를 만드는 과정을 &lt;span style=&quot;color:red&quot;&gt;Policy Improvement&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;만약 새로운 Greedy Policy $\pi^{\prime}$이 이전 Policy인 $\pi$만큼 좋지만, 더 좋지는 않다고 가정하면, 식 (4.9)로부터 $v_{\pi} = v_{\pi^{\prime}}$ 이고 모든 State $s \in \mathcal{S}$에 대해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{\pi &apos;}(s) &amp;amp;= \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi &apos;} (S_{t+1}) | S_t = s, A_t = a \right] \\ \\
&amp;amp;= \max_a \sum_{s&apos;, r} p(s&apos;, r | s, a) \left[ r + \gamma v_{\pi&apos;} (s&apos;) \right]
\end{align}\]

&lt;p&gt;위 식은 Bellman Optimality Theorem과 동일하므로 $v_{\pi’}$는 $v_{*}$와 동일함을 알 수 있습니다. 즉, Policy $v_{\pi}$와 $v_{\pi’}$는 모두 Optimal Policy입니다. 따라서 Policy Improvement은 원래 Policy가 Optimal인 경우를 제외하고는 반드시 더 나은 Policy를 제공해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 모두 Deterministic Policy만을 고려했으나, Stochastic Policy에서도 Policy Improvement Theorem은 동일하게 적용됩니다. 위의 그림은 GridWorld Environment에서 반복적으로 Policy Evaluation를 시행함으로써 Stochastic Policy에서 Optimal Policy을 찾는 과정을 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;주어진 Environment는 모든 Action에 대해 Reward가 $R_t = -1$로 주어져 있기 때문에 최대한 빨리 출발점(왼쪽 맨위)에서 도착점(오른쪽 맨아래)에 도달하는 것이 목표입니다. 가능한 Action은 $\text{UP}$, $\text{DOWN}$, $\text{LEFT}$, $\text{RIGHT}$ 4방향이며 초기 Policy $\pi$는 모든 방향에 대해 동일한 확률 $0.25$로 Action을 선택합니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement를 위해서는 왼쪽의 State-Value Function을 토대로 Greedy하게 Policy를 선택합니다. State-Value Function은 식 (4.5)를 사용하여 계산합니다. 예를 들어, 맨 윗줄에서 왼쪽 두 번째 State를 $s_1$이라 하면, $k=1$에서의 State-Value Function $v_1$은  $v_1 (s_1) =$ $\pi (\text{RIGHT} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{LEFT} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{UP} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{DOWN} \mid s_1) \cdot \left[-1 + 0 \right]$ $= -1$ 와 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;사실 주어진 Environment에서 State $s_1$에서 Action $\text{UP}$은 불가능하지만, 여기서는 일단 무시하고 넘어가겠습니다. 이러한 방법으로 모든 시간 단계 $k$ 마다 State-Value Function Table을 갱신할 수 있다라는 것만 이해하시면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Policy Iteration&lt;/span&gt;은 지금까지 배운 Policy Evaluation과 Policy Improvement를 반복하여 Optimal Policy를 찾는 방법입니다. 이 과정을 표현하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 E는 Policy Evaluation, I는 Policy Improvement를 의미합니다. Optimal Policy가 아닌 이상 새로 만들어지는 Policy는 이전의 Policy보다 확실히 우수함을 보장하며 Finite MDP에는 유한한 수의 Deterministic Policy만 있기 때문에 이 과정은 유한한 반복 횟수로 Optimal Policy와 최적의 Value Function으로 수렴되어야 합니다. Policy Iteration 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Policy Iteration에 대한 예제가 교재에 몇 개 나와있긴 하지만, 여기서는 그보다 더 간단한 예제를 통해 Policy Iteration이 Optimal Policy에 수렴하는 것을 보여드리도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Example) State와 Action이 각각 $\mathcal{S} = \{ s_1, s_2 \}$, $\mathcal{A} (s_1) = \{ a, b \}$, $\mathcal{A} (s_2) = \{ c \}$로 주어져 있다. Action $a$는 0.5의 확률로 5의 Reward를 받고 다음 State가 $s_1$으로 되며, 0.5의 확률로 5의 Reward를 받고 다음 State가 $s_2$가 된다. Action $b$는 1의 확률로 10의 Reward를 받고 다음 State가 $s_2$가 된다. 또한 Action $c$는 1의 확률로 -1의 Reward를 받고 다음 State가 $s_2$가 된다. 이를 그림으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Policy Iteration에서 Discount factor $\gamma$는 0.95로 설정되어 있으며, 초기 Policy $\pi_0$는 $\pi_0 (s_1) = b$, $\pi_0 (s_2) = c$로 주어져 있다. 이를 사용하여 Optimal Policy를 찾야아 한다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Solution)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi_0} (s_1) &amp;amp;= 10 + 0.95 \cdot v_{\pi_0} (s_2) \\ \\
v_{\pi_0} (s_2) &amp;amp;= -1 + 0.95 \cdot v_{\pi_0} (s_2)
\end{align}\]

&lt;p&gt;두 식을 $v_{\pi_0} (s_1)$, $v_{\pi_0} (s_2)$에 대해 연립방정식을 풀면, $v_{\pi_0} (s_1) = -9$, $v_{\pi_0} (s_2) = -20$을 구할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2. Policy Improvement&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
\pi_1 (s_1) &amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ 5 + 0.475 \cdot v_{\pi_0} (s_1) + 0.475 \cdot v_{\pi_0} (s_2) , 10 + 0.95 \cdot v_{\pi_0} (s_2) \right\} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ -0.8775, -9 \right\}
\end{align}\]

&lt;p&gt;Policy Improvement를 통해 새로운 Policy를 계산 $\Rightarrow \pi_1 (s_1) = a, \pi_1 (s_2) = c$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3. Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi_1} (s_1) &amp;amp;= 5 + 0.475 \cdot v_{\pi_1} (s_1) + 0.475 \cdot v_{\pi_1} (s_2) \\ \\
v_{\pi_1} (s_2) &amp;amp;= -1 + 0.95 \cdot v_{\pi_1} (s_2)
\end{align}\]

&lt;p&gt;또 다시 $v_{\pi_1} (s_1)$, $v_{\pi_1} (s_2)$에 대해 연립방정식을 풀면, $v_{\pi_1} (s_1) \approx -8.57$, $v_{\pi_1} (s_2) = -20$을 구할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4. Policy Improvement&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
\pi_2 (s_1) &amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ 5 + 0.475 \cdot v_{\pi_1} (s_1) + 0.475 \cdot v_{\pi_1} (s_2) , 10 + 0.95 \cdot v_{\pi_1} (s_2) \right\} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ -12.595, -9 \right\}
\end{align}\]

&lt;p&gt;Policy Improvement를 통해 새로운 Policy를 계산 $\Rightarrow \pi_2 (s_1) = a, \pi_2 (s_2) = c$&lt;/p&gt;

&lt;p&gt;모든 State $s_1$, $s_2$에 대해 $\pi_2 (s_1) = \pi_1 (s_1)$, $\pi_2 (s_2) = \pi_1 (s_2)$가 성립하므로 &lt;strong&gt;Policy-stable&lt;/strong&gt;. 따라서 Policy Iteration이 종료된다.&lt;/p&gt;

\[\therefore \pi_{*} (s_1) = a, \pi_{*} (s_2) = c\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h2&gt;

&lt;p&gt;Policy Iteration의 단점은 매 시간 단계마다 Policy Evaluation이 포함된다는 것입니다. 위에 보여드린 예제에서는 State가 단 2개였기 때문에 Policy Evaluation이 비교적 간단하였으나, 복잡한 문제에서는 이 과정 자체가 매우 오래 걸릴 수 있습니다. 만약 Policy Iteration을 무한히 반복한다면, GridWorld 예제처럼 $k=3$ 이후로 Policy가 변하지 않는데도 계속 Policy Evaluation을 수행하는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;Policy Iteration에서의 수렴 보장을 유지하면서 Policy Evaluation 단계를 줄일 수도 있습니다. 여러 방법 중 각 State를 단 한번만 업데이트 하는 특별한 방법을 &lt;span style=&quot;color:red&quot;&gt;Value Iteration&lt;/span&gt;이라고 부릅니다. 이 때, Policy Improvement와 줄여낸 Policy Evaluation을 결합하여 다음과 같이 Value Function을 업데이트할 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{k+1} (s) &amp;amp;\doteq \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k (S_{t+1}) | S_t = s , A_t = a \right] \\ \\
&amp;amp;= \max_a \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_k (s&apos;) \right] \tag{4.10}
\end{align}\]

&lt;p&gt;식 (4.10)은 모든 State $s \in \mathcal{S}$에 대해 성립하며, 수열 $\{ v_k \}$는 임의의 $v_0$에서부터 $v_{*}$로 수렴합니다.&lt;/p&gt;

&lt;p&gt;Value Iteration을 쉽게 설명하면 단순히 Bellman Optimality Equation을 업데이트 규칙으로 바꾼 것입니다. 매 시간 단계마다 Value Iteration의 업데이트가 항상 최대값을 취해야 한다는 점을 제외하면 식 (4.5)와 매우 유사함을 알 수 있습니다. Value Iteration은 Policy Iteration과 마찬가지로 $v_{*}$에 정확히 수렴하기 위해서는 무한히 반복해야 합니다. 다만 실제 Value Iteration 알고리즘에서는 무한히 반복할 수 없기 때문에 Policy Iteration 알고리즘과 마찬가지로 종료 조건이 있습니다. Value Iteration 알고리즘에서는 Value Function의 변동이 매우 작을 때(임의의 작은 값 $\triangle$) 종료됩니다. 전체 Value Iteration 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Value Iteration은 Policy Iteration과 다르게 Evaluation과 Improvement 과정이 한번에 일어납니다. 따라서 구현 방법은 Value Iteration이 더 간단합니다. 이렇게 보면 Value Iteration이 Policy Iteration보다 우수해보이지만, 실제로는 그렇지 않습니다. Value Iteration은 각각의 단계가 짧은 대신, 그만큼 계산량이 더 많습니다. 또한 Policy 자체를 업데이트하는 Value Iteration은 Value Function을 먼저 구한 다음 Policy를 구하기 때문에 두 알고리즘을 비교했을 때 평균적으로 Policy Iteration이 더 빠르게 수렴합니다.&lt;/p&gt;

&lt;p&gt;Value Iteration도 마찬가지로 간단한 예제를 통해 실제로 어떻게 Optimal Policy을 구할 수 있는지 알아보도록 하겠습니다. 예제는 Policy Iteration에서 사용했던 예제를 그대로 사용해서, 해결 방법만 Value Iteration으로 바꾸어 풀어보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Solution)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Initialization&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_0 (s_1) &amp;amp;= 0 \\ \\
v_0 (s_2) &amp;amp;= 0 \\ \\
\triangle &amp;amp;= 0.1
\end{align}\]

&lt;p&gt;&lt;strong&gt;Step 2. 1st Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_1 (s_1) &amp;amp;= \max \left\{ 0.5 \cdot (5 + 0.95 \cdot v_0 (s_1) + 0.5 \cdot (5 + 0.95 \cdot v_0 (s_2), 10 + 0.95 \cdot v_0 (s_2) \right\} \\ \\
v_1 (s_2) &amp;amp;= \max \left\{ -1 + 0.95 \cdot v_0 (s_2) \right\} \\ \\
&amp;amp;\Rightarrow v_1 (s_1) = 10, v_1 (s_2) = -1
\end{align}\]

&lt;p&gt;$v_1 (s_1) - v_0 (s_1) = 10 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3. 2nd Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_2 (s_1) &amp;amp;= \max \left\{ 9.275, 9.05 \right\} \\ \\
v_2 (s_2) &amp;amp;= \max \left\{ -1.95 \right\} \\ \\
&amp;amp;\Rightarrow v_2 (s_1) = 9.275, v_2 (s_2) = -1.95
\end{align}\]

&lt;p&gt;$v_2 (s_1) - v_1 (s_1) = 0.725 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4. 3rd Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_3 (s_1) &amp;amp;= \max \left\{ 8.48, 8.15 \right\} \\ \\
v_3 (s_2) &amp;amp;= \max \left\{ -2.85 \right\} \\ \\
&amp;amp;\Rightarrow v_3 (s_1) = 8.48, v_3 (s_2) = -2.85
\end{align}\]

&lt;p&gt;$v_3 (s_1) - v_2 (s_1) = 0.795 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;이런식으로 계속 Step을 밟아가며 $v_{t+1} - v_t &amp;lt; 0.1$이 될 때 까지 반복합니다. Step이 너무 길어져서 임의로 Step 4에서 끝났다고 가정하면, Policy를 구하는 것은 마지막 Step (여기서는 Step 4)에서 Value Function을 최대화했던 Action, 즉 $s_1$에서 $a$, $s_2$에서 c가 됩니다. 따라서 Optimal Policy는 Policy Iteration과 마찬가지로 $ \pi_{*} (s_1) = a, \pi_{*} (s_2) = c$ 입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;위의 예제에서 원래는 종료 조건을 확인할 때, $s_1$에 대한 Value Function 뿐만 아니라 $s_2$에 대한 Value Function까지 확인해야 하지만, 편의상 $s_1$만 확인하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;asynchronous-dynamic-programming&quot;&gt;Asynchronous Dynamic Programming&lt;/h2&gt;

&lt;p&gt;지금까지 다루었던 DP 방법의 가장 큰 단점은 MDP의 전체 State 집합을 한번에 다룬다는 것입니다. 따라서 State 집합이 크다면 한 단계에서의 계산량이 매우 크다는 문제가 있습니다. 예를 들면 Backgammon이라는 보드게임이 있는데 (아래 그림 참고), 이 게임에는 $10^{20}$개의 State가 존재합니다. 이것을 Value Iteration으로 풀 때, 1초에 100만개의 State를 업데이트할 수 있다고 쳐도 단 한번에 Step을 완료하는데 천 년 이상이 걸립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-07.png&quot; alt=&quot;&quot; width=&quot;500&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Asynchronous DP Algorithm&lt;/span&gt;은 State의 Value를 업데이트할 때, 이미 알고 있는 State의 Value를 사용하여 업데이트하는 방법입니다. 물론 수렴성이 보장되려면 모든 State의 Value를 업데이트해야하는 것은 같습니다.&lt;/p&gt;

&lt;p&gt;예를 들면, Value Iteration 업데이트 식 (4.10)을 사용하여 각 단계 $k$마다 단 하나의 State $s_k$의 값만을 업데이트합니다. 물론 $v_{*}$로 수렴을 보장하려면 이것을 무한히 반복해야 합니다.&lt;/p&gt;

&lt;p&gt;하지만 이렇게 전체 State에 대한 업데이트를 피한다고 해서 계산량이 줄어든다는 의미가 아닙니다. Asynchronous DP Algorithm의 목적은 한 단계에 너무 오래 갇히는 것을 피할 뿐입니다. 또한 이것을 응용한다면, 최적의 Action과 관련이 없는 State에 대해서는 업데이트를 줄일 수도 있는데, 이것에 대한 자세한 내용은 8장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;Asynchronous DP Algorithm의 또 다른 장점은 Agent가 MDP를 수행하는 것과 동시에 수행할 수 있다는 것입니다. 즉, Agent가 State를 방문할 때, 이것을 바로 업데이트에 적용할 수 있습니다. 추후 다룰 강화학습에서는 이렇게 Agent의 수행과 State의 Value 업데이트가 동시에 발생하는 경우가 많습니다.&lt;/p&gt;

&lt;h2 id=&quot;generalized-policy-iteration&quot;&gt;Generalized Policy Iteration&lt;/h2&gt;

&lt;p&gt;Policy Iteration은 매 단계마다 Policy Evaluation과 Policy Improvement가 수행되며 Policy를 업데이트하지만, 이것이 동시에 수행되지는 않습니다. 즉, Policy Evaluation이 끝나야 Policy Improvement가 수행되고, Policy Improvement가 끝나야 다음 단계의 Policy Evaluation이 시작된다는 뜻입니다. 하지만 Asynchronous DP Algorithm처럼 꼭 이렇게 수행할 필요는 없습니다. 어차피 Policy Evaluation과 Policy Improvement를 계속 수행하기만 하면 언젠가 최적의 Policy에 도달하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Generalized Policy Iteration (GPI)&lt;/span&gt;는 Policy Evaluation과 Policy Improvement가 상호 작용하도록 하는 일반적인 개념을 말합니다. 즉, Policy는 항상 Value Function에 의해 개선되고, Policy로부터 항상 Value Function이 계산됩니다. Evalution과 Improvement가 모두 안정화된다면 Value Function과 Policy가 최적이라는 뜻입니다. 이 때 Bellman Optimality Equation이 성립하기 때문입니다. 대부분의 강화학습은 GPI 관점에서 잘 설명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPI의 Evaluation과 Improvement 과정은 여러 가지 관점에서 볼 수 있습니다. 예를 들어, Value Function으로 Policy를 greedy하게 만드는 것은 변경된 Policy로 인해 Value Function을 틀리게 만들고, Value Function과 Policy을 일관되게 만들면 Policy가 greedy하지 않는 문제가 생깁니다. 하지만 장기적으로 두 과정은 최적의 Value Function과 최적의 Policy를 찾기 위해 상호 작용하기 때문에, 경쟁과 협력 관계로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또 다른 관점으로는 두 가지 제약 조건, 또는 목표라고 볼 수도 있습니다. 위의 그림은 이 관계를 대략적으로 도식화한 모습인데, Evaluation과 Improvement의 각 과정은 서로의 목표에 도달할 수 있는 하나의 솔루션을 도출합니다. (Value Function을 통해 조금 더 최적의 Policy를 유도하며, 그 반대도 성립) 그림의 화살표를 따라가다보면 하나의 목표를 향하게 될 때 다른 하나의 목표는 멀어지게 됩니다. 하지만 두 과정의 목표는 결국 하나로 이어지기 때문에 결국에는 두 과정의 목표가 함께 달성됩니다.&lt;/p&gt;

&lt;h2 id=&quot;efficiency-of-dynamic-programming&quot;&gt;Efficiency of Dynamic Programming&lt;/h2&gt;

&lt;p&gt;지금까지 배운 DP는 다른 방법들과 비교했을 때 매우 효율적입니다. 물론 State의 수 $n$과 Action의 수 $k$가 매우 크다면 그렇지 않을 수 있지만, 일반적인 상황에서 DP는 (Deterministic) Policy의 수가 $k^n$개인 경우라도 다항식 시간에 Optimal Policy를 찾는 것이 보장됩니다. Linear Programming 방법과 비교해 봤을 때, 최악의 경우 수렴에 대한 보장이 DP 방법보다 낫다는 장점이 있지만, State의 수가 적을 때 약 100배나 비효율적입니다. 게다가 정 반대로 State가 매우 많은 상황에서는 DP 방법만 가능하다는 문제도 있습니다.&lt;/p&gt;

&lt;p&gt;DP 방법의 단점으로 계속 수많은 State에서 계산이 어렵다는 점을 지적해왔으나(Curse of Dimensionality), 현재 컴퓨터의 수준으로 수백만 개의 State 정도는 DP 방법으로 해결이 가능합니다. 많은 문제가 Policy Iteration과 Value Iteration으로 해결되고 있으며 보통 초기 Value나 Policy를 예제처럼 완전 무작위로 설정하지 않고 어느 정도 최적에 가깝게 주어지기 때문에 이론상 최악의 실행 시간보다 빠르게 수렴합니다.&lt;/p&gt;

&lt;p&gt;매우 큰 State 집합의 문제에서는 마지막으로 다루었던 Asynchronous DP Algorithm이 선호됩니다. 동기식 방법은 한 Step 마다 모든 State에 대한 계산과, 저장할 메모리가 필요하기 때문에 비효율적이기 때문입니다. Asynchronous DP Algorithm 외에도 계산량과 메모리의 낭비를 줄이기 위해 GPI를 변형하는 방법도 연구되고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 Finite MDP를 풀기 위해 &lt;strong&gt;Dynamic Programming&lt;/strong&gt;의 기본 아이디어와 알고리즘을 익혔습니다. &lt;strong&gt;Policy Evaluation&lt;/strong&gt;은 주어진 Policy로 반복적인 계산을 통해 Value Function을 계산합니다. &lt;strong&gt;Policy Improvement&lt;/strong&gt;는 Policy Evaluation으로 구한 Value Function을 이용하여 개선된 Policy를 계산합니다. 이 두 가지 절차를 반복하거나, 혼합하는 방식으로 DP의 핵심 알고리즘인 Policy Iteration과 Value Iteration을 얻을 수 있습니다. 이 두 방법은 MDP에 대한 완전한 지식이 주어졌을 때, Optimal Policy 및 Value Function을 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 DP 방법은 매 Step마다 모든 State 집합에 대해 업데이트 작업을 수행합니다. (이것을 Sweep이라고 부릅니다) 이러한 작업은 Bellman Optimality Equation을 이용하여 가능한 모든 후속 State의 Value와 전이 확률을 기반으로 업데이트합니다. 업데이트 후 더 이상 변경되는 값이 없다면 수렴했다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;DP 방법을 포함하여 대부분의 강화학습 방법에 대한 기본 아이디어는 &lt;strong&gt;Generalized Policy Iteration (GPI)&lt;/strong&gt;의 개념에서 얻을 수 있습니다. GPI는 근사적인 Policy과 근사적인 Value Function 사이의 상호 작용 프로세스에 대한 일반적인 개념입니다. 주어진 Policy를 토대로 Value Function을 Policy에 대한 실제 Value Function과 유사하게 맞추는 프로세스와, 주어진 Value Function을 토대로 현재 Policy보다 더 나은 Policy로 변경하는 프로세스가 같이 일어납니다. 이번 장에서 다룬 DP 방법은 GPI가 Optimal Policy와 Value Function에 수렴하는 것이 보장되어 있지만, 모든 GPI가 그러한 것은 아닙니다.&lt;/p&gt;

&lt;p&gt;또한 DP에서 매 Step 마다 모든 State 집합을 한번에 업데이트할 필요는 없습니다. &lt;strong&gt;Asynchronous DP Algorithm&lt;/strong&gt;은 이미 구한 State의 정보를 토대로 새로운 State를 업데이트 함으로써 일부분의 State만 업데이트 하는 방식입니다. Asynchronous DP Algorithm라고 해도 결국 모든 State를 업데이트 해야하는 것은 동일하지만, DP 알고리즘이 지나치게 오랜 시간동안 한 Step에 머무는 것을 방지해줍니다.&lt;/p&gt;

&lt;p&gt;마지막으로 DP 방법은 모두 이어지는 State의 추정 Value를 기반으로 State의 추정 Value를 업데이트합니다. 즉, 다른 추정치를 기반으로 현재의 추정치를 업데이트합니다. 이것을 &lt;strong&gt;Bootstrapping&lt;/strong&gt;이라고 부릅니다. 이번 장에서는 Environment에 대한 Model이 정확하게 주어진 상황에서 Bootstrapping을 수행하였지만,  그렇지 않은 경우에도 Bootstrapping을 사용할 수 있습니다. 우선 다음 장에서는 Environment에 대한 Model이 주어지지 않은 상황에서 Bootstrapping을 사용하지 않는 강화학습 방법을 먼저 소개하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;4장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">Dynamic Programming은 Markov Decision Process (MDP)와 같이 완벽한 Environment Model이 주어졌을 때 Optimal Policy을 계산할 수 있는 알고리즘입니다. Dynamic Programming은 학부 알고리즘 수업에서도 다루는 중요한 알고리즘이지만, 완벽한 Model이 주어져야 한다는 가정과 막대한 계산 비용으로 인해 강화학습에 직접적으로 적용하기는 힘든 단점이 있습니다. 다만 Dynamic Programming으로 강화학습 문제를 해결하는 과정은 다른 강화학습 해결 방법을 이해하는데 큰 도움이 되기 때문에 반드시 짚고 넘어가야 합니다.</summary></entry><entry><title type="html">Finite Markov Decision Processes</title><link href="http://localhost:4000/studies/finite-markov-decision-processes/" rel="alternate" type="text/html" title="Finite Markov Decision Processes" /><published>2022-02-03T00:00:00+09:00</published><updated>2022-02-03T00:00:00+09:00</updated><id>http://localhost:4000/studies/finite-markov-decision-processes</id><content type="html" xml:base="http://localhost:4000/studies/finite-markov-decision-processes/">&lt;p&gt;이번 장에서는 이 책에서 해결하고자 하는 목표인 &lt;span style=&quot;color:red&quot;&gt;Finite Markov Decision Processes (Finite MDP)&lt;/span&gt;에 대해 소개합니다. $k$-armed bandit 문제에서는 즉각적인 Reward에 대한 피드백만 고려하였으나, MDP는 즉각적인 Reward와 더불어 이어지는 State와 미래에 받을 Reward 등을 모두 포함한 의사 결정이 필요합니다. 수식으로 표현하면 bandit 문제에서는 각각의 Action $a$에 대하여 $q_* (a)$를 추정하였으나, MDP에서는 각각의 State $s$와 Action $a$를 모두 포함한 $q_* (s, a)$를 추정하거나, State $s$에 대한 Value $v_* (s)$를 추정합니다.&lt;/p&gt;

&lt;p&gt;MDP는 강화학습 문제를 이론적으로 접근하기 위해 수학적으로 표현된 형태입니다. MDP에서 핵심적으로 다루게 될 요소는 Return, Value Function, Bellman Equation입니다. 그리고 여기서는 Finite MDP에만 한정하여 접근할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-agentenvironment-interface&quot;&gt;The Agent–Environment Interface&lt;/h2&gt;

&lt;p&gt;MDP는 목표를 달성하기 위해 상호 작용을 통해 학습하는 문제의 간단한 프레임을 의미합니다. 의사결정을 통해 학습을 하는 주체를 Agent로 정의하고, Agent가 상호작용하는 모든 외부적인 것들을 Environment라고 합니다. Agent가 Action을 선택하고 수행하면 Environment로부터 Reward를 제공받습니다.&lt;/p&gt;

&lt;p&gt;시간은 $t = 0, 1, 2, … $와 같이 이산적으로 주어지며, 각 시간 단계 $t$에서 Agent는 Environment에 포함된 State $S_t$를 인지하고 이를 기반으로 Action $A_t$를 선택합니다. 그로부터 Agent는 Environment으로부터 Reward $R_{t+1} \in \mathbb{R}$를 받고 다음 State $S_{t+1}$에 도달합니다. 이 과정을 아래와 같이 나열한 것을 &lt;span style=&quot;color:red&quot;&gt;Sequence&lt;/span&gt; 또는 &lt;span style=&quot;color:red&quot;&gt;Trajectory&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

\[S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ... \tag{3.1}\]

&lt;p&gt;또한 이것을 그림으로 표현하면 다음과 같습니다. 강화학습을 다루는 논문에는 이것과 비슷한 그림이 들어가는 경우가 많으니 참고해주시기 바랍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/3. Finite Markov Decision Processes/RL 03-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finite MDP에서 State set $\mathcal{S}$, Action set $\mathcal{A}$, Reward set $\mathcal{R}$은 모두 유한집합입니다. 이 때, 확률변수 $R_t$와 $S_t$는 이전 State와 Action에 따른 이산 확률 분포가 됩니다. 이것을 확률적으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[p(s&apos;, r | s, a) \doteq Pr \left\{ S_t = s&apos;, R_t = r | S_{t-1} = s, A_{t-1} = a \right\} \tag{3.2}\]

&lt;p&gt;함수 $p$는 MDP의 Dynamics를 정의합니다. 식 (3.2)의 오른쪽 항을 해석하면 $t-1$ 시간의 State가 $s$이고 Action이 $a$일 때, 다음 시간 $t$에서의 State가 $s’$, Reward가 $r$일 확률이라는 뜻이 됩니다. 중간의 $\mid$는 조건부 확률이라는 의미입니다. &lt;strong&gt;확률&lt;/strong&gt;이기 때문에 다음과 같이 모든 State와 Action에 대해서 확률의 합이 1이 나와야 합니다.&lt;/p&gt;

\[\sum_{s&apos; \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s&apos;, r | s, a) = 1, \quad \text{for all } s \in \mathcal{S}, a \in \mathcal{A} (s) \tag{3.3}\]

&lt;p&gt;MDP에서 특이한 점은 현재의 State와 Reward는 오직 &lt;strong&gt;직전&lt;/strong&gt;의 State와 Action에만 영향을 받는다는 것입니다. 실제 확률식을 보더라도 조건으로 걸린 부분은 직전 State $S_{t-1}$와 Action $A_{t-1}$ 뿐입니다. 다시 말해서, 그보다 이전에 발생했던 State와 ACtion에는 전혀 영향받지 않는다는 뜻입니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Markov Property&lt;/span&gt;라고 합니다. 이 책에서 다루는 강화학습은 모두 Markov Property를 가정하고 있습니다.&lt;/p&gt;

&lt;p&gt;방금 소개한 확률식은 다양한 변형이 가능합니다. 먼저 Reward $r$을 제외한다면 다음과 같이 쓸 수도 있습니다.&lt;/p&gt;

\[\begin{align}
p(s&apos; | s, a) &amp;amp; \doteq Pr \left\{ S_t = s&apos; | S_{t-1} = s, A_{t-1} = a \right\} \\ \\
&amp;amp;= \sum_{r \in \mathcal{R}} p(s&apos;, r | s, a) \tag{3.4}
\end{align}\]

&lt;p&gt;또는 다음과 같이 Reward를 State와 Action으로 표현한 식도 있습니다.&lt;/p&gt;

\[\begin{align}
r(s, a) &amp;amp; \doteq \mathbb{E} \left[ R_t | S_{t-1} = s, A_{t-1} = a \right] \\ \\
&amp;amp;= \sum_{r \in \mathcal{R}} r \sum_{s \in \mathcal{S}} p(s&apos;, r | s, a) \tag{3.5}
\end{align}\]

&lt;p&gt;마지막으로 Reward를 이전 State와 Action, 다음 State로 표현할 수도 있습니다.&lt;/p&gt;

\[\begin{align}
r(s, a, s&apos;) &amp;amp; \doteq \mathbb{E} \left[ R_t | S_{t-1} = s, A_{t-1} = a, S_t = s&apos; \right] \\ \\
&amp;amp;= \sum_{r \in \mathcal{R}} \frac{p(s&apos;, r | s, a)}{p(s&apos;| s, a)} \tag{3.6}
\end{align}\]

&lt;p&gt;이렇게 여러 표현이 존재하지만, 이 책에서는 식 (3.2)와 같은 표현을 주로 사용할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;goals-and-rewards&quot;&gt;Goals and Rewards&lt;/h2&gt;

&lt;p&gt;강화학습에서 Agent의 목표는 Environment으로부터 주어지는 Reward의 총합을 최대화하는 것으로 정의할 수 있습니다. 각 시간 단계에서의 Reward는 $R_t \in \mathbb{R}$로 정의합니다. 하지만 Agent는 즉각적인 Reward $R_t$가 아니라 장기적으로 받을 수 있는 누적 Reward를 극대화해야 합니다. 교재에서는 이것을 다음과 같이 말하고 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Reward는 Agent의 목표를 공식화하는데 사용합니다. 예를 들면 미로를 탈출하는 최단 경로를 찾을 때 모든 시간 단계에 -1의 Reward를 주고, 목적지에 도달하면 10의 Reward을 주는 방식으로 설계하여 최대한 빨리 미로를 탈출하게 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 이것이 Agent에게 사전 지식을 전달하는 역할은 아닙니다. 바둑을 예로 들면 축을 만들거나 돌을 딴다고 해서 Reward를 주는 것이 아니라, 게임에서 승리해야 Reward를 주는 방식으로 설계해야 합니다. 즉, 하위 목표를 따로 설계하는 것이 아닙니다.&lt;/p&gt;

&lt;h2 id=&quot;returns-and-episodes&quot;&gt;Returns and Episodes&lt;/h2&gt;

&lt;p&gt;직전 부분에서 Agent의 목표는 장기적으로 받는 누적 Reward를 극대화하는 것이라고 정의했습니다. 이번에는 이것을 구체적으로 정의해보겠습니다. 먼저 시간 단계 $t$ 이후로 받은 Reward를 순서대로 $R_{t+1}, R_{t+2}, R_{t+3}, …$로 표현하도록 하겠습니다. Agent가 시간 단계 $t$ 이후부터 마지막 시간 단계인 $T$까지 받을 수 있는 기대 이익를 $G_t$라고 정의하면, $G_t$는 다음과 같이 Reward의 합으로 정의할 수 있습니다. $G_t$는 &lt;span style=&quot;color:red&quot;&gt;Return&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T} \tag{3.7}\]

&lt;p&gt;이러한 정의는 &lt;em&gt;마지막 시간 단계라는 개념이 있는 상황&lt;/em&gt;에서만 의미가 있습니다. 예를 들어 위에서 언급한 미로 탈출 문제에서는 목적지에 도달하면 끝나기 때문에 마지막 시간 단계가 존재합니다. 이렇게 처음부터 끝까지 Agent와 Environment의 상호작용을 마치는 것을 하나의 &lt;span style=&quot;color:red&quot;&gt;Episode&lt;/span&gt;라 부릅니다. 각 Episode는 독립적이며 게임의 승리/패배와 같이 서로 다른 Reward를 받을 수도 있습니다. 이러한 Episode에서 마지막 State는 $\mathcal{S}$로 표현한 일반적인 State와 구분짓기 위해 $\mathcal{S^+}$로 표현합니다. 또한 이렇게 Episode로 이루어진 문제를 &lt;span style=&quot;color:red&quot;&gt;Episodic Task&lt;/span&gt;라고 부릅니다. 각각의 Episode마다 마지막 State에 도달하는 시간이 차이가 날 수 있기 때문에 마지막 시간 단계 $T$는 Random Variable (확률 변수)입니다.&lt;/p&gt;

&lt;p&gt;하지만 모든 문제들이 Episode를 갖고 있는 것은 아닙니다. 끊임없이 프로세스를 할당하는 작업 관리자처럼 마지막 시간 단계라는 것이 존재하지 않고 무한히 이어지는 문제도 많습니다. 이러한 문제들을 &lt;span style=&quot;color:red&quot;&gt;Continuing Task&lt;/span&gt;이라고 부릅니다. Continuing Task에서 식 (3.7)을 그대로 사용한다면 Return $G_t$가 무한대가 될 가능성이 크다는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;이 때 사용되는 개념이 &lt;span style=&quot;color:red&quot;&gt;Discounting&lt;/span&gt;입니다. 이것은 미래에 받는 Reward을 일정 비율로 줄임으로써 현재로부터 멀어질수록 Reward에 대한 Value를 낮추는 방법입니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Discounted Return&lt;/span&gt;이라고 정의하며 다음과 같이 표현됩니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \tag{3.8}\]

&lt;p&gt;위 식에서 $0 \le \gamma \le 1$은 &lt;span style=&quot;color:red&quot;&gt;Discount Rate&lt;/span&gt;라고 부릅니다. Discount Rate는 미래에 받을 Reward가 현재에 얼마나 Value가 있는지 판단하는 기준입니다. 만약 $k$ 시간 단계 후에 받을 Reward를 지금 당장 받는다면 $\gamma^{k-1}$ 배의 Value가 있습니다. 만일 $\gamma = 0$인 경우에는 눈앞에 있는 Reward만을 최대화하는 근시적 판단을 하게 되고, $\gamma$가 1에 가까워질수록 미래의 Reward를 더 많이 반영하게 됩니다.&lt;/p&gt;

&lt;p&gt;식 (3.8)은 다음과 같이 표현할 수도 있습니다. 이렇게 변경한 식은 Continuing Task가 아니라도 사용할 수 있습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp;\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\ \\
&amp;amp;= R_{t+1} + \gamma \left( R_{t+2} + \gamma R_{t+3} \cdots \right) \\ \\
&amp;amp;= R_{t+1} + \gamma G_{t+1} \tag{3.9}
\end{align}\]

&lt;p&gt;식 (3.8)은 항의 개수가 무한대지만, $\gamma &amp;lt; 1$이고 Reward가 0이 아닌 상수라면 그 합은 유한합니다. 예를 들어, Reward가 1이라면 다음과 같이 간단하게 정리할 수 있습니다.&lt;/p&gt;

\[G_t = \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma} \tag{3.10}\]

&lt;h2 id=&quot;unified-notation-for-episodic-and-continuing-tasks&quot;&gt;Unified Notation for Episodic and Continuing Tasks&lt;/h2&gt;

&lt;p&gt;지금까지 강화학습에서 사용되는 Return $G_t$를 Episodic Task의 경우와 Continuing Task의 경우로 나누어 다루었습니다. 이번에는 두 경우 모두 사용할 수 있는 통일된 표기법을 소개하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 Episodic Task를 고려해보면, 각각의 Episode를 구분해야하고 Episode별 시간 단계 또한 구분해야 합니다. 따라서 원래는 Episode 숫자와 시간 단계 숫자가 모두 표기된 $S_{t, i}$와 같은 표기 방식이 필요하지만($i$는 에피소드 숫자), 여기서는 보통 특정 Episode를 고려하거나 모든 Episode에 해당하는 것들을 다루기 때문에 이러한 명시적인 표기를 사용하지 않고, 그냥 $S_t$를 사용하기로 합니다.&lt;/p&gt;

&lt;p&gt;다음은 Return에 대한 표기입니다. 이전에는 Episodic Task에서 식 (3.7)로 표기하였고 Continuing Task에서는 식 (3.8)로 표기하였습니다. 이것은 Episodic Task를 Continuing Task와 같은 식으로 바꾸는 것으로 해결할 수 있습니다. 아래 그림과 같이 Episodic Task가 끝난 이후로는 모든 Reward를 0으로 정의하고 같은 State에 반복적으로 돌아오게 설계하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/3. Finite Markov Decision Processes/RL 03-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 &lt;strong&gt;State Diagram&lt;/strong&gt;에서 마지막 사각형은 Episode의 끝을 나타내며, 이를 &lt;span style=&quot;color:red&quot;&gt;Absorbing State&lt;/span&gt;라고 부릅니다. 위의 예제에서 모든 Reward를 합쳐도 $R_4$ 이후로는 Reward가 0이기 때문에 $R_1$, $R_2$, $R_3$을 더한 값과 동일합니다. Discounting을 포함해도 마찬가지입니다. 이로써 다음과 같이 Return $G_t$를 통합하여 표기할 수 있습니다.&lt;/p&gt;

\[G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1} \tag{3.11}\]

&lt;p&gt;식 (3.11)은 $T = 1$ 또는 $T = \infty$도 상관없이 유효합니다. 이 책의 나머지 부분에서는 이렇게 표기를 단순화함으로써 Episodic Task나 Continuing Task를 구분하지 않고 문제를 다루도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;policies-and-value-functions&quot;&gt;Policies and Value Functions&lt;/h2&gt;

&lt;p&gt;대부분의 강화학습 알고리즘은 Agent가 주어진 State에 있는 것이 얼마나 좋은지(또는 주어진 State에서 특정 Action을 수행하는 것이 얼마나 좋은지)를 추정하는 Value Function을 가지고 있습니다. 여기서 &lt;strong&gt;얼마나 좋은가&lt;/strong&gt;를 판단하는 기준은 받을 수 있는 미래의 Reward, 즉 총 Reward의 기대값으로 정의됩니다. 이 때의 기대값은 어떤 Action을 선택하는지에 따라 다르며, Value Function은 Policy를 기반으로 정의됩니다.&lt;/p&gt;

&lt;p&gt;Policy은 각 State에서 각각의 Action을 선택할 확률로 정의됩니다. Agent가 시간 $t$에서 Policy $\pi$를 따르는 경우 $\pi (a \mid s)$는 State $S_t = s$일 때 $A_t = a$일 확률입니다. Policy $\pi$는 강화학습이 진행되는 동안 계속 업데이트됩니다.&lt;/p&gt;

&lt;p&gt;Value Function은 State $s$에서 Policy $\pi$를 따를 때 예상되는 Return을 의미합니다. Value Function는 $v_{\pi} (s)$로 표기하며 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp; \doteq \mathbb{E}_{\pi} \left[ G_t | S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s \right], \text{ for all } s \in \mathcal{S} \tag{3.12}
\end{align}\]

&lt;p&gt;$v_{\pi} (s)$는 Policy $\pi$에 대한 &lt;span style=&quot;color:red&quot;&gt;State-Value Function&lt;/span&gt;이라고 부릅니다. State-Value Function과는 다르게 State 뿐만 아니라 Action을 포함한 Value Function도 있습니다. State와 Action을 모두 고려하는 Value Function을 &lt;span style=&quot;color:red&quot;&gt;Action-Value Function&lt;/span&gt;이라고 부르고, 이를 $q_{\pi} (s, a)$으로 표현합니다. $q_{\pi} (s, a)$는 다음과 같이 정의합니다.&lt;/p&gt;

\[\begin{align}
q_{\pi} (s, a) &amp;amp; \doteq \mathbb{E}_{\pi} \left[ G_t | S_t = s , A_t = a \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s , A_t = a \right] \tag{3.13}
\end{align}\]

&lt;p&gt;Value Function $v_{\pi} (s)$와 $q_{\pi} (s, a)$는 경험을 통해 추정할 수 있습니다. 예를 들어, Agent가 Policy $\pi$를 따를 때 발생하는 각각의 State에 대해 실제 얻게 되는 Reward의 평균을 구하게 되면 실제 $v_{\pi} (s)$ 값에 수렴하게 됩니다. $q_{\pi} (s, a)$도 이와 마찬가지로 각 State에 대해 특정 Action을 선택했을 때 받는 Reward의 평균을 계산하면 됩니다. 이러한 방법을 &lt;span style=&quot;color:red&quot;&gt;Monte Carlo Method&lt;/span&gt;이라고 합니다. Monte Carlo Method는 많은 무작위 샘플을 구한 다음 그것들의 평균을 계산하는 방법입니다. Monte Carlo Method에 대한 구체적인 내용은 5장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;그러나 State와 Action이 매우 많은 경우 그만큼 Sample이 많이 필요하게 되고, 어지간한 수의 Sample로는 평균이 정확한 값에 수렴하지 않을 수도 있습니다. 이 때는 Value Function를 매개 변수들로 표현하고 Sample에 맞게 함수 자체를 추정하는 방법을 사용합니다. 이것은 9장 이후로 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;강화학습은 4장에서 다룰 &lt;strong&gt;Dynamic Programming&lt;/strong&gt;과 매우 밀접한 관련이 있는데, Value Function 또한 Dynamic Programming처럼 표현할 수 있습니다. 식 (3.9)와 연관되어 모든 Policy $\pi$ 및 모든 State $s$에 대해, 다음 관계가 성립합니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp; \doteq \mathbb{E}_{\pi} \left[ G_t | S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} | S_t = s \right] \tag{by 3.9} \\ \\
&amp;amp;= \sum_{a} \pi (a | s) \sum_{s&apos;} \sum_{r} p ( s&apos; , r | s, a ) \left[ r + \gamma \mathbb{E}_{\pi} [ G_{t+1} | S_{t+1} = s&apos; ] \right] \\ \\
&amp;amp;= \sum_{a} \pi (a | s) \sum_{s&apos;, r} p ( s&apos; , r | s, a ) \left[ r + \gamma v_{\pi} (s&apos;) \right], \quad \text{for all } s \in \mathcal{S} \tag{3.14}
\end{align}\]

&lt;p&gt;식 (3.14)를 $v_{\pi}$에 대한 &lt;span style=&quot;color:red&quot;&gt;Bellman Equation&lt;/span&gt;이라고 합니다. Bellman Equation은 현재 State의 Value와 이어지는 State의 Value 사이의 관계를 표현합니다. 아래에 있는 Backup Diagram을 보시면 현재 State $s$에서 이어질 수 있는 모든 후속 State $s’$을 고려하는 것을 알 수 있습니다. 앞에서 말씀드렸듯이 Policy $\pi$는 Action을 선택할 확률을 의미하기 때문에, 각 Action을 선택했을 때 받을 수 있는 Reward에 확률 가중치를 포함한 평균을 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/3. Finite Markov Decision Processes/RL 03-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Value Function $v_{\pi}$는 Bellman Equation에 대한 유일한 해법입니다. 4장에서 Bellman Equation이 어떻게 $v_{\pi}$를 계산하고, 근사화하고, 학습하는지를 자세하게 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;optimal-policies-and-optimal-value-functions&quot;&gt;Optimal Policies and Optimal Value Functions&lt;/h2&gt;

&lt;p&gt;강화학습 문제를 해결한다는 것은 장기적인 관점에서 최대의 보상을 얻을 수 있는 Policy를 찾는다는 것과 동일한 의미입니다. 먼저 두 개의 Policy가 존재할 때, 어떤 Policy가 더 좋은가를 판단하는 방법부터 알아보도록 하겠습니다. Finite MDP에서, Policy $\pi$가 Policy $\pi^{\prime}$보다 좋다는 것은(즉, $\pi \ge \pi^{\prime}$) 모든 State $s \in \mathcal{S}$에 대해 Value Function $v_{\pi}$가 작지 않다는 것입니다. (즉, $v_{\pi} (s) \ge v_{\pi^{\prime}}$) Finite MDP에서는 항상 다른 모든 Policy보다 좋거나 같은 Policy가 하나 이상 존재하는데, 이것을 &lt;span style=&quot;color:red&quot;&gt;Optimal Policy&lt;/span&gt;라고 부릅니다. 최적의 Policy가 여러 개 존재할 수도 있지만, 어차피 동일하기 때문에 모두 $\pi_{*}$라고 표기합니다. 또한 이 때의 State-Value Function를 &lt;span style=&quot;color:red&quot;&gt;Optimal State-Value Function&lt;/span&gt;이라고 부르고, $v_{*}$로 표기하며, 다음과 같이 정의합니다.&lt;/p&gt;

\[v_{*} (s) \doteq \max_{\pi} v_{\pi} (s) \quad \text{for all } s \in \mathcal{S} \tag{3.15}\]

&lt;p&gt;마찬가지로, &lt;span style=&quot;color:red&quot;&gt;Optimal Action-Value Function&lt;/span&gt;는 $q_{*}$로 표기하며, 다음과 같이 정의합니다.&lt;/p&gt;

\[q_{*} (s, a) \doteq \max_{\pi} q_{\pi} (s, a) \quad \text{for all } s \in \mathcal{S} \text{ and } a \in \mathcal{A} (s) \tag{3.16}\]

&lt;p&gt;최적의 Policy를 따르는 $q_{*}$에서 예상되는 Reward는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[q_{*} (s, a) = \mathbb{E} \left[ R_{t+1} + \gamma v_{*} ( S_{t+1} ) | S_t = s, A_t = a \right] \tag{3.17}\]

&lt;p&gt;$v_{*}$는 Policy에 대한 Value Function이기 때문에 식 (3.14)의 Bellman Equation에 의해 주어진 Self-Consistency 조건을 만족해야 합니다. 즉, Bellman Equation에서 최적의 Policy를 따른다는 것은 $v_{*}$에 대한 Bellman Equation이 &lt;span style=&quot;color:red&quot;&gt;Bellman Optimality Equation&lt;/span&gt;이 된다는 것을 의미합니다. 따라서 Bellman Optimality Equation에서의 Value Function는 각 State에서 최적의 Action을 수행한다는 것이며, 그 결과로 최대의 Reward를 받아야 합니다. 이것을 수식으로 전개하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
v_{*} (s) &amp;amp;= \max_{a \in \mathcal(A) (s)} q_{\pi_*} (s, a) = \max_a \mathbb{E}_{\pi_*} \left[ G_t | S_t = s, A_t = a \right] \\ \\
&amp;amp;= \max_a \mathbb{E}_{\pi_*} \left[ R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a \right] \tag{by (3.9)} \\ \\
&amp;amp;= \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{*} (S_{t+1}) | S_t = s, A_t = a \right]\tag{3.18} \\ \\
&amp;amp;= \max_a \sum_{s&apos;, r} p ( s&apos;, r | s, a ) \left[ r + \gamma v_{*} (s&apos;) \right] \tag{3.19}
\end{align}\]

&lt;p&gt;식 (3.18)과 (3.19)는 $v_{*}$에 대한 Bellman Optimality Equation의 두 가지 형태입니다. $q_{*}$에 대한 Bellman Optimality Equation은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
q_{*} (s, a) &amp;amp;= \mathbb{E} \left[ R_{t+1} + \gamma \max_{a&apos;} q_{*} (S_{t+1}, a&apos;) \bigg| S_t = s, A_t = a \right] \\ \\
&amp;amp;= \sum{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma \max_{a&apos;} q_{*} (s&apos;, a&apos;) \right] \tag{3.20}
\end{align}\]

&lt;p&gt;아래 그림은 $v_{*}$와 $q_{*}$에 대한 Bellman Optimality Equation에서 고려되는 미래의 State와 Action의 범위를 표현한 Backup Diagram입니다. 왼쪽은 식 (3.19)를 표현한 그림이고, 오른쪽은 식 (3.20)을 표현한 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/3. Finite Markov Decision Processes/RL 03-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finite MDP에서 $v_{*}$에 대한 Bellman Optimality Equation에는 고유한 해법이 있습니다. Bellman Optimality Equation에서는 각각의 State에 대해 하나의 방정식이 있는 구조이기 때문에, $n$개의 State가 있으면 $n$개의 미지수가 있는 $n$개의 방정식이 나옵니다. 만약 Environment에서 $p$를 알 수 있다면 Nonlinear Equation을 품으로써 $v_{*}$를 구할 수 있습니다. (ex. Newton-Raphson Method) 같은 이유로 $q_{*}$도 마찬가지입니다.&lt;/p&gt;

&lt;p&gt;$v_{*}$를 구한 이후, 각각의 State에서 Bellman Optimality Equation에서 최대값을 얻는 하나 이상의 Action을 찾습니다. 이런 Action에만 0이 아닌 확률을 할당하는 Policy이 바로 최적의 Policy입니다. $q_{*}$를 구했다면 최적의 Policy을 찾기 더 쉽습니다. 모든 State $s$에 대해 $q_{*}(s, a)$를 최대화하는 Action을 선택하면 됩니다.&lt;/p&gt;

&lt;p&gt;하지만 Bellman Optimality Equation을 푸는 것으로 강화학습 문제를 해결하는 것은 사실 유용한 방법이 아닙니다. Bellman Optimality Equation을 풀기 위해서는 &lt;strong&gt;(1) Environment가 정확하게 알려져 있고, (2) 계산하기 위한 자원이 충분해야 하며, (3) Markov Property를 갖고 있어야 한다&lt;/strong&gt;는 3가지 조건이 필요합니다. 하지만 대부분의 문제에서는 이 3가지 조건을 모두 갖고있지 않습니다. Environment가 정확하게 알려져있는 문제는 사실상 인위적으로 만든 문제들 뿐이고, 최신 슈퍼 컴퓨터로도 간단한 문제의 Bellman Equation을 푸는데 굉장히 오랜 시간이 걸리는데다 대부분의 문제에서 Markov Property를 갖고있다는 증명을 하기가 어렵기 때문입니다. 따라서 대부분의 강화학습 문제에서는 근사적인 솔루션으로 해결하는 수밖에 없습니다. 근사적인 솔루션 중 대표적인 방법이 바로 Dyanmic Programming 방법인데, 다음 장에서 어떤 방법으로 해결하는지 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;optimality-and-approximation&quot;&gt;Optimality and Approximation&lt;/h2&gt;

&lt;p&gt;지금까지 최적의 Value Function와 최적의 Policy을 정의했습니다. 하지만 Bellman Optimality Equation을 통해 최적의 Policy을 찾는 방법은 매우 많은 계산량으로 인해 실제 문제에서는 사용하기 어려운 방법입니다. 근사적인 솔루션으로 접근해도 Value Function나 Policy, 그리고 Model의 근사치를 구축하기 위해서는 많은 크기의 메모리가 필요합니다. State과 Action의 집합이 작은 경우에는 행과 열을 State/Action으로 정의하여 표로 나타낼 수 있지만, State와 Action이 엄청나게 많은 경우에는 매개변수화하여 함수 자체를 근사해야할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 근사적인 접근에서 가장 중요한 것은 기회를 최소한으로 낭비함으로써 최적의 Action을 찾는 것입니다. 쉽게 표현하면 지금 당장 보이는 가장 좋은 Action을 고를 수도 있지만, 내가 선택해보지 않은 Action이 장기적으로 봤을 때 더 좋은 Reward를 제공할 수 있으므로 선택해봐야할 수도 있습니다. 이 경우 높은 Reward를 제공하면 기회를 낭비한 것이 아니지만, 생각한 것과 다르게 실제로도 낮은 Reward를 제공하는 Action이었다면 그 기회를 낭비한 것이 됩니다. 앞으로 많은 강화학습 방법에서는 최적의 Policy을 근사화하기 위해 기회를 최소한으로 낭비하는 많은 노력들을 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;3장은 꽤 많은 내용을 다뤘습니다. 배운 내용을 요약해보면 강화학습은 Environment와의 상호 작용을 통해 목표를 달성하기 위한 방법을 배우는 것입니다. Agent는 매 시간 State를 마주하고, 해당 State에서 어떤 Action을 선택할 것인지 결정합니다. Agent의 Action에 따라 Environment는 Agent에게 Reward를 제공합니다. 강화학습의 목적은 이러한 구조에서 최대의 누적 Reward를 받기 위한 방법인 Policy를 구하는 것입니다.&lt;/p&gt;

&lt;p&gt;이러한 강화학습을 수식화한 것이 Markov Decision Process (MDP)입니다. 특히 여기서는 유한개의 State, 유한개의 Action과 유한개의 Reward가 있는 Finite MDP에 집중합니다. 강화학습에서의 대부분의 이론은 Finite MDP에만 제한적으로 적용되기 때문입니다.&lt;/p&gt;

&lt;p&gt;Return은 Agent가 최대화하려는 미래의 Reward를 나타냅니다. 강화학습 환경에서 끝이 존재하는 Episodic Task는 Reward의 합으로 간단하게 표현이 가능하지만, Continuing Task는 Reward의 합이 무한대일 수도 있기 때문에 Reward에 보정값이 필요합니다. 이 때, 받게 되는 Reward가 현재로부터 멀어질수록 작아지는 가중치를 곱해주는데, 이것을 Discounting라고 합니다.&lt;/p&gt;

&lt;p&gt;Policy의 Value Function $v_{\pi}$와 $q_{\pi}$는 Agent가 Policy $\pi$를 사용할 때 State, 또는 State-Action 쌍에서 예상되는 수익을 나타냅니다. 최적의 Value Function은 최적의 Policy를 사용했을 때 받을 수 있는 총 Reward의 기대값입니다. 최적의 Value Function는 고유하지만, 최적의 Policy는 여러 개가 있을 수도 있습니다. Bellman Optimality Equation은 최적의 Value Function을 계산하고 이를 통해 최적의 Policy를 구할 수도 있습니다. 하지만 계산량과 같은 현실적인 문제로 인해 대부분 문제에서 Bellman Optimality Equation으로 직접 강화학습 문제를 해결하지는 못합니다.&lt;/p&gt;

&lt;p&gt;따라서 강화학습 문제는 앞으로 근사적인 방법을 사용하여 접근할 예정입니다. 안타깝게도 강화학습에서 완벽한 최적의 솔루션을 찾을 수 없지만, 어떻게 최적의 솔루션에 가깝게 근사할 수 있는지를 다양한 한 시도를 통해 알아볼 것입니다.&lt;/p&gt;

&lt;p&gt;3장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서는 이 책에서 해결하고자 하는 목표인 Finite Markov Decision Processes (Finite MDP)에 대해 소개합니다. $k$-armed bandit 문제에서는 즉각적인 Reward에 대한 피드백만 고려하였으나, MDP는 즉각적인 Reward와 더불어 이어지는 State와 미래에 받을 Reward 등을 모두 포함한 의사 결정이 필요합니다. 수식으로 표현하면 bandit 문제에서는 각각의 Action $a$에 대하여 $q_* (a)$를 추정하였으나, MDP에서는 각각의 State $s$와 Action $a$를 모두 포함한 $q_* (s, a)$를 추정하거나, State $s$에 대한 Value $v_* (s)$를 추정합니다.</summary></entry><entry><title type="html">Multi-armed Bandits</title><link href="http://localhost:4000/studies/multi-armed-bandits/" rel="alternate" type="text/html" title="Multi-armed Bandits" /><published>2022-01-14T00:00:00+09:00</published><updated>2022-01-14T00:00:00+09:00</updated><id>http://localhost:4000/studies/multi-armed-bandits</id><content type="html" xml:base="http://localhost:4000/studies/multi-armed-bandits/">&lt;h1 class=&quot;no_toc&quot; id=&quot;part-i-tabular-solution-methods&quot;&gt;Part I: Tabular Solution Methods&lt;/h1&gt;

&lt;p&gt;이 책은 크게 두 부분으로 나뉘어져 있습니다. 첫 번째는 강화학습에서 State와 Action을 &lt;strong&gt;Table&lt;/strong&gt;에 정리하는 방법입니다. Table을 사용하는 Tabular Method는 대부분의 문제에서 Optimal Policy를 정확하게 찾을 수 있다는 것을 보장하지만 State의 집합과 Action의 집합이 Table을 사용할 수 있을만큼 충분히 작아야 한다는 단점이 있습니다. 두 번째는 Table을 사용하지 않고 State와 Action에 대한 함수를 추정하는 방법인 Approximation Method가 있습니다. 함수를 추정하는 방법은 State와 Action의 집합 크기가 크더라도 (심지어 무한대라고 하더라도) Policy를 구할 수 있지만 Optimal Policy를 보장하기 힘들다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;Tabular Method의 첫 번째는 단일 State만 갖고 있는 Bandit 문제를 해결하는 것으로 시작합니다. (2장) 다음으로는 &lt;strong&gt;Finite Markov Decision Process (Finite MDP)&lt;/strong&gt;와 &lt;strong&gt;Bellman Equation&lt;/strong&gt;, &lt;strong&gt;Value Function&lt;/strong&gt;에 대한 주요 아이디어를 설명합니다. (3장)&lt;/p&gt;

&lt;p&gt;그 다음으로 3개 장에 걸쳐 Finite Markov Decision Process를 해결할 수 있는 방법들을 하나씩 소개합니다. 4장에서 &lt;strong&gt;Dynamic Programming&lt;/strong&gt;, 5장에서 &lt;strong&gt;Monte Carlo Method&lt;/strong&gt;, 그리고 6장에서 &lt;strong&gt;Temporal-Difference Learning&lt;/strong&gt;에 대해 소개합니다. 이 방법들은 각각 장점과 단점이 있기 때문에 어느 하나가 우월하다고 표현하기 어렵습니다. Dynamic Programming은 수학적으로 잘 설계되었으나 Environment에 대해 완전하고 정확한 Model이 필요하다는 단점이 있습니다. Monte Carlo Method은 Model이 필요하지 않고 개념적으로 간단하지만 Step-by-step으로 계산하기 적합하지 않다는 단점이 있습니다. Temporal-Difference Learning도 Model이 필요하지 않고 Step-by-step으로 계산하기 적합하지만, 그만큼 복잡하다는 단점이 있습니다. 또한 이 3가지 방법들은 각각 효율성과 수렴 속도도 차이가 납니다.&lt;/p&gt;

&lt;p&gt;나머지 2개 장에서는 이 세 가지 방법을 결합하여 각각의 최상의 기능을 얻는 방법을 설명합니다. 7장에서는 Monte Carlo Method의 장점인 Multi-step bootstrapping method를 Temporal-Difference Learning과 결합할 수 있는 방법을 설명합니다. 마지막으로 8장에서는 Tabular Method 강화학습 문제에 대한 완전하고 통합된 솔루션을 위해 Temporal-Difference Learning이 Model Learning 및 Planning과 결합할 수 있는 방법을 소개하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;a-k-armed-bandit-problem&quot;&gt;A $k$-armed Bandit Problem&lt;/h2&gt;

&lt;p&gt;$k$-armed Bandit는 $k$ 개의 레버가 달린 슬롯머신을 말합니다. 도박은 결국 돈을 잃을 가능성이 높기 때문에 슬롯머신을 &lt;strong&gt;Bandit&lt;/strong&gt;라고 부르는 것 같습니다. 원래라면 슬롯머신 따위는 거들떠보지도 않겠지만, 여기서는 꼭 이 슬롯머신을 플레이해야하는 상황이라고 가정해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리는 한 번의 게임에서 슬롯머신의 레버 중 하나를 선택하여 당길 수 있습니다. 그런데 이 슬롯머신은 특이하게 각 레버마다 Reward가 다르게 설정되어 있습니다. 물론 어떤 레버가 가장 높은 보수를 받는지는 모르는 상황입니다. 또한 각 레버의 Reward는 고정된 값이 아니라 확률 분포로 이루어져 있습니다. 따라서 레버를 당겼을 때 낮은 Reward가 나왔다고 해도 레버의 Reward가 낮게 설정되어 있는 것인지, 아니면 운이 나빠서 낮은 Reward가 나온 것인지 알 수 없습니다. 물론 같은 레버를 여러번 플레이한다면 대략적인 확률 분포를 예측할 수는 있습니다.&lt;/p&gt;

&lt;p&gt;이런 상황에서 특정 시간이 주어졌을 때(예를 들면 총 1000번의 게임, 또는 T 시간) Reward를 최대화하는 Policy를 찾는 것이 &lt;span style=&quot;color:red&quot;&gt;$k$-armed Bandit Problem&lt;/span&gt;입니다.&lt;/p&gt;

&lt;p&gt;먼저 $k$-armed Bandit Problem에서 $k$개의 Action에 대한 평균적인 Reward를 수식화해보겠습니다. 즉, 각 Action의 Value를 수학적으로 표현하는 것입니다. $t$라는 시간에서 선택한 Action을 $A_t$라고 하고, 그 때 받는 Reward를 $R_t$로 정의합니다. 임의의 Action $a$에 대한 Value를 $q_{*}(a)$라고 하면, $q_{*}(a)$는 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[q_*(a) \doteq \mathbb{E} \left[ R_t \mid A_t = a \right]\]

&lt;p&gt;만약 각 Action에 대한 Value를 알고 있다면 항상 가장 높은 Value를 가진 Action을 선택함으로써  $k$-armed Bandit Problem을 해결할 수 있습니다. 하지만 여기서는 각 Action의 정확한 Value를 알지 못하고 대략적인 추정치만 알고 있다고 가정해봅시다. $t$라는 시간에서 Action $a$의 Value를 추정한 것이 $Q_t (a)$라고 하면, $Q_t (a)$가 $q_*(a)$에 가까울수록 정확한 정답을 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;우리는 아무런 정보가 없는 상황에서 슬롯머신의 레버를 어느 정도 당겨보고 각 레버에 대한 대략적인 추정치를 알게 되었습니다. 이제 또 어떤 레버를 당길지 선택을 해야하는데, 이 때 두 가지 선택이 있습니다. 첫째로 지금까지 당겼던 레버 중 가장 높은 Reward를 제공했던 레버를 당기는 것입니다. 이러한 Action을 Greedy Action이라고 합니다. 강화학습에서 이것은 지금까지의 경험을 &lt;strong&gt;활용&lt;/strong&gt;하는 것이기 때문에 &lt;span style=&quot;color:red&quot;&gt;Exploitation&lt;/span&gt;이라고 부릅니다. 다른 또 하나의 선택은 정보가 부족한 다른 레버를 당겨보는 것입니다. 왜냐하면 내가 당겨보지 않았거나, 적게 당겨보았던 레버가 알고보니 내가 알고있는 레버보다 더 높은 Reward를 제공할 수도 있기 때문입니다. 이렇게 다른 Action을 &lt;strong&gt;탐색&lt;/strong&gt;하는 행위를 강화학습에서는 &lt;span style=&quot;color:red&quot;&gt;Exploration&lt;/span&gt;이라고 부릅니다. Exploration을 하는 동안의 Reward는 Exploitation을 할 때보다 낮겠지만, 더 좋은 Action을 찾고 난 다음에는 더 높은 Reward를 얻을 수 있기 때문에 장기적으로 봤을 때 Exploration이 더 좋을 수도 있습니다. 여기서 주어진 시간 동안 어느 만큼의 Exploration을 하고 어느 만큼의 Exploitation을 할 지도 문제가 됩니다. 또한 어떤 상황에 Exploration을 해야하고 어떤 상황에 Exploitation을 해야하는 지는 각 Action의 추정치가 얼마나 정확한지, 남은 시간 단계가 얼마나 되는지에 따라 달라집니다. 또 생각해 볼 수 있는 문제는 Exploration을 효율적으로 하기 위해 주어진 Action 중 어떤 Action을 선택해야할지도 있겠지만, 이 책에서는 그런 것보다 Exploration과 Exploitation을 얼마나 균형있게 선택하는지를 더 중점적으로 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;action-value-methods&quot;&gt;Action-value Methods&lt;/h2&gt;

&lt;p&gt;이제 각 Action에 대한 가치를 추정하는 방법과 어떤 Action을 선택할지 결정을 내리기 위해 추정한 값을 사용하는 방법에 대해 알아보도록 하겠습니다. 각 Action의 Value는 그 Action을 선택했을 때의 평균적인 Reward로 정의할 수 있습니다. 즉, $Q_t (a)$는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Q_t (a) &amp;amp; \doteq \frac{\text{sum of rewards when } a \text{ taken prior to } t }{\text{number of times } a \text{ taken prior to }t} \\ \\ 
&amp;amp;= \frac{\sum_{i=1}^{t-1} R_i \cdot 1_{A_t = a}}{\sum_{i=1}^{t-1} 1_{A_t = a}} \tag{2.1}
\end{align}\]

&lt;p&gt;여기서 $1_{A_t = a}$는 $A_t = a$가 true면 1이고 그렇지 않으면 0을 나타내는 확률 변수입니다. 초기에는 분모가 0이기 때문에 $Q_t (a)$에 적절한 초기값(이를테면 0)을 설정합니다. 분모가 무한대에 가까워질수록 $Q_t (a)$는 $q_{*}(a)$에 수렴합니다. 이런 방법으로 Action의 Value를 추정하는 것을 &lt;span style=&quot;color:red&quot;&gt;Sample-average method&lt;/span&gt;라고 합니다. Action에 대한 Value를 추정하는 방법은 이외에도 있지만, 여기서 그것까지 다루지는 않고 어떻게 추정값을 통해 Action을 선택하는지를 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;가장 간단하게 Action을 선택하는 방법은 추정 Value가 가장 높은 Action을 선택하는 것입니다. 즉, Greedy Action을 선택하는 방법입니다. 만약 가장 높은 Value를 가지는 Action이 2개 이상 있다면 그 중 무작위로 선택한다고 가정합니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[A_t \doteq \underset{a}{\operatorname{argmax}} \ Q_t (a) \tag{2.2}\]

&lt;p&gt;$\underset{a}{\operatorname{argmax}} \ Q_t (a)$는 $Q_t(a)$을 최대값으로 만드는 $a$라는 뜻입니다. 식 (2.2)과 같은 Greedy 방법은 현재 알고 있는 정보를 기반으로 즉각적인 Reward를 최대화하는 전략입니다. 다시 말해, 더 나은 Action이 있을지도 모르지만 Reward가 낮은 Action을 Exploration하는데 전혀 시간을 들이지 않습니다.&lt;/p&gt;

&lt;p&gt;이에 대한 대안으로 $\epsilon$-greedy 방법이 있습니다. 이것은 대부분 Greedy하게 Action을 선택하지만 작은 확률($\epsilon$)로 무작위 Action을 선택하는 방법입니다. 이 방법은 시간이 무한대에 가까워지면 모든 Action 또한 무한대로 샘플링되므로 모든 $Q_t (a)$가 $q_{*}(a)$에 수렴하는 것을 보장한다는 장점이 있습니다. $\epsilon$-greedy 방법은 강화학습에서 Action을 선택하는 전략으로 매우 많이 사용하기 때문에 나중에 더 자세히 설명하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-10-armed-testbed&quot;&gt;The 10-armed Testbed&lt;/h2&gt;

&lt;p&gt;greedy 방법과 및 $\epsilon$-greedy 방법을 비교하기 위해 구체적인 예제를 하나 살펴보도록 하겠습니다. 예제는 $k$를 10으로 설정하고 무작위로 2000개의 $k$-armed bandit 문제를 생성하였습니다. 각 Action $a$에 대해 $q_{*}(a)$는 평균이 0이고 분산이 1인 정규분포에 따라 만들어졌습니다. 또한 Agent가 Action을 선택하였을 때 받는 Reward 또한 평균이 $q_{*}(A_t)$이고 분산이 1인 정규분포를 따르도록 설정하였습니다. 이러한 분포를 그림으로 표현하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 테스트한 작업 모음을 10-armed testbed라고 부릅니다. 어떤 학습 방법이든 한 번에 1000개 이상의 시간 단계 정도면 학습 성능과 동작이 얼마나 향상되었는지 대략적으로 측정할 수 있습니다. 이것을 하나의 &lt;span style=&quot;color:red&quot;&gt;Run&lt;/span&gt;이라고 합니다. 본 예제에는 총 2000개의 독립적인 Run에 대해 학습 알고리즘의 평균적인 측정값을 구했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 $\epsilon$의 값에 따른 성능을 비교한 결과입니다. 성능 비교는 크게 2가지 관점에서 측정하였는데, 위의 그래프는 평균 Reward를 나타내고 아래의 그래프는 Optimal Action을 선택하는 비율을 나타냅니다. $\epsilon$은 각각 0.1과 0.01, 그리고 0(=Greedy)의 3가지로 나누었습니다. Greedy 방법은 초기에는 다른 두 비교군에 비해 좋은 성능을 보이지만, Exploration을 전혀 하지 않기 때문에 적당히 좋은 Action에만 고집하게 됩니다. 시간이 흐를수록 다른 비교군에 비해 성능이 월등히 떨어짐을 알 수 있습니다. $\epsilon$이 0.1일 때와 0.01일 때를 비교해보면 0.1인 경우가 비교적 빠른 시간 안에 높은 결과를 보임을 알 수 있습니다. 하지만 이것은 시간 단계가 1000에서 멈추었기 때문이고, 더 많은 시간이 주어진다면 개선 시간은 느릴지언정 결국 0.01일 때 0.1보다 높은 성능을 보일 가능성이 높습니다. 이 두 가지 방법의 장점을 모두 활용하기 위해 초기에는 $\epsilon$의 값을 높게 잡다가 시간이 지남에 따라 줄이는 것도 하나의 방법이 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;$\epsilon$-greedy 방법과 Greedy 방법 중 어느 것이 더 우수한지는 환경이 어떻게 주어졌는지에 따라 다릅니다. 예를 들어 Reward의 분산이 1이 아니라 10이라면 그만큼 Exploration이 더 많이 필요하게 되며, 이 때는 $\epsilon$-greedy 방법이 확실히 우수한 방법이라고 말할 수 있습니다. 하지만 반대로 Reward의 분산이 0이라면 오히려 Exploration을 하는 만큼 손해가 커지게 되므로, Greedy 방법이 더 우수한 결과가 나올 수 있습니다. 이렇게 되면 Deterministic 문제에서는 Greedy 방법만 사용해야할 것 같지만, 꼭 그렇지도 않습니다. 예를 들어 Deterministic 문제에서도 각 Action이 가지는 Reward가 시간이 지남에 따라 변경된다면 Exploration이 반드시 필요하게 되므로 $\epsilon$-greedy 방법을 사용해야하기 때문입니다. 이런 케이스는 생각보다 흔하게 접할 수 있으므로 앞으로 배울 강화학습에서 많이 다룰 예정입니다. 심지어 Deterministic이면서 Stationary인 경우라도 의사 결정 정책 자체가 변경되는 경우도 있으므로 Exploration을 아예 배제할 수는 없습니다. 즉, 강화학습에서는 Exploration과 Exploitation 사이의 균형이 필요합니다.&lt;/p&gt;

&lt;h2 id=&quot;incremental-implementation&quot;&gt;Incremental Implementation&lt;/h2&gt;

&lt;p&gt;지금까지 다루었던 방법들은 모두 평균적으로 어느 정도의 Reward를 얻었는지에 따라 Action에 대한 Value를 추정했습니다. 이번에는 평균 Reward를 어떻게 효율적으로 계산할 것인지에 대해 알아보도록 하겠습니다. 먼저 표기법을 단순히하기 위해 몇 가지 기호를 정의하겠습니다. $R_i$는 이 Action을 $i$번째 선택한 후 받은 Reward를 나타냅니다. 그리고 $Q_n$은 이 Action을 $n-1$번 선택한 후에 Action에 대한 Value를 추정한 값을 의미합니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[Q_n \doteq \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}\]

&lt;p&gt;이것을 그대로 구현하려면 $Q_n$이 필요할 때마다 저장된 모든 Reward를 체크하여 계산을 수행해야 합니다. 문제는 이렇게 하면 시간이 지날 때마다 Reward를 저장해야하는 메모리가 계속 증가할 수밖에 없습니다. 그렇기 때문에 아래처럼 &lt;span style=&quot;color:red&quot;&gt;Recurrence Relation&lt;/span&gt; 형태로 만들면 Reward를 저장할 필요 없이 간단하게 계산할 수 있습니다.&lt;/p&gt;

\[\begin{align}
Q_{n+1} &amp;amp;= \frac{1}{n} \sum_{i=1}^n R_i \\
&amp;amp;= \frac{1}{n} \left( R_n + \sum_{i=1}^{n-1} R_i \right) \\
&amp;amp;= \frac{1}{n} \left( R_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \right) \\
&amp;amp;= \frac{1}{n} \bigg( R_n + (n-1) Q_n \bigg) \\
&amp;amp;= \frac{1}{n} \bigg( R_n + n Q_n - Q_n \bigg) \\
&amp;amp;= Q_n + \frac{1}{n} \bigg[ R_n - Q_n \bigg] \tag{2.3}
\end{align}\]

&lt;p&gt;이 Recurrence Relation은 $n=1$부터 가능합니다. Recurrence Relation을 사용함으로써 메모리에 $Q_n$, $n$만 저장하면 되고 계산도 간단한다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;이전에 배운 $\epsilon$-greedy 방법과 Recurrence Relation을 사용하여 완전한 Bandit 알고리즘을 Pseudocode (의사코드)로 작성하면 다음과 같습니다. 함수 $bandit(a)$는 Action $a$를 선택하고 Reward를 반환한다고 가정합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tracking-a-nonstationary-problem&quot;&gt;Tracking a Nonstationary Problem&lt;/h2&gt;

&lt;p&gt;평균을 사용하여 Action의 Value를 측정하는 방법은 Reward가 고정된 Stationary Problem에 적합합니다. 하지만 같은 Action을 하더라도 시간에 따라 Reward가 변하는 Nonstationary Problem에서는 과거의 Reward보다 최근의 Reward에 더 많은 가중치를 두는 것이 합리적입니다. 이것을 위해 가장 많이 쓰이는 방법은 Step-size parameter를 사용하는 것입니다. 예를 들어, 직전에 다룬 Recurrence Relation을 다음과 같이 수정해보겠습니다.&lt;/p&gt;

\[Q_{n+1} \doteq Q_n + \alpha \bigg[ R_n - Q_n \bigg] \tag{2.5}\]

&lt;p&gt;이전 식에서 $\frac{1}{n}$를 상수 $\alpha \in (0,1]$로 수정한 것입니다. 하지만 수정된 Recurrence Relation을 다시 풀어써보면 어떤 차이가 있는지 알 수 있습니다.&lt;/p&gt;

\[\begin{align}
Q_{n+1} &amp;amp;= Q_n + \alpha \bigg[ R_n - Q_n \bigg] \\ \\
&amp;amp;= \alpha R_n + (1 - \alpha) Q_n \\ \\
&amp;amp;= \alpha R_n + (1 - \alpha) \left[ \alpha R_{n-1} + (1 - \alpha) Q_{n-1} \right] \\ \\
&amp;amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\ \\
&amp;amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + \\ \\
&amp;amp; \qquad \qquad \cdots + (1 - \alpha)^{n-1} \alpha R_1 + (1 - \alpha)^n Q_1 \\ \\
&amp;amp;= (1 - \alpha)^n Q_1 + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} R_i \tag{2.6}
\end{align}\]

&lt;p&gt;식 (2.6)의 마지막 부분을 보시면 $(1 - a)^n + \sum_{i=1}^n \alpha (1 - \alpha)^{n-i} = 1$ 이므로 Weight $\alpha$의 합이 1이 됨을 알 수 있습니다.  그렇기 때문에 이것을 &lt;span style=&quot;color:red&quot;&gt;Weighted Average&lt;/span&gt;이라고 부릅니다. 여기서 Reward $R_i$에 부여되는 Weight $(1 - \alpha)^{n-i}$는 이전 Reward가 많이 누적될수록 기하급수적으로 감소합니다. 따라서 이것을 &lt;strong&gt;Exponential Recency-weighted Average&lt;/strong&gt; 라고도 부릅니다.&lt;/p&gt;

&lt;p&gt;이 Step-size parameter는 상황에 따라 변경할 수도 있습니다. Action $a$를 $n$번째 선택했을 때의 Step-size parameter를 $\alpha_n (a)$라고 정의합니다. 만약 $\alpha_n (a) = \frac{1}{n}$으로 설정할 경우 이전에 다루었던 평균과 동일한 식이 됩니다. 이 경우에는 &lt;strong&gt;Law of Large number&lt;/strong&gt;에 의해 실제 Action의 Value에 수렴하지만, 모든 Sequence에 대한 수렴이 보장되지는 않습니다. $\alpha_n (a)$이 수렴하기 위해서는 다음 두 조건을 모두 만족해야 합니다.&lt;/p&gt;

\[\sum_{n=1}^{\infty} \alpha_n (a) = \infty \tag{2.7.1}\]

\[\sum_{n=1}^{\infty} \alpha^2_n (a) &amp;lt; \infty \tag{2.7.2}\]

&lt;p&gt;식 (2.7.1)은 초기 조건이나 무작위 변동에 상관없이 Step에 따라 충분히 크다는 것을 보장해야 한다는 뜻이고, 식 (2.7.2)는 Step이 수렴할 만큼 충분히 작아지는 것을 보장한다는 뜻입니다. 간단한 예시를 들면, $\alpha_n (a) = \frac{1}{n}$인 경우 두 조건이 모두 충족되므로 수렴하지만, $\alpha_n (a) = \alpha$인 경우 두 번째 조건이 충족되지 않기 때문에 수렴하지 않습니다.&lt;/p&gt;

&lt;p&gt;강화학습에서 자주 사용되는 Nonstationary 환경에서는 $\alpha_n (a)$를 정하는 것 또한 하나의 문제가 됩니다. 조건 (2.7.1)과 (2.7.2)를 모두 만족해도 수렴 속도까지 보장하지는 않기 때문입니다. 어떤 경우에는 너무 느리게 수렴할 수도 있기 때문에 적절한 $\alpha_n (a)$이 필요합니다. 이론 부분에서는 이러한 연구 또한 진행되고 있지만, 여기서는 이 부분까지 다루지는 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;optimistic-initial-values&quot;&gt;Optimistic Initial Values&lt;/h2&gt;

&lt;p&gt;지금까지 다루었던 Action에 대한 Value를 추정하는 방법들은 모두 초기 추정치인 $Q_1 (a)$에 상당 부분 의존합니다. 그러므로 이것은 초기 추정치에 의해 &lt;strong&gt;Biased&lt;/strong&gt;되는 위험성이 있습니다. 처음 제안하였던 Sample-average 방법에서는 모든 Action을 한 번 이상 선택하면 Bias가 사라지지만, Step-size parameter를 사용하는 방법에서는 시간이 지남에 따라 감소될지언정 Bias가 사라지지 않습니다. 다만 실제로 이러한 Bias는 일반적으로 크게 문제가 되지 않으며 때때로 오히려 도움이 될 수도 있습니다. 왜냐하면 예상되는 Reward에 대한 대략적인 값을 사전 지식으로 제공할 수도 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;Action에 대한 초기 값은 Exploration을 유도하는 방법으로 간단하게 사용할 수 있습니다. 10-armed testbed에서 초기 Action에 대한 Value을 0 대신 5로 설정했다고 가정해봅시다. $q_{*}(a)$는 평균이 0이고 분산이 1인 정규분포에서 선택되었기 때문에 초기 추정치를 5로 설정하는 것은 매우 &lt;strong&gt;Optimistic&lt;/strong&gt;합니다. 쉽게 설명하면, 처음에 어떤 Action을 선택하든 얻게 되는 Reward는 초기 추정치인 5보다 작습니다. 그렇기 때문에 다음 단계에서는 다른 Action을 선택할 가능성이 높고, 추정치가 수렴하기 전까지 모든 Action을 여러 번 선택하게 됩니다. 따라서 초기 추정치를 Optimistic하게 설정한다면 Agent가 Exploration을 많이 하도록 유도할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 10-armed testbed에서 $Q_1 (a) = 5$로 설정한 Greedy 방법과 $Q_1 (a) = 0$으로 설정한 $\epsilon$-greedy 방법의 성능을 비교한 결과입니다. 초기에는 낙관적으로 설정한 Greedy 방법이 Exploration을 더 많이 하기 때문에 성능이 좋지 않지만, 시간이 지남에 따라 Exploration이 감소하기 때문에 결국에는 더 좋은 성능을 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 간단한 꼼수는 Stationary 문제에 대해 상당히 효과적이지만, 이 방법에서의 Exploration은 결국 일시적이기 때문에 Exploration이 자주 필요한 Nonstationary 문제에서는 별로 도움이 되지 않습니다. 하지만 이러한 꼼수는 구현이 매우 간단하고, 일부이긴 하지만 몇 가지 문제에서 실제로 유용하게 사용할 수 있습니다. 앞으로 이 책에서는 이러한 간단한 꼼수들을 몇 개 더 소개할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;upper-confidence-bound-action-selection&quot;&gt;Upper-Confidence-Bound Action Selection&lt;/h2&gt;

&lt;p&gt;Action에 대한 Value를 추정하는 것은 정확성이 보장되지 않기 때문에 꾸준한 Exploration이 필요합니다. Greedy 방법은 현재 가장 좋아보이는 Action을 선택하지만 실제로 다른 Action이 더 가치가 높을 수도 있기 때문입니다. $\epsilon$-greedy 방법은 낮은 확률로 무작위 Action을 선택하지만, 이 무작위성 때문에 Exploration이 부족하여 추정하지 못했던 Action이나 추정값이 최대값으로 예상되는 Action을 따로 고려하지 않습니다. 이런 특수한 경우를 고려하기 위해 다음과 같은 방법을 생각해볼 수 있습니다.&lt;/p&gt;

\[A_t \doteq \underset{a}{\operatorname{argmax}} \ \left[ Q_t (a) + c \sqrt{\ln t \over N_t (a)} \, \right] \tag{2.10}\]

&lt;p&gt;$N_t (a)$는 시간 $t$까지 Action $a$를 선택한 횟수이고 $c &amp;gt; 0$는 Exploration의 정도를 나타내는 상수입니다. $N_t (a)$가 만약 0이라면, $a$는 가장 가치가 큰 Action으로 취급됩니다.&lt;/p&gt;

&lt;p&gt;이러한 방법을 &lt;span style=&quot;color:red&quot;&gt;Upper Confidence Bound (UCB)&lt;/span&gt;라고 부르며 제곱근 항은 Action $a$에 대한 추정값의 불확실성, 또는 분산의 척도를 의미합니다. 즉, 이것의 최대값은 Action의 가치에 대한 상한선으로 볼 수 있으며 $c$는 신뢰 수준을 결정합니다. $a$가 선택될 때마다 $N_t (a)$가 증가하므로 (=분모가 증가하므로) 불확실성이 감소된다고 볼 수 있습니다. 반면에 다른 Action이 선택된다면 $t$가 증가하므로 (=분자가 증가하므로) 불확실성이 증가됩니다. 즉, 이것은 자주 선택된 Action을 또 다시 선택하는 빈도를 줄이면서 모든 Action을 선택할 수 있는 방법이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 10-armed testbed에서 UCB를 사용한 결과입니다. 그래프에서는 UCB가 잘 수행되는 것으로 보이지만, 이 책에서 다룰 일반적인 강화학습에 확장하기에는 어렵습니다. 대표적으로 계속 언급하고 있는 Nonstationary 문제나, State가 매우 많은 경우에는 적용할 수 없으므로 이런 방법도 있다 정도만 이해하고 넘어가시면 되겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-bandit-algorithms&quot;&gt;Gradient Bandit Algorithms&lt;/h2&gt;

&lt;p&gt;지금까지는 Action을 선택할 때 Action의 Value를 추정하는 방법을 사용하였습니다. 이번에는 Action을 선택하는 또 다른 방법을 하나 소개하려고 합니다. 여기서는 각 Action $a$에 대한 &lt;span style=&quot;color:red&quot;&gt;Preference&lt;/span&gt; $H_t (a) \in \mathbb{R}$를 사용합니다. Preference가 높을 수록 해당 Action이 더 많이 선택되지만, 이것이 많은 Reward를 받는다는 뜻은 아닙니다. 단지 어떤 Action이 다른 Action에 비해 상대적으로 Preference가 높은지만을 나타내는 지표입니다. 모든 Action의 Preference에 1000을 더하면 확률에 영향이 없으며, 다음과 같이 Soft-max 분포로 나타납니다. (Soft-max 분포는 Gibbs, 또는 Boltzmann 분포라고도 부릅니다)&lt;/p&gt;

\[Pr \left\{ A_t = a \right\} \doteq \frac{e^{H_t (a)}}{\sum_{b=1}^k e^{H_t (b)}} \doteq \pi_t (a) \tag{2.11}\]

&lt;p&gt;$\pi_t (a)$는 시간 $t$에서 Action $a$를 선택할 확률을 의미합니다. 식 (2.11)에서 처음에는 모든 Action에 대해 Preference가 동일하므로(즉, 모든 $a$에 대해 $H_1 (a) = 0$) 모든 Action이 선택될 확률은 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stochastic Gradient Ascent&lt;/strong&gt;를 기반으로 하는 Soft-max Action Preference를 학습하는 알고리즘이 있습니다. 각 단계에서 Action $A_t$를 선택하고 Reward $R_t$를 받은 후, Action에 대한 Preference는 다음과 같이 업데이트됩니다.&lt;/p&gt;

\[H_{t+1} (A_t) \doteq H_t (A_t) + \alpha (R_t - \bar{R}_t) (1 - \pi_t (A_t) \qquad \text{and} \tag{2.12.1}\]

\[H_{t+1} (a) \doteq H_t (a) - \alpha (R_t - \bar{R}_t) \pi_t (a) \qquad \text{for all} \ a \neq A_t \tag{2.12.2}\]

&lt;p&gt;여기서 $\alpha &amp;gt; 0$은 이전에 언급했던 Step-size parameter이고, $\bar{R}_t \in \mathbb{R}$는 시간 $t$를 제외된 평균 보수입니다. (단, $\bar{R}_1 \doteq R_1$) $\bar{R}_t$의 계산법은 Incremental Implementation이나 Tracking a Nonstationary Problem에서 설명한 것과 동일합니다. $\bar{R}_t$의 역할은 Reward가 상대적으로 얼마나 높은지를 측정하는 &lt;span style=&quot;color:red&quot;&gt;Baseline&lt;/span&gt; 역할을 합니다. Reward가 Baseline보다 높으면 미래에 $A_t$를 선택할 확률이 증가하고, Baseline보다 낮으면 확률이 감소하는 방식입니다. 선택한 Action은 식 (2.12.1)로, 선택하지 않은 Action은 식 (2.12.2)로 계산됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 10-armed testbed에서 Gradient bandit 알고리즘의 결과를 나타냅니다. 여기서 $q_*(a)$는 평균이 4, 분산이 1인 정규분포에 따라 선택되었습니다. Reward의 평균을 올려도 새로운 Reward가 바로 반영되는 Baseline으로 인해 Gradient bandit 알고리즘에 전혀 영향을 끼치지 않습니다. 하지만 만약 Baseline을 제외한다면(즉, $\bar{R}_t = 0$) 그래프에서 보여주는 것처럼 성능이 크게 저하됩니다.&lt;/p&gt;

&lt;h2 id=&quot;contextual-bandits&quot;&gt;Contextual Bandits&lt;/h2&gt;

&lt;p&gt;지금까지는 현재 하고 있는 학습을 다른 상황이나 작업과 연관시킬 필요가 없는 경우만 고려했습니다. 즉, Agent가 고정된 최대 Reward를 얻는 Action(Stationary인 경우)을 찾거나, 시간이 지남에 따라 변경되는 최적의 Action(Nonstationary인 경우)을 찾기 위해 노력했습니다. 그러나 일반적인 강화학습에서는 여러 상황이나 환경이 주어질 수 있으며, 이 때는 Policy 자체를 학습할 필요가 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 몇 가지 다른 $k$-armed bandit 작업이 있고 각 단계에서 무작위로 하나만 선택한다고 가정합니다. 따라서 단계마다 선택되는 작업은 무작위로 변경됩니다. 각 작업이 선택되는 확률이 (시간이 지남에 따라) 변경되지 않으면 그냥 단일 Stationary $k$-armed bandit 작업과 동일합니다. 하지만 여기서는 작업이 선택될 때 어떻게 바뀌었는지 알 수 있는 힌트가 주어진다고 가정해봅시다. 슬롯머신으로 치면 슬롯머신 화면의 색이 변하는 것처럼 말입니다. 빨간색 화면이 나오면 첫 번째 레버를 선택하고, 초록색 화면이 나오면 두 번째 레버를 선택한다라는 식으로 결정하는 것이 바로 Policy를 학습하는 것입니다.&lt;/p&gt;

&lt;p&gt;이렇게 최적의 Action을 검색하기 위해 Trial-and-error와 주어진 상황을 &lt;strong&gt;Association&lt;/strong&gt;시키는 작업이 포함되기 때문에 &lt;span style=&quot;color:red&quot;&gt;Associative Search&lt;/span&gt;이라고 부릅니다. Associative Search는 또 다른 말로 &lt;span style=&quot;color:red&quot;&gt;Contextual Bandits&lt;/span&gt;라고도 부릅니다. Associative Search는 $k$-armed bandit 문제와 전체 강화학습 문제의 중간에 위치해 있습니다. Policy에 대한 학습을 포함한다는 점에서 강화학습과 비슷하지만, 각 Action이 즉각적인 Reward에만 영향을 미친다는 점에서 $k$-armed bandit 문제와 같습니다. Action이 Reward 뿐만 아니라 다음 상황에 영향을 미치도록 설정한다면 강화학습이라고 말할 수 있습니다. 다음 장에서 이에 대해 더 자세히 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;2장에서는 Exploration과 Exploitation의 균형을 맞추는 간단한 방법들을 몇 가지 소개했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\epsilon$-greedy 방법은 낮은 확률로 무작위 Action을 선택함으로써 Exploration을 합니다.&lt;/li&gt;
  &lt;li&gt;UCB 방법은 적게 선택된 Action이 선택될 확률을 높임으로써 Exploration을 합니다.&lt;/li&gt;
  &lt;li&gt;Gradient bandit 알고리즘은 각 Action에 대한 Value 대신 Preference를 추정하여 Exploration을 합니다.&lt;/li&gt;
  &lt;li&gt;초기값을 Optimistic하게 설정하는 간단한 방법으로 간단한 Exploration이 가능합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 방법들 중 어느 것이 가장 좋은지 판단하기는 어렵습니다. $k$-armed testbed 문제에서 같은 방법을 사용했더라도 Step-size parameter가 달라지면 성능의 차이가 확연히 드러났습니다. 따라서 여기서는 보기 쉽도록 각 방법마다 적절한 매개변수 구간에서 1000 단계가 지났을 때의 결과만 비교해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/2. Multi-armed Bandits/RL 02-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 성능 비교에서 재밌는 점은 모든 방법이 &lt;strong&gt;Convex&lt;/strong&gt;하다는 것입니다. 또한 각 방법마다 Step-size parameter를 중간 정도로 설정하는 것이 가장 뛰어난 성능을 보인다는 것도 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;교재에서는 $k$-armed bandit 문제를 해결하는 또 다른 방법으로 Gittins index 같은 것도 소개했지만, 내용상 중요한 부분이 아니라고 생각했기 때문에 여기에서는 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;2장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">Part I: Tabular Solution Methods</summary></entry><entry><title type="html">The Reinforcement Learning Problem</title><link href="http://localhost:4000/studies/the-reinforcement-learning-problem/" rel="alternate" type="text/html" title="The Reinforcement Learning Problem" /><published>2022-01-06T00:00:00+09:00</published><updated>2022-01-06T00:00:00+09:00</updated><id>http://localhost:4000/studies/the-reinforcement-learning-problem</id><content type="html" xml:base="http://localhost:4000/studies/the-reinforcement-learning-problem/">&lt;p&gt;사람은 여러 환경과 상호 작용하며 많은 것을 학습합니다. 맛있는 음식을 먹으면 기분이 좋다는 것을 통해 먹는 것을 좋아하게 되며 날카로운 것에 찔리고 나서는 날카로운 것을 멀리하게 되는 것이 그 예입니다. 이렇게 원인과 결과를 학습하게 되고 행동으로부터 어떤 결과가 나오는 지 알게 되며 목표를 달성하기 위해 무엇을 해야하는지 알게 됩니다. 이러한 상호 작용은 사람들에 대한 지식의 원천이라고 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;일반적인 기계학습에서는 주어진 데이터를 분석하여 &lt;strong&gt;Classification&lt;/strong&gt;하거나 &lt;strong&gt;Regression&lt;/strong&gt;하는 것을 목적으로 합니다. (기계학습 포스트를 참고하세요) 하지만 강화학습은 주어진 환경과 상호작용함으로써 목표에 도달하기 위해 어떤 행동을 하는 가에 중점을 둡니다.&lt;/p&gt;

&lt;p&gt;강화학습의 가장 큰 특징은 &lt;span style=&quot;color:red&quot;&gt;Trial-and-error&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;Delayed Reward&lt;/span&gt;입니다. 사람도 마찬가지지만, 강화학습의 Agent는 새로운 상황에 마주쳤을 때 어떤 선택이 최선의 선택인지 알 수 없으므로 막연하게 이것 저것 시도해보다가 실패를 되풀이하는 방식으로 최선의 선택을 찾습니다. 지연 보상의 의미는 내가 방금 행동한 것이 좋은 것인지 나쁜 것인지 바로 피드백 되는 것이 아니라, 하나의 Episode가 끝나고 나서야 그 행동에 대해 피드백할 수 있다는 것입니다. 이것은 추후 예제를 통해 다시 설명드리도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;p&gt;교재에서는 강화학습에 대해 다양한 예시를 보여주고 있습니다. 하지만 제가 봤을 때 교재의 예시가 그렇게 좋지 않아 보여서 제가 직접 몇 가지 예시를 보여드리도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 가장 유명한 사례로 바둑이 있습니다. 2016년에 Google Deepmind에서 보여준 AlphaGo가 대표적인 강화학습 프로그램입니다. 바둑은 검은색 돌부터 시작하여 흰색 돌과 서로 번갈아가며 하나씩 바둑판 위에 두어 승부를 겨루는 게임입니다. 각 플레이어는 매 턴마다 바둑판의 상황을 보고 어느 위치에 바둑알을 놓아야 결과적으로 이길 수 있을지 고민하는 게임이므로 강화학습에 알맞은 예시라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;자전거를 배우는 과정 또한 강화학습의 예시라고 볼 수 있습니다. 처음에는 자전거를 타고 제대로 서는 것 조차 못하지만, 핸들을 이리저리 돌려보고 페달을 밟아보면서 점점 자전거에 익숙해질 수 있게 됩니다. 이 과정에서 넘어지는 것이 시행착오라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;네트워크에서의 &lt;strong&gt;Routing&lt;/strong&gt;은 현재 라우터에서 어떤 라우터를 선택하고 이동해야 최종 목적지까지 도달할 수 있을지 결정하는 문제입니다. 각 라우터에 도달할 때마다 다음 라우터를 선택해야 하고, 빨리 도착할 수록 높은 보상을 받을 수 있습니다. 따라서 Routing도 강화학습을 적용할 수 있는 좋은 예시입니다.&lt;/p&gt;

&lt;h2 id=&quot;elements-of-reinforcement-learning&quot;&gt;Elements of Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;강화학습의 구성 요소는 먼저 &lt;span style=&quot;color:red&quot;&gt;Agent&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;Environment&lt;/span&gt;가 있습니다. 그 외에 4가지 주요 요소들로는 &lt;span style=&quot;color:red&quot;&gt;Policy&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;Reward Signal&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;Value Function&lt;/span&gt; 그리고 &lt;span style=&quot;color:red&quot;&gt;Model&lt;/span&gt;이 있습니다. 단, Model은 모든 문제에서 쓰이는 요소는 아닙니다. (대표적으로 강화학습의 한 종류인 Q-Learning은 Model을 사용하지 않습니다)&lt;/p&gt;

&lt;p&gt;먼저 Policy는 Agent가 주어진 시간에서 행동 방식으로 정의됩니다. 즉, Agent가 주어진 Environment에서 특정 State에 있을 때 어떤 Action을 취할지 나타낸다고 생각하시면 됩니다. 따라서 강화학습의 주요 목표는 Reward의 총합이 최대가 되는 &lt;span style=&quot;color:red&quot;&gt;Optimal Policy&lt;/span&gt;를 구하는 것입니다.&lt;/p&gt;

&lt;p&gt;다음으로 Reward Signal은 강화학습 문제의 목표를 정의합니다. 각 Step에서 Environment는 강화학습 Agent에게 Reward라는 실수 숫자를 보냅니다. 이것을 통해 Agent의 목표는 장기적인 관점에서 받는 Reward의 총합을 최대화하는 것으로 정의됩니다. 따라서 보상 신호는 Agent가 어떤 Action을 했을 때 그것이 좋은지 나쁜지를 정의하는 역할을 합니다. Reward가 높을 수록 좋은 Action을 했다는 뜻이며 낮을 수록 나쁜 Action을 했다는 뜻이 됩니다. 만약 어떤 Action을 했을 때 낮은 Reward를 받는 다면 다음에 같은 상황에 처했을 때 다른 Action을 하도록 Policy가 변경될 수 있습니다. 이러한 Reward Signal은 상수로 정의될 수도 있지만 함수로 정의될 수도 있습니다.&lt;/p&gt;

&lt;p&gt;Reward Signal이 즉각적은 의미에서 무엇이 좋은지를 나타내는 것과 다르게, Value Function은 장기적으로 무엇이 좋은지를 나타냅니다. 어떤 특정한 State의 Value는 해당 State에서 시작하여 앞으로 누적될 것으로 기대할 수 있는 총 Reward입니다. 이로 인해 당장 높은 Reward를 받을 수 있는 Action이 장기적으로 보았을 때 낮은 Value를 가질 수 있고, 그 반대 또한 성립할 수 있습니다. 따라서 Agent는 항상 가장 높은 Reward가 아니라 가장 높은 Value의 State를 얻을 수 있는 Action을 선택해야 합니다. 즉, Action에 대한 선택은 Value에 대한 판단을 기반으로 해야 합니다.&lt;/p&gt;

&lt;p&gt;하지만 높은 Value를 얻을 수 있는 Action을 선택하는 것은 쉽지 않습니다. Reward는 기본적으로 Environment에 의해 직접적으로 제공되지만, Value는 전체 시나리오를 관찰하여 계산하고, 추정하고, 이를 반복해야만 알 수 있기 때문입니다. 따라서 대부분의 강화학습 알고리즘에서는 Value를 정확하게 추정하는 방법을 중요하게 다루고 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로는 Environment에 대한 Model입니다. Model이 존재한다면 Environment가 어떻게 이루어져 있는지 Agent가 예측할 수 있게 되고, 어떻게 행동할 것인지에 대한 추론이 가능해집니다. 예를 들어, State와 Action이 주어진다면 Model은 다음 State와 다음 Reward를 예측할 수 있게 만들어줍니다. Model은 Planning에 사용되며 실제로 경험하기 전에 가능한 미래 상황을 고려하여 Action을 결정하는 모든 방법을 의미합니다. Model과 Planning을 사용하여 강화학습 문제를 해결하는 것을 Model-based 방법이라고 하며 Trial-and-error를 사용하여 강화학습 문제를 해결하는 것을 Model-free 방법이라고 합니다. Planning은 추후 8장에서 더 자세하게 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;limitation-and-scope&quot;&gt;Limitation and Scope&lt;/h2&gt;

&lt;p&gt;강화학습에서 State는 매우 중요한 요소입니다. 하지만 이 책에서는 State는 이미 주어진 것으로 가정하여 State를 새로 만들거나, 변경하거나, State Signal을 학습하는 부분은 다루지 않습니다. 대신 State가 주어졌을 때 어떤 Action을 취해야 하는 지를 집중적으로 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;또한 이 책의 많은 강화학습의 방법들은 Value Function을 추정하는 방법을 위주로 다루고 있습니다. 하지만 강화학습 문제(정확히는 최적화 문제)에서 Value Function을 예측하는 것이 반드시 필요한 것은 아닙니다. 예를 들어 Genetic Algorithm, Genetic Programming, Simulated Annealing 과 같은 최적화 문제들은 Value Function을 추정하지 않습니다. 이러한 Evolutionary Method는 가장 Reward가 높은 Policy와 무작위 변형 Policy를 섞고 반복합니다. 다만 이러한 방법들을 사용하기 위해서는 Policy의 범위가 충분히 작거나 쉽게 찾을 수 있도록 구조화 되어야 하고, 최적의 답을 도출하는 데 시간이 많이 필요한 단점이 있습니다. 반대로 이러한 Evolutionary Method들은 학습 Agent가 Environment의 State를 완전히 알 필요가 없다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;이 책에서 최적화 문제를 해결하는 데 사용하는 것은 Environment와 상호 작용하며 Agent가 직접 Policy를 학습하는 강화학습 방법입니다. 이렇게 Agent가 단독으로 상호 작용하며 세부 사항을 활용할 수 있는 방법은 Evolutionary Method보다 효율적인 경우가 많습니다. Evolutionary Method들은 Agent가 어떤 State를 거치고 어떤 Action을 선택하는지 알 수 없다는 단점이 있습니다. 어떤 경우에는 그러한 과정 자체가 중요할 수도 있기 때문입니다. 물론 Evolutionary Method와 함께 사용하는 강화학습 방법도 존재하지만, 이 책에서는 그러한 방법들이 다소 적합하지 않다고 생각하기 때문에 다루지는 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;an-extended-example--tic-tac-toe&quot;&gt;An Extended Example : Tic-Tac-Toe&lt;/h2&gt;

&lt;p&gt;강화학습의 일반적인 개념과 다른 접근 방식과 비교할 수 있도록 구체적인 예시를 하나 살펴보겠습니다. Tic-Tac-Toe는 보드게임 중 하나로 2명의 플레이어가 3 * 3 크기의 보드에서 교대로 플레이하는 게임입니다. 플레이어는 자신의 턴에 9개의 칸 중 비어있는 칸에 O 또는 X를 표기할 수 있으며 만약 한 플레이어가 가로, 세로, 대각선 중 하나를 자신의 기호(O 또는 X)로 연결하면 이기는 게임입니다. 만약 두 명 다 한 줄을 만들지 못하고 9칸을 채우면 무승부가 됩니다. 사실 이 게임은 필승 전략이 존재하는 게임이기 때문에 상대방 플레이어는 이 게임을 처음 해보는 상황이라고 가정하겠습니다. 이 게임을 이기기 위해서는 어떤 Policy을 취해야 할까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/1. The Reinforcement Learning Problem/RL 01-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임의 대한 전략을 구상할 때 빼놓을 수 없는 이론이 바로 &lt;span style=&quot;color:red&quot;&gt;Game Theory&lt;/span&gt;입니다. 하지만 Game Theory의 방법 중 하나인 Minimax 솔루션은 상대방 이득의 최대값을 최소화한다는 전략으로 플레이하는 것을 가정하기 때문에 이 문제에 적합하지 않습니다. Dynamic Programming 같은 순차적 최적화 방법은 모든 경우에 대해 최적의 솔루션을 계산할 수 있지만 상대방이 어떤 위치에 체크를 할 것인가에 대한 확률 정보를 포함한 완전한 데이터가 필요합니다. 이렇게 완전한 정보가 주어지는 경우는 현실에서 찾아보기 쉽지 않기 때문에 역시 적합하지 않습니다. 물론 이러한 정보는 경험을 통해 추정할 수는 있습니다. 만약 동일한 상대방과 수많은 게임을 플레이한다면 대략적으로 이 사람이 특정 위치에 체크할 확률을 알 수 있습니다. 이렇게 수많은 게임을 통해 상대방의 행동 패턴을 신뢰할 수 있을 정도가 되면 그 후에 동적 프로그래밍을 사용하여 최적의 솔루션을 계산할 수 있습니다. 추후 다룰 강화학습의 방법 중에는 이러한 메커니즘을 이용하는 경우도 있습니다.&lt;/p&gt;

&lt;p&gt;Evolutionary Method를 이 문제에 적용하려면 상대방에게 이길 확률이 높은 Policy를 먼저 탐색하는 것입니다. 이 문제에서의 Policy는 플레이어에게 모든 State (3 * 3 크기의 보드에서 발생할 수 있는 모든 경우의 수)에 대해 어떤 Action (내가 어디에 체크할 것인지)을 취할지 선택하는 규칙입니다. 각각의 경우의 수에서 상대방과 여러번 게임을 함으로써 이길 확률을 추정할 수 있습니다. 그 다음에는 이러한 Policy를 계속 생성하며 점진적인 개선을 함으로써 최대화하는 것이 Evolutionary Method가 됩니다. 대표적으로 Genetic Algorithm은 승리확률이 높은 Policy들을 섞어 새로운 Policy를 만들어냄으로써 최적의 Policy를 찾습니다.&lt;/p&gt;

&lt;p&gt;Value Function을 Tic-Tac-Toe 문제에 적용할 수도 있습니다. Dynamic Programming을 적용할 때와 같이 모든 가능한 State (경우의 수)에 대해 숫자를 매칭할 수 있도록 테이블을 생성합니다. 이 숫자는 해당 State에서 얼마나 이길 가능성이 높은지를 나타냅니다. 이 수는 Value Function을 갱신할 때마다 변경될 수 있으므로 현재 입력된 숫자는 최신 추정치라고 보시면 됩니다. 특정 State에 대해서 특정 Value를 도출 할 수 있으므로 &lt;strong&gt;함수&lt;/strong&gt;로 표현됩니다. 임의의 두 State A, B에 대하여 State A의 Value가 State B의 Value보다 높다면 State A는 State B보다 더 나은 것으로 판단합니다.&lt;/p&gt;

&lt;p&gt;그 후 마찬가지로 상대방 플레이어와 수많은 게임을 플레이합니다. 자신의 매 턴마다 가능한 선택지(다음 State)를 확인하여 대부분 가장 큰 Value를 가진 State로 갈 수 있는 곳에 체크를 합니다. 하지만 더 나은 State가 있을지도 모르기 때문에 가끔은 가장 큰 Value가 아닌 State를 무작위로 선택함으로써 다른 State의 Value를 갱신합니다. 이 방법을 요약하면 다음 그림과 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/1. The Reinforcement Learning Problem/RL 01-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임을 하는 동안 승률을 높일 수 있도록 State Value를 보다 정교하게 수정합니다. 위 그림에서의 빨간색 선은 자신이 행동한 후의 결과를 토대로 이전의 State Value를 수정한다는 의미입니다. 현재 시간을 $t$라고 했을 때 $S_t$는 현재 State, $S_{t+1}$은 다음 State를 의미합니다. $S_t$의 Value를 $V(S_t)$라고 하면 $V(S_t)$는 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[V(S_t) \leftarrow V(S_t) + \alpha \left[ V(S_{t+1}) - V(S_t)\right]\]

&lt;p&gt;위 식에서 $\alpha$는 &lt;span style=&quot;color:red&quot;&gt;Step-size parameter&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;Learning rate&lt;/span&gt;라고 부릅니다. 즉, 새로 학습한 Value를 얼마나 반영할지 나타내는 수치입니다. $\alpha$가 1에 가까울수록 새로 학습한 Value를 많이 반영하며, 0에 가까울수록 기존의 Value가 더 많이 반영됩니다. 위의 식과 같은 학습 방법을 &lt;span style=&quot;color:red&quot;&gt;Temporal-difference learning&lt;/span&gt;이라고 하는데, 이것은 추후 더 자세히 다룰 예정입니다. &lt;strong&gt;Difference&lt;/strong&gt;라는 이름이 붙은 이유는 $V(S_{t+1}) - V(S_t)$와 같이 연속된 두 State Value의 &lt;strong&gt;차이&lt;/strong&gt;가 반영되기 때문입니다.&lt;/p&gt;

&lt;p&gt;이 방법은 초기에 $\alpha$를 크게 잡아 학습을 시킨 다음, 시간이 지날수록 점점 감소시켜 Value가 수렴하도록 만듭니다. 또한 다음 State의 선택도 Value가 가장 큰 쪽을 선택하는 비율을 늘려간다면 (고정된) 상대방에 대해 Optimal Policy로 수렴합니다. 만약 상대가 고정된 전략을 사용하는 플레이어가 아니라 유동적으로 전략을 천천히 바꾸는 상대라면 $\alpha$의 값을 천천히 줄여나가되, 0이 되지 않도록 설정하는 것으로 대응할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 예제는 Evolutionary Method와 Value Function을 학습하는 방법 간의 차이점을 보여줍니다. Evolutionary Method는 Policy를 평가하기 위해 Policy를 고정하고 상대방과 많은 게임을 하거나 상대방의 Model을 사용하여 많은 게임을 시뮬레이션합니다. 수많은 게임을 통해 얼마나 승리했는지는 다음 Policy를 선택하는데 사용할 수 있지만, Policy를 변경하는 것은 많은 게임 후에야 가능하며 게임의 최종 결과만 사용됩니다. (즉, 게임 도중에 일어나는 일은 무시됩니다) 게임에서 중요한 것은 승리/패배의 여부이기 때문에 게임 중에 행했던 특정 선택은 (실제 게임에서) 얼마나 중요했는지와는 상관 없이 동등한 Value를 지니게 됩니다. 이와 반대로 Value Function은 개별 State를 평가합니다. Evolutionary Method과 Value Function 방법은 모두 Policy를 탐색하는 공통점이 있지만 Value Function은 게임 도중에 사용할 수 있는 정보(즉, State)를 활용한다는 차이점이 있습니다.&lt;/p&gt;

&lt;p&gt;이 예제를 통해서 강화학습의 몇 가지 주요한 기능을 알 수 있습니다. 첫째로, Environment(여기서는 상대방 플레이어)과 상호작용 하면서 학습하는 것에 중점을 둡니다. 둘째로, 명확한 목표가 있고 그에 따른 올바른 선택을 하기 위해서 플레이어의 선택에서 지연된 영향을 고려한 예측이나 계획이 필요합니다. 상대방의 Model을 사용하지 않고 계획 및 예측의 효과를 달성할 수 있다는 것이 강화학습 솔루션의 특징이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Tic-Tac-Toe 게임은 상대적으로 State의 수가 적은 편이지만 강화학습은 State Space가 매우 크거나 무한할 때도 사용할 수 있습니다. 예를 들어 Google Deepmind에서 개발한 AlphaGo는 강화학습을 사용하여 바둑을 플레이하는 프로그램입니다. 바둑은 Tic-Tac-Toe에 비해 보드의 크기가 매우 크고 흑돌과 백돌의 위치에 따른 경우의 수가 매우 많습니다. 이러한 Environment에서 Artificial Neural Network는 프로그램의 경험을 일반화할 수 있는 기능을 제공하므로 기존의 경험과 비슷한 State에서 저장된 정보를 기반으로 처음 보는 State에서도 Action을 선택할 수 있습니다. 최근 논문에서는 강화학습과 인공신경망 같은 지도학습을 융합하는 연구가 많은데, 강화학습에서 State Space가 매우 큰 경우에는 과거의 경험을 얼마나 적절하게 일반화할 수 있는지가 핵심이기 때문입니다. 물론 인공신경망이나 딥러닝 외에 방법이 없는 것은 아닙니다.&lt;/p&gt;

&lt;p&gt;Tic-Tac-Toe 예제에서는 Agent가 게임의 규칙과 같은 사전지식이 없이 학습하였으나 강화학습이 항상 백지에서 시작하는 것은 아닙니다. 오히려 사전 정보를 사용함으로써 효율적으로 학습할 수 있기도 합니다. (9장 참고) 또한 이와 반대로 State의 일부가 숨겨져 있거나, 학습 Agent가 여러 State를 동일하게 보일 때도 강화학습을 적용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;마지막으로, Tic-Tac-Toe 예제에서 플레이어는 어떤 수를 선택하는가에 따라 다음에 발생할 State를 알 수 있었습니다. 이를 위해서는 선택하지 않는 수에 대해 Environment가 어떻게 변할지 예측할 수 있는 게임 Model이 있어야 했습니다. Tic-Tac-Toe처럼 선택하지 않는 수에 대해서도 예측이 가능하면 좋지만, 다른 문제에서는 Action의 효과에 대한 단기 Model조차 부족합니다. 다행히도 강화학습에 Model이 필수는 아니지만 Model이 있거나 학습할 수 있는 경우 Optimal Policy를 쉽게 계산할 수 있습니다. (8장 참고)&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;강화학습은 목표 지향 학습과 의사 결정을 이해하고 자동화하기 위한 Computational 접근 방식입니다. 다른 기계학습과는 다르게 모범적인 Supervise나 Environment의 완전한 Model을 요구하지 않고 Environment와의 직접적인 상호작용으로부터 Agent의 학습을 강조하는 점이 특징입니다. 교재의 저자는 Environment와 상호 작용을 통해 학습할 때 발생하는 계산량을 해결하는 것이 첫 번째 분야라고 주장하고 있습니다.&lt;/p&gt;

&lt;p&gt;강화학습은 Markov Decision Process (MDP)의 프레임워크를 사용하여 State와 Action으로 이루어진 Environment에서 Agent가 얻는 Reward를 정의합니다. 이것은 인공지능 문제에서 필수적인 요소들을 표현하기 위함입니다. 이것들은 원인과 결과, 불확실성과 비결정론, 그리고 명시적인 목표의 존재가 포함됩니다.&lt;/p&gt;

&lt;p&gt;Value Function의 개념은 교재의 저자가 주장하는 강화학습 방법의 핵심입니다. Policy들 간의 효율적인 탐색을 위해 Value Function을 사용하여 각 Policy가 얼마나 뛰어난지, Policy에서 몇몇 중간 지점이들이 얼마나 중요한지를 파악할 수 있게 만드는 점이 Evolutionary Method들과 차이를 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;※ 1.7 Early History of Reinforcement Learning 파트는 생략합니다.&lt;/strong&gt;&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">사람은 여러 환경과 상호 작용하며 많은 것을 학습합니다. 맛있는 음식을 먹으면 기분이 좋다는 것을 통해 먹는 것을 좋아하게 되며 날카로운 것에 찔리고 나서는 날카로운 것을 멀리하게 되는 것이 그 예입니다. 이렇게 원인과 결과를 학습하게 되고 행동으로부터 어떤 결과가 나오는 지 알게 되며 목표를 달성하기 위해 무엇을 해야하는지 알게 됩니다. 이러한 상호 작용은 사람들에 대한 지식의 원천이라고 할 수 있습니다.</summary></entry><entry><title type="html">Prologue</title><link href="http://localhost:4000/studies/prologue/" rel="alternate" type="text/html" title="Prologue" /><published>2022-01-01T00:00:00+09:00</published><updated>2022-01-01T00:00:00+09:00</updated><id>http://localhost:4000/studies/prologue</id><content type="html" xml:base="http://localhost:4000/studies/prologue/">&lt;p&gt;안녕하세요, 2022년에는 새로운 주제로 글을 써볼까 합니다. 예전부터 제가 공부하고 있던 강화학습(Reinforcement Learning) 내용을 정리해보고 싶었는데, 이런 저런 일들로 여건이 되지 않아 미루고 있었습니다. 한동안 다른 문제로 골머리를 썩히다가 이번에 논문 때문에 강화학습을 다시 공부할 일이 생겨 겸사겸사 내용을 정리하고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/0. Prologue/RL 00-01.jpg&quot; alt=&quot;&quot; width=&quot;300&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;강화학습 포스트에 참고할 교재는 대부분 학교에서 널리 사용하고 있는 Richard Sutton 교수님의 Reinforcement Learning : An Introduction 이라는 책입니다. 워낙 유명한 책이다보니 한국 서점에서도 쉽게 구할 수 있을 뿐만 아니라 번역본까지 있는 것을 확인했습니다. (사실 저 말고도 이 책에 대해 자세히 정리해놓은 블로그가 있기도 합니다) 게다가 저자분께서 교재 PDF를 인터넷에 공개해놓은 상태이기 때문에 굳이 구입할 필요가 없는 것이 장점이기도 합니다. 교재 PDF는 &lt;a href=&quot;http://www.incompleteideas.net/book/the-book-2nd.html&quot;&gt;이 곳&lt;/a&gt;에서 다운받을 수 있습니다.&lt;/p&gt;

&lt;p&gt;필요한 부분에서는 소스코드를 같이 첨부할 예정인데, 언어는 Python을 사용하려고 합니다. 사실 저는 C언어에 익숙해서 지금까지 대부분 C언어로 시뮬레이션을 했는데, 강화학습 관련 시뮬레이션을 구현하려니 C언어로는 너무 복잡하여 어쩔 수 없이 강화학습 관련 라이브러리가 많은 Python이 낫겠다는 생각이 들었습니다. Python을 제대로 공부해본 적이 없어 많이 헤멜 것 같은데, 최대한 빨리 익숙해져서 시뮬레이션 프로그램을 구현하고 싶습니다.&lt;/p&gt;

&lt;p&gt;따라서 가급적이면 각 주제에 맞는 프로그램을 작성하여 소스 코드도 첨부하려고 노력하겠습니다. 다만 아무래도 대학원 단계의 주제이기 때문에 포스트를 작성하는 빈도는 줄어들 것으로 생각하고 있습니다. 또한 분량은 기계학습 때와 마찬가지로 1장당 한 개의 포스트로 정리하려고 합니다.&lt;/p&gt;

&lt;p&gt;작성하는데 부족한 부분이 있을 수 있기 때문에, 일단 먼저 각 포스트를 대략적으로 작성해놓고 추후 보충하는 방식으로 진행하려고 합니다. 틀린 부분이나 의견은 언제든지 댓글로 남겨주시면 확인하고 반영하겠습니다. 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">안녕하세요, 2022년에는 새로운 주제로 글을 써볼까 합니다. 예전부터 제가 공부하고 있던 강화학습(Reinforcement Learning) 내용을 정리해보고 싶었는데, 이런 저런 일들로 여건이 되지 않아 미루고 있었습니다. 한동안 다른 문제로 골머리를 썩히다가 이번에 논문 때문에 강화학습을 다시 공부할 일이 생겨 겸사겸사 내용을 정리하고자 합니다.</summary></entry><entry><title type="html">Epilogue</title><link href="http://localhost:4000/studies/epilogue/" rel="alternate" type="text/html" title="Epilogue" /><published>2019-11-16T00:00:00+09:00</published><updated>2019-11-16T00:00:00+09:00</updated><id>http://localhost:4000/studies/epilogue</id><content type="html" xml:base="http://localhost:4000/studies/epilogue/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 대망의 마지막 장입니다. 이번 장에서는 지금까지 배웠던 기계학습을 정리하고 강의에서 다루지 못했던 기계학습에 대해 간략하게 설명하고 마무리합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 크게 4가지의 소주제로 이루어져 있습니다. 가장 먼저 지금까지 배운 기계학습을 간단하게 정리하고, 본 강의에서 다루지 못했던 기계학습 중 Baysian Learning과 Aggregation Methods를 간략하게 소개합니다. 마지막으로는 강의에 큰 도움을 줬던 분들에게 감사를 표한다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;the-map-of-machine-learning&quot;&gt;The Map of machine learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;기계학습&lt;/strong&gt;을 다루는 책들은 굉장히 많은 내용을 소개하고 있습니다. 이것들 중 일부분을 나열하면 위와 같이 정신이 없을 정도로 많은 주제가 있음을 알 수 있습니다. 어떤 것들이 있는지 대충 보시면 지금까지 다루었던 것들도 있지만, 그렇지 않은 것들도 있다는 것을 아실 겁니다.&lt;/p&gt;

&lt;p&gt;이렇게 보기 힘들게 주제들을 나열하면 머리만 아프고 이해도 힘드니, 강의에서는 좀 더 체계적인 방법으로 기계학습을 분류한 것을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 기계학습의 분야에는 크게 3가지 갈래가 있습니다. 첫째는 &lt;span style=&quot;color:red&quot;&gt;Theory (이론)&lt;/span&gt;, 두 번째는 &lt;span style=&quot;color:red&quot;&gt;Technique (기술)&lt;/span&gt;, 마지막으로 &lt;span style=&quot;color:red&quot;&gt;Paradigm (패러다임)&lt;/span&gt;이 있습니다. Paradigm은 학습 상황에 대한 다른 가정을 의미합니다. 수학적 가정이 아니라 Supervised Learning이나 Reinforcement Learning과 같은 다른 학습 상황을 다루는 가정이라는 뜻입니다. 이러한 가정을 할 때, 해결해야 할 문제는 기존의 기계학습의 문제와 다르기 때문에 공부해야 할 지식이 달라지게 됩니다. 그렇기에 이것을 Paradigm이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;가장 상위 개념인 Pradigm부터 이야기하면, Supervised Learning은 본 강의 대부분에서 다루는 주제였습니다. PLA부터 Support Vector Machine까지 대부분의 학습 알고리즘은 데이터에 Label이 있는 상황을 가정한 것이었기 때문입니다. 기계학습에서 가장 인기 있으면서도 유용한 주제입니다. Unsupervised Learning은 본 강의에서 많이 다루지는 않았지만, 최소한 Clustering이라는 핵심 아이디어를 배웠습니다. Reinforcement Learning은 첫 번째 강의에서만 잠깐 언급하였습니다. 좋은 행동을 하면 보상을 주고(강화하고) 나쁜 행동에 패널티를 부과하여 결국에는 좋은 해결책으로 수렴하게 만드는 방법입니다. 그 외에 Active Learning이나 Online Learning 등이 있지만 강의에서는 다루지 않았기에 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;다음으로 Theory 입니다. 기계학습에서 주요 이론은 Vapnic-Chervonenkis (VC) 이론입니다. 7장부터 시작하여 이후로도 지속적으로 기계학습의 일반화를 설명하기 위해 VC, 그리고 Bias-Variance 이론을 다루었습니다. Complextiy는 기계학습에서의 실용적인 부분입니다. 강의에서는 다루지 않았지만, 이것이 다항시간 내에 일어나는지, 혹은 그렇지 않은지를 통해 이론적인 알고리즘을 실질적으로 구현이 가능하지를 분석하는 이론입니다. 마지막으로 Bayesian은 기계학습을 확률의 한 갈래로 취급하는 이론입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Technique은 Model과 Method 2가지로 분리됩니다. Model은 지금까지 대부분의 강의에서 다루었던 부분입니다. 기본적인 Linear Model부터 시작하여 선형 분리가 되지 않는 데이터 집합에서 어떻게 처리해야하는지 Transform과 Neural Network 등을 배워나갔습니다. 그 이후로도 SVM, RBF를 포함하여 많은 영역을 다루었습니다. 이 외에도 Gaussian Process, Singular Value Decomposition (SVD), Graphical Model 등이 있지만, 강의에서 이 모든 것을 다루지는 못했습니다.&lt;/p&gt;

&lt;p&gt;Method는 Model에 관계없이 많은 영역을 다루기 때문에 매우 중요합니다. 강의에서는 Neural Network를 기점으로 발생할 수 있는 위험성인 Overfitting을 해결하기 위해 이 부분에 많은 시간을 투자하였습니다. Regularization과 Validation이 바로 대표적인 해결 방법이었습니다. Aggregation과 Input Processing은 강의에서 다루지 않은 요소입니다. 그중 Input Processing은 기계학습의 실무 과정에서 많이 다루는 실용적인 방법입니다.&lt;/p&gt;

&lt;p&gt;이번 장의 나머지 부분은 지금까지 나열했던 것 중 Bayesian과 Aggregation에 대해 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-learning&quot;&gt;Bayesian learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Bayesian Learning으로 넘어갑니다. 깊이있게 이 내용을 다루기보단, Bayesian 접근법의 기초를 다룰 것이며 언제 사용할 수 있는지, 단점은 무엇인지 정도만 짚을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오랜만에 Learning Diagram을 살펴봅시다. 이 Diagram에서 확률적인 요소는 두 가지가 있었습니다. 하나는 Data가 알 수 없는 특정한 확률분포에 의해 생성된다는 것이었고, 다른 하나는 Input $\mathbf{x}$가 주어졌을 때 Output $y$가 나올 확률이었습니다. 이것은 Noise로 인해 더 이상 Target Function이 아니라 Target Distribution으로 불리게 되었기 때문이었습니다,&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 이러한 확률적인 역할을 확장하는 개념입니다. 이전에 9장에서 Likelihood (가능도)를 잠시 떠올려보면, 가설 $h$와 Target Function $f$가 같다면 $\mathcal{D}$가 주어졌을 때 Output $y$를 얻을 확률을 의미하였습니다. 그래서 주어진 데이터를 제일 잘 표현할 수 있는 최대 확률을 계산하였습니다.&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 이와 반대로 접근하고 있습니다. 데이터가 이미 발생하였기 때문에, 수 많은 가설 중 Target Function을 가장 잘 반영하는 가설이 무엇인지를 찾는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 전공자들 사이에서도 의견이 분분합니다. 어떤 사람은 종교적인 수준으로 찬양을 하고, 어떤 사람은 완전히 쓰레기 같은 방법이라고 평가하기도 합니다. 강의에서도 이 점을 언급하며 Prior가 이러한 논쟁을 불러일으키는 주요 요소라고 합니다.&lt;/p&gt;

&lt;p&gt;우리는 주어진 데이터 $\mathcal{D}$ 하에서 가설 $h$와 Target Function $f$가 일치하기를 바랍니다. 이것은 Bayes’ Theorem에 의해 가운데 식처럼 변형할 수 있습니다. 이 중 $P(\mathcal{D} \mid h=f)$는 로지스틱 회귀 등을 통해 구할 수 있습니다. 그리고 $P(h=f)$는 필요 없는 요소라고 하던데, 사실 제가 Bayesian을 잘 모르기 때문에 왜 그런지는 아직 모르겠습니다. 어쨌든 이 둘을 곱하면 Joint Probability Distribution을 얻을 수 있고, $P(h=f \mid \mathcal{D})$는 이것에 비례합니다.&lt;/p&gt;

&lt;p&gt;Bayes’ Theorem에 나오는 항 중에 $P(h=f)$는 &lt;span style=&quot;color:red&quot;&gt;Prior&lt;/span&gt;라고 부르고 데이터를 얻기 전 가설 집합에 대한 믿음이라고 합니다. 이와 비슷하게 $P(h=f \mid \mathcal{D})$는 데이터를 얻은 후의 가설 집합에 대한 믿음이기 때문에 &lt;span style=&quot;color:red&quot;&gt;Posterior&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;Bayes’ Theorem을 통해 만약에 Prior가 주어진다면, 전체 가설 집합에 대한 전체 확률 분포를 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Prior의 예를 들어봅시다. 가설 $h$를 $d$차원 Perceptron 모델의 가중치 $\mathbf{w}$로 가정합니다.&lt;/p&gt;

&lt;p&gt;가중치 $\mathbf{w}$의 Prior는 각각의 $w_i$가 독립적이고 $[-1, 1]$에서 균등하다고 정했다고 가정합니다. 이것은 모든 가중치에 대한 확률 분포를 얻을 수 있다면, 어떤 가중치가 특정 가설에 기여하는지 알 수 있음을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 Prior는 가정에 불과합니다. 아주 간단한 사례로 알 수 없는 숫자를 찾는 문제가 있다고 생각해봅시다. 내가 아는 정보는 그 숫자가 -1과 1 사이라는 것뿐입니다. 누군가가 이것을 -1과 1 사이의 Uniform Distribution으로 모델링하고 &lt;strong&gt;이것은 -1과 1 사이의 내가 모르는 숫자가 있다는 것과 동일하다&lt;/strong&gt;라고 말한다면, 얼핏 듣기에는 그럴듯해 보이지만 그것은 틀린 사실입니다. 왜냐하면 Uniform Distribution에는 보이지 않는 많은 가정이 들어가 있기 때문입니다. 예를 들어 이 상황에서 많은 숫자를 뽑았을 때 Uniform Distribution의 평균은 0이지만, 원래 문제의 평균은 -1과 1 사이의 어떤 숫자이든 가능합니다. 실제로 이 문제와 동일한 것은 우리가 모르는 그 $x$가 $a$인 Delta Function으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 실제로 Prior를 알고 있다면, 모든 가설 $h$의 Posterior를 계산할 수 있기 때문에 완벽한 방법이 될 수 있습니다. 다시 말해, VC Analysis나 Regularization 같은 것도 필요 없이 가장 가능성 있는 가설을 선택할 수 있습니다. 심지어 모든 $\mathbf{x}$에 대해 가설의 평균 $\mathbb{E}(h(\mathbf{x}))$이나 Error Bar 또한 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 말은 예를 들어 주식 시장에서, 오늘의 주식 $\mathbf{x}$을 입력하면 가격 변동의 예상치나 그 예상치의 오차율까지도 계산이 가능하다는 말입니다. 상상할 수 있는 모든 것을 얻을 수 있다는 것입니다. 그렇기 때문에 정확한 Prior을 얻을 수 없다는 현실적인 문제로 인해, Bayesian 접근 방식을 선호하지 않는 과학자들도 있습니다. (사실 저도 이 방법은 좋아하지 않습니다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bayesian Learning이 올바르게 수행되기 위해선 둘 중 하나가 필요합니다. 첫째로, 유효한 Prior를 얻었을 경우에는 Bayesian Learning이 모든 다른 방법을 압도하는 해결방법이 됩니다.&lt;/p&gt;

&lt;p&gt;둘째로, Prior가 무관하게 만드는 것입니다. Prior를 가정할 때 점점 더 많은 데이터를 얻고 Posterior를 보면, Posterior가 데이터 집합에 의해 크게 영향을 받고 Prior에 의해 점점 덜 영향을 받습니다. 그렇기 때문에 Prior가 중요하지 않은 데이터가 충분하다면, Prior를 개념적 요소가 아닌 것으로 생각할 수 있습니다. 이렇게 하면 유효한 Prior는 아니게 되지만, Prior를 가지고 데이터를 얻게 되면 Posterior 계산이 쉽습니다. 이것을 Conjugate Prior라고 하며, 전체 함수에 대해 Posterior를 다시 계산할 필요가 없습니다. 간단하게 말해, 계산 과정을 매개변수화하는 용도로만 Prior를 사용한다는 뜻입니다.&lt;/p&gt;

&lt;h2 id=&quot;aggregation-methods&quot;&gt;Aggregation methods&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 Aggregation Method입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation은 모든 모델에 적용되는 방법입니다. 기본적인 아이디어는 다른 Solution을 결합한다는 것입니다. 예를 들어 컴퓨터 비전에서 사람의 얼굴을 구별하는 학습을 한다고 가정해봅시다. 여러 사용자에게 이 문제를 준다면 어떤 사람은 눈으로, 어떤 사람은 얼굴형으로, 어떤 사람은 이목구비의 위치로 사람을 구별할 것입니다. 총 관리자는 이들의 해결책을 결합해 최종적인 결과물을 만들 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 결합하는 방법에 대해 이야기해봅시다. 의외로 방법은 간단합니다. 만약에 Regression 문제라면 그저 평균을 내면 되는 것이고, Classification 문제라면 더 많은 사람이 분류한 것으로 판단하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아이디어만 보면 Aggregation과 2-Layer Learning이 비슷해 보입니다. 하지만 이 둘은 분명한 차이가 있습니다. 먼저 2-Layer Model은 모든 Unit이 동시에 참여합니다. 예를 들어 각 Unit에 Weight를 곱해서 더하는 방식으로 합치게 됩니다. 그에 반해 Aggregation은 각 Unit이 Training Data를 사용해 각자 학습하고, 각각의 Unit의 Output만을 사용해 최종 결과를 출력하는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation에는 두 종류가 있습니다. 하나는 &lt;span style=&quot;color:red&quot;&gt;Aftter the fact&lt;/span&gt; 입니다. 이것은 이미 Solution이 있음을 의미합니다. 예를 들어 이전에 다루었던 Netflix 추천 문제는, 이미 기존의 여러 해결 방법이 존재했고 그것들을 합치는 것만을 고려하면 되었습니다.&lt;/p&gt;

&lt;p&gt;다른 하나는 &lt;span style=&quot;color:red&quot;&gt;Before the fact&lt;/span&gt; 입니다. 이것은 결합하기 위한 Solution을 만드는 것입니다. 예를 들어, 주어진 데이터 집합 $\mathcal{D}$를 여러 번 독립적으로 &lt;strong&gt;Resampling (재생산)&lt;/strong&gt;하여 모두에게 다른 Sample Data를 주는 것입니다. 그렇게 해서 각각의 Unit을 학습시키고 합치는 방법이 Before the fact가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation을 하기 위한 방법으로는 Boost Algorithm이 있습니다. 각각의 가설을 순차적으로 만드는 아이디어 입니다. 위 슬라이드에 나온 그림처럼 4번째 Unit을 만드는 상황에서, Training Data를 1~3번까지 만든 가설을 참고하는 것입니다. 이렇게 되면 각 Unit이 서로 관련이 생겨버리므로, 이를 독립시키는 과정이 필요합니다.&lt;/p&gt;

&lt;p&gt;만약 몇 개의 Unit을 사용하여 Data를 60%는 올바르게, 40%는 틀리게 분류했다고 가정해봅시다. 그런데 다음 Unit에게 넘겨주는 데이터를 독립적으로 만들기 위해서 틀리게 분류한 데이터에 가중치를 부여합니다. 이 과정을 통해 올바른 결과와 틀린 분류를 50%/50% 비율로 맞춥니다. 이 방법을 사용한 가장 유명한 방법은 &lt;span style=&quot;color:red&quot;&gt;AdaBoost (Adaptive Boosting)&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 이미 모든 Unit의 학습이 끝난 상태에서 결과를 합치는 &lt;span style=&quot;color:red&quot;&gt;Blending&lt;/span&gt;을 알아봅시다. Regression 문제에서 최종 가설 $g(\mathbf{x})$를 도출하기 위해서는 각각의 Unit들의 최종 가설인 $h_t(\mathbf{x})$에 가중치 $\alpha_t$를 곱한 다음 더해야 합니다.&lt;/p&gt;

&lt;p&gt;가장 좋은 성능을 보이는 (=Error를 최소화하는) 결과를 내기 위해서는 적절한 $\alpha_t$를 정해야 합니다. Squared Error로 Measure한다고 가정한다면, Pseudo-Inverse를 통해 계산할 수 있습니다. 이 과정에서 특정 $\alpha_t$는 음수가 나올 수도 있습니다. 하지만 음수가 나왔다고 해당 가설이 Aggregation에서 쓸모가 없다는 뜻은 아닙니다. 가설이 Aggregation에서 쓸모가 있는지는 다른 방법을 통해 측정합니다.&lt;/p&gt;

&lt;p&gt;어떤 가설 $h$가 Aggregation에서 얼마나 기여했는지 평가하는 방법은 그 가설 $h$를 포함했을 때의 결과와 포함하지 않았을 때의 결과를 비교하는 것입니다. 그 둘을 비교했을 때 Out of Sample Error의 차이가 크면 클수록 가설 $h$의 기여도가 높다고 판단할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이론적인 내용은 모두 끝났고, 이 강의에 도움을 준 사람에게 감사를 표하는 시간입니다. (이 부분부터는 읽지 않으셔도 됩니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 먼저 &lt;strong&gt;Malik Magdon-Ismail&lt;/strong&gt; 교수님과 &lt;strong&gt;Hsuan-Tien Lin&lt;/strong&gt; 교수님입니다. 이 두 교수님은 교재 작성에 큰 기여를 하셨고, 그 기여도로 인해 교재에도 공동 저자로 등록되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로 &lt;strong&gt;Carlos&lt;/strong&gt;, &lt;strong&gt;Ron&lt;/strong&gt;, &lt;strong&gt;Costis&lt;/strong&gt;, 그리고 &lt;strong&gt;Doris&lt;/strong&gt;는 강의 슬라이드 및 숙제 문제를 만드는데 큰 기여를 했다고 합니다. 특히 Carlos는 이 강의에서 Q &amp;amp; A 세션의 진행을 담당했고 마지막 온라인 강의에서 이 분의 얼굴을 볼 수 있습니다. Yaser 교수님이 이들이 받는 봉급보다 많은 일을 했다는 것으로 보아 이 4명은 대학원생인 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leslie&lt;/strong&gt;와 &lt;strong&gt;Rich&lt;/strong&gt;는 강의 중 슬라이드의 크기 등을 조절할 수 있게 도와주고, 강의를 촬영하여 온라인 강의를 제작할 수 있게 도움을 주었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 강의는 모든 사람에게 무료로 열려 있습니다. 하지만 그렇게 하기 위해 많은 돈이 필요했는데, 슬라이드에 나와있는 몇몇 Caltech의 직원들이 그 비용을 마련할 수 있도록 도와주었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 외에 언급하지 않은 모든 Caltech의 TA 및 스태프, 졸업생, 동문, 그리고 Yaser 교수님의 동료들로부터도 많은 도움을 받았다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 가장 큰 가르침을 얻은 &lt;strong&gt;Faiza A. Ibrahim&lt;/strong&gt;에게 감사를 표합니다. 가장 큰 글씨로 적었길래 누군가 하고 인터넷에 검색해보니 Yaser 교수님의 어머님이라고 나오네요.&lt;/p&gt;

&lt;p&gt;이번 장에서는 각 슬라이드의 내용도 많고, 특히 제가 잘 모르는 분야에 대한 내용이 많아 정리하기 쉽지 않았습니다. 그렇기에 Yaser 교수님의 말씀을 최대한 오역하지 않도록 정리했는데, 나중에 다시 읽어보며 틀린 내용이나 어색한 표현을 찾아 고치겠습니다. 댓글로도 지적해주신다면 반영하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;이로써 기계학습 관련 포스트는 여기까지입니다. 지금까지 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Three Learning Principles</title><link href="http://localhost:4000/studies/three-learning-principles/" rel="alternate" type="text/html" title="Three Learning Principles" /><published>2019-11-09T00:00:00+09:00</published><updated>2019-11-09T00:00:00+09:00</updated><id>http://localhost:4000/studies/three-learning-principles</id><content type="html" xml:base="http://localhost:4000/studies/three-learning-principles/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;17장은 기계학습에서 중요한 3가지 원칙에 대해 소개합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각각의 주제는 이번 장의 제목과 같이 3가지 원칙을 하나씩 나열하고 있습니다. Occam’s Razor는 Learning Model과 관련이 있는 주제이고 Sampling Bias는 데이터 수집(Collecting), Data Snooping은 데이터 처리(Handling)에 관련이 있는 주제입니다.&lt;/p&gt;

&lt;h2 id=&quot;occams-razor&quot;&gt;Occam’s Razor&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Occam’s Razor를 설명하기 전에, 먼저 아인슈타인의 말을 인용하면서 시작합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“데이터에 대한 설명은 가능한 한 단순해야 하지만, 더 단순해서는 안됩니다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이것이 바로 Occam’s Razor의 기본 아이디어입니다. 이것과 면도날이 왜 관련이 있는지 궁금하실 수도 있는데, 만약에 면도기로 &lt;strong&gt;설명&lt;/strong&gt;을 깎는다고 생각해봅시다. 우리가 어떤 물건을 10개의 문장으로 설명하고 있다고 합시다. 그런데 그 설명을 &lt;strong&gt;깎아&lt;/strong&gt; 5개의 문장만으로 동일한 설명이 가능하다고 하면, 그것이 더 좋은 설명이라는 논리입니다.&lt;/p&gt;

&lt;p&gt;이렇게 되면 인용구의 뒷 소절인 &lt;strong&gt;더 단순해서는 안됩니다&lt;/strong&gt;의 의미가 궁금해집니다. 이것은 만약에 설명을 더 깎을 수 있더라도, 그것이 원래의 의미를 퇴색시킨다면 그렇게는 하면 안 된다는 의미입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 아이디어를 기계학습으로 가져와봅시다. 기계학습에서 Occam’s Razor를 한 문장으로 정리하면 다음과 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“데이터에 맞는 가장 간단한 모델은 가장 타당하기도 합니다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;문장은 멋있지만, 이것에 대한 의미를 해석하려면 그 전에 먼저 두 가지 질문에 대답해야 합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Model이 단순하다는 것은 어떤 의미인가?&lt;/li&gt;
  &lt;li&gt;이 말이 맞다는 것을 어떻게 알 수 있나? (=성능 측면에서 단순할수록 더 좋다는 것을 어떻게 알 수 있나?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 궁금증에 대해 하나씩 풀어보도록 합시다.&lt;/p&gt;

&lt;p&gt;※ Occam’s Razor는 기계학습에서만 사용하는 용어가 아니기 때문에, 좀 더 일반적인 뜻을 알고 싶으시다면 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%98%A4%EC%BB%B4%EC%9D%98_%EB%A9%B4%EB%8F%84%EB%82%A0&quot;&gt;위키백과&lt;/a&gt;를 함께 읽어보시는 것을 추천드립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫번째 질문부터 생각해봅시다. &lt;strong&gt;단순하다&lt;/strong&gt;라는 것은 정확히 무엇을 의미할까요?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Complexity (복잡성)&lt;/strong&gt;를 측정할 때는 기본적으로 두 가지 유형이 있습니다. 첫 번째는 &lt;strong&gt;Object의 복잡성&lt;/strong&gt;입니다. 기계학습에서는 가설 $h$나 최종 가설 $g$를 의미합니다. 두 번째로는 &lt;strong&gt;Set of Object의 복잡성&lt;/strong&gt;입니다. 이것은 기계학습에서 가설 집합 $\mathcal{H}$를 의미합니다.&lt;/p&gt;

&lt;p&gt;가설 $h$의 Complexity의 예로는 Minimum Description Length (MDL), 다항식의 차수 등이 있습니다. MDL은 Object를 만들고 가능한 한 적은 Bit로 표현하는 것을 말합니다. 예를 들어, 100만에서 1을 뺀 수를 가정해보겠습니다. 이를 숫자로 표현하면 999999 입니다. 100만에서 1을 뺀 수와 999999 중에 어떤 방법이 더 간단하게 표현하는 것일까요? 당연히 전자가 더 편리한 표현임을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;다항식의 차수는 더 간단합니다. 17차 다항식과 100차 다항식이 있다고 하면 높은 차수의 다항식이 더 복잡한 모델임을 이미 알고 있습니다. 이것은 11장에서 Deterministic Noise를 통해 배웠습니다.&lt;/p&gt;

&lt;p&gt;다음으로 가설 집합 $\mathcal{H}$의 Complexity의 예로는 Entropy와 VC Dimension이 있습니다. VC Dimension은 7장에서 이미 다루었기 때문에 넘어가겠습니다. Entropy는 Information Theory에 나오는 개념으로, 정보량을 측정하는 척도를 의미합니다. 가장 유명한 식으로 &lt;span style=&quot;color:red&quot;&gt;Shannon’s Entropy&lt;/span&gt;가 있는데, 지금 중요한 부분은 아니니 여기서는 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;다시 원래의 질문으로 돌아오면, 일반적으로 &lt;strong&gt;단순하다&lt;/strong&gt;에 대해 언급할때는 첫 번째인 가설 $h$의 단순함을 일컫는 것입니다. 하지만 Occam’s Razor를 수학적으로 증명할 때 언급하는 단순함은 가설 집합 $\mathcal{H}$의 단순함을 말하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 가설 $h$의 Complexity와 가설 집합 $\mathcal{H}$의 Complexity 사이에는 어떤 관련이 있는지 알아봅시다.&lt;/p&gt;

&lt;p&gt;먼저, 가설 $h$를 특정하기 위해서는 $l$ bit가 필요하다고 가정해봅시다. 이 가정에서 가설 $h$의 복잡도는 $l$ bit가 됩니다. 이것을 가설 집합 $\mathcal{H}$에 관련지어 표현하면, 가설 $h$는 가설 집합 $\mathcal{H}$의 $2^l$개의 원소 중 하나가 된다고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이것을 기계학습과 연관지어 예를 들어보면, 17차 다항식을 생각해봅시다. 17차 다항식을 특정하기 위해서는 17개의 Parameter가 필요하기 때문에 가설 집합 $\mathcal{H}$는 무한대가 됩니다. 그렇기에 이것은 &lt;strong&gt;복잡하다&lt;/strong&gt;라고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 규칙에는 예외가 있는데, 복잡해 보이지만 실제로는 그렇지 않은 SVM이 있습니다. 오른쪽의 그림을 보시면 SVM으로 나눈 평면은 굉장히 복잡해 보이지만, 실제로는 극소수의 Support Vector로 정의되기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이와 관련해서 간단한 퍼즐을 하나 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;매주 월요일 저녁에 축구 경기가 있다고 가정합시다. 그런데 어느 월요일 아침, 당신 앞으로 편지가 한통 왔습니다. A팀과 B팀이 경기하는데 A팀이 이길 것이라는 내용입니다. 아직 경기가 있기 전이고, 대부분이 B팀의 승리를 예측했기 때문에 당신은 편지의 내용을 믿지 않았지만, 실제로 그날 경기는 A팀이 이기게 됩니다.&lt;/p&gt;

&lt;p&gt;다음 주 월요일 아침, 또 동일한 사람에게 편지가 왔습니다. 역시 그 날 저녁의 축구 경기 결과를 예측하는 내용이었으며, 또 맞춰버리고 말았습니다. 이렇게 5주 연속 편지가 왔고, 5주 내내 편지에서는 그 날의 축구 경기 결과를 정확하게 예측하였습니다.&lt;/p&gt;

&lt;p&gt;그런데 6주 째가 되었을 때, 또 편지가 왔지만 이번에는 다른 내용이었습니다. 축구 경기 예측 결과를 더 보고 싶으면 50달러를 지불하라는 내용이었습니다. 이런 상황에서, 당신은 그 가격을 지불할 것인가요?&lt;/p&gt;

&lt;p&gt;정답부터 말씀드리면 당연히 지불해서는 안됩니다. 만약에 편지를 보내는 사람이 처음엔 32명을 대상으로 절반은 A팀 승리/나머지 절반은 B팀 승리로 적어서 편지를 보내고, 맞은 쪽에만 다시 절반은 A팀 승리/나머지 절반은 B팀 승리라는 편지를 보내는 과정을 반복했을지도 모르기 때문입니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 기계학습에서는 예측 값이 의미가 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 두 번째 질문을 해결해봅시다. 왜 단순한 것이 더 좋을까요? 여기서 더 좋다는 의미는 우아해 보인다는 것이 아니라 Out of Sample에서의 성능이 더 좋다는 의미입니다.&lt;/p&gt;

&lt;p&gt;이것에 대해 더 엄밀한 증명은 이상적인 상황을 가정하지만, 여기서는 증명의 요점만을 짚고 넘어가겠습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;단순한 가설이 복잡한 가설보다 적다. (이것은 5장에서 Growth Function을 통해 배웠습니다)&lt;/li&gt;
  &lt;li&gt;단순한 가설은 주어진 데이터 셋에 맞추기 더 적합하지 않다.&lt;/li&gt;
  &lt;li&gt;그렇기 때문에 단순한 가설이 데이터 셋에 맞춰지는 일이 발생한다면, 그것이 더 중요하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;방금 전에 다루었던 우편 퍼즐의 Growth Function을 생각해보면, 편지를 받는 당신은 자신만 그러한 편지를 받았다고 생각했었지만 (일어나기 힘든 일), 현실적으로는 가능한 모든 경우를 고려해서 편지를 보낸 것 (무조건 일어나는 일)이기 때문에 의미가 없던 것이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;적합이 무의미한 과학 실험을 살펴보겠습니다. 실험의 주제는 어떤 특정한 금속의 Conductivity (전도성)이 Temperature (온도)에 선형이라는 가설을 증명하는 것입니다. 이 주제를 가지고 두 명의 과학자가 실험을 하였습니다.&lt;/p&gt;

&lt;p&gt;과학자 A는 두 지점에서 실험을 하였고, 그 둘을 잇는 선을 그렸습니다.&lt;/p&gt;

&lt;p&gt;과학자 B는 세 지점에서 실험을 하였고, 그 셋을 잇는 선을 그렸습니다.&lt;/p&gt;

&lt;p&gt;Conductivity (전도성)이 Temperature (온도)에 선형이라는 가설을 더 명확하게 밝힌 사람은 누구인가요? 오래 생각하지 않더라도 과학자 B가 더 많은 정보를 제공하는 것을 알 수 있습니다. 왜냐하면, 과학자 A가 제시한 2개의 점은 항상 선으로 연결할 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;이와 관련된 개념을 &lt;span style=&quot;color:red&quot;&gt;Falsifiable (위조 가능성)&lt;/span&gt;이라고 합니다. 과학자 A가 제시한 그래프는 사실 위 슬라이드의 3번째 그림처럼 선을 벗어난 점이 있을 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;sampling-bias&quot;&gt;Sampling Bias&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 데이터 수집에서 발생할 수 있는 문제인 Sampling Bias에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 퍼즐은 1948년에 일어났던 미국 대통령 선거입니다. 당시 후보는 Truman과 Dewey 였는데, 한 신문사가 선거가 끝난 직후 당선자를 예측하기 위해 여론조사를 실시하였습니다. 여론조사의 방법은 무작위 사람에게 전화를 걸어 누구에게 투표했는지 물어보는 것이었습니다.&lt;/p&gt;

&lt;p&gt;여론조사를 해보니 오차를 감안하더라도 Dewey가 확실하게 Truman을 이긴다는 결론을 내렸고, 사진과 같이 Dewey가 Truman을 이겼다고 신문에 실었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 문제는 실제로 Truman이 당선되었다는 것입니다.&lt;/p&gt;

&lt;p&gt;더 이상한 것은 통계의 산출 방법이 틀리지 않았다는 것입니다. 충분한 양의 표본을 모았고, 결과를 계산하는 과정도 아무런 문제가 없었습니다.&lt;/p&gt;

&lt;p&gt;단순히 운이 없어서 이런 일이 발생했다고 생각할 수도 있지만, 그렇지 않았습니다. 신문사는 데이터 표본을 10배, 100배 늘린다고 해도 똑같은 결과가 나올 것이라고 판단했기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이 여론 조사의 문제점은 무엇일까요? 바로 표본에 &lt;strong&gt;Bias (편향)&lt;/strong&gt;가 있었습니다. 지금이야 누구나 휴대폰을 갖고 있지만, 1948년에는 전화기 자체가 비싼 물건이었기 때문에 전화를 갖고 있다는 것 자체가 부유한 계층이라는 뜻이었기 때문입니다. 부유한 사람들에게만 여론 조사를 했기 때문에, 부유한 사람이 많이 지지했던 Dewey에게 투표한 사람이 많았고, 그 결과 표본 자체가 부유한 사람들의 의견만을 반영한 결과가 나온 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 데이터가 편항된 방식으로 수집된다면, 학습은 비슷하게 편향된 결과를 낳는다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;학습은 사용자가 제공한 데이터를 통해 세상을 봅니다. 사용자가 비뚤어진 데이터를 준다면, 학습 또한 사용자에게 비뚤어진 가설을 줍니다.&lt;/p&gt;

&lt;p&gt;이와 비슷한 또 하나의 예제를 보겠습니다. 재무 예측에서 기계학습은 많이 사용되는 방법입니다. 당신은 시장의 정상적인 기간을 구하려고 합니다. 실제로 사람들이 사고팔 때 특정한 패턴이 존재합니다. 만약에 실제 시장에서 일어나는 Live Trading를 데이터로 사용한다면, 이것은 데이터 편향이 존재한다고 말할 수 있습니다. 왜냐하면 Live Trading 이외의 부분은 어떠할지 전혀 알 수 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sampling Bias를 처리하는 한 가지 방법은 분포를 일치시키는 것입니다. 실제로 많이 사용하는 방법인데, Input Space에 분포가 있다고 가정하는 것입니다. Hoeffding’s Inequality와 VC Analysis에서는 Training과 Testing이 같은 분포를 갖고 있다고 가정하였습니다. 그렇기에 이 경우 Sampling Bias가 존재하면 가정에 위배되므로 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;따라서 애초부터 Training과 Testing의 분포가 동일하지 않다고 가정하는 것입니다. 그래서 Training과 Testing의 분포를 일치시키기 위해 Training Data에 가중치를 부여하거나, 또는 Resampling 할 수도 있습니다. 단순한 방법이지만, 이 방법을 사용하면 Sampling Bias를 처리할 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;하지만 만약 Training에서의 확률은 0인데, Testing에서의 확률이 0보다 큰 경우에는 사용할 수 없다고 합니다. 방금 보았던 미국의 대선이 바로 이것을 설명하는 예시인데, 전화기가 없는 사람이 실제(Testing)에서는 확률이 0보다 크지만 표본(Training)에서는 확률이 0이었기 때문입니다. 이 때는 확률이 0인 부분에서 어떤 일이 일어날 지 알 수 없기 때문에 데이터에 가중치를 부여하는 등의 작업이 불가능함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3번째 퍼즐은, 이 상황에서의 Sampling Bias를 찾아내는 것입니다.&lt;/p&gt;

&lt;p&gt;은행에서 고객의 신용카드 발급을 자동으로 승인하는 시스템을 만드려고 합니다. 이전에 신청한 고객들의 과거 기록을 기반으로 새 고객의 신용 정보 (오른쪽 표와 같은)을 입력받았을 때 이 사람이 은행에 이익을 가져다 줄지(=신용카드를 발급해줘도 괜찮은지)를 판단하는 시스템입니다.&lt;/p&gt;

&lt;p&gt;혹시 Sampling Bias가 어디서 일어나는지 찾으셨나요? 바로 &lt;strong&gt;이전에 신청한 고객들의 과거 기록&lt;/strong&gt;입니다. 이들은 이미 은행에서 신용카드를 발급해준 대상자들입니다. 그러니까, 신용카드 발급을 거절당한 사람들의 기록은 고려되지 않는 것입니다.&lt;/p&gt;

&lt;p&gt;그런데 사실 이것은 Sampling Bias가 크게 문제 되지 않는 상황이기도 합니다. 은행은 신용카드를 발급해줄 때 어느 정도의 위험성(ex. 고객이 카드를 쓰고 돈을 갚지 않는 상황)을 감수해야 하기 때문에, 다소 보수적으로 기준을 잡기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;data-snooping&quot;&gt;Data Snooping&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 주제로 Data Snooping에 대해 이야기해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 원칙을 먼저 설명한 후에 이야기가 진행됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 데이터 집합이 학습 과정의 어떤 단계라도 영향을 미쳤다면, 결과를 평가하는 능력은 손상된다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이것은 실무자들에게 가장 흔하게 발생하는 실수라고 합니다. 이전에 9장에서도 Data Snooping에 대해 이야기한 적이 있었는데, 그 때는 데이터를 먼저 보고 모델을 선택했을 때 발생하는 실수라고 언급하고 넘어갔습니다. 하지만 이것은 Data Snooping에 빠질 수 있는 경우의 수 중 한 가지에 불과하며 실제로는 이런 함정에 빠지는 방법이 많다는 것입니다.&lt;/p&gt;

&lt;p&gt;이제 Data Snooping이 일어날 수 있는 몇 가지의 예를 확인할 것입니다. 전에 보았던 예도 있지만, 그렇지 않은 것들도 있습니다. 이 예들을 통해 무엇을 피해야 하며 어떤 종류의 Data Snooping이 있는지 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전에 배웠던 Nonlinear Transform으로 시작해봅시다. 오른쪽 그림은 9장에서 Data Snooping을 처음 언급할 때 나왔던 예제입니다. 이때 2차식을 사용하여 Transform 하는 방법으로 문제를 풀었고, 그 결과 $\mathbf{z}$는 6차원의 벡터가 되었습니다.&lt;/p&gt;

&lt;p&gt;문제는 이것을 보고 더 간단하게 표현하고 싶어 $\mathbf{z}$를 직접 손댔을 때 발생했습니다. 이렇게 하면 VC Dimension이 3이기 때문에 더 좋다고 생각할 수 있습니다. 하지만 이렇게 함으로써 실제로 하는 일은 데이터가 아닌, 사용자 스스로 학습하게 일이 되어 버립니다.&lt;/p&gt;

&lt;p&gt;Data Snooping은 데이터 집합 $\mathcal{D}$와 관련이 있습니다. 그렇기 때문이 주어진 데이터 집합에서는 잘 수행될지 모르지만, 독립적으로 생성된 다른 데이터 집합에서도 잘 수행될지의 여부는 알 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또 퍼즐이 나왔습니다. 4번째 퍼즐은 재무 예측 문제에서 Data Snooping이 일어나는 곳을 찾는 것입니다.&lt;/p&gt;

&lt;p&gt;이것은 미국 달러와 영국 파운드 사이의 환율을 예측하는 문제입니다. 여기 8년 분량의 일일 거래 자료가 있습니다. 오른쪽 하단의 초록색 $\Delta r$은 오늘을 기준으로 20일 전까지 일어났던 예측 오율입니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 데이터를 평균과 단위 분산이 0이 되도록 Normalize합니다. 총 2000여 일간의 데이터 중 1500일을 Training Set으로 사용하고, 500일을 Testing Set으로 사용합니다. 물론 두 집합 모두 무작위로 추출합니다.&lt;/p&gt;

&lt;p&gt;이 과정에서 사용자는 어떤 데이터도 눈으로 보지 않았습니다. 방금까지 설명한 모든 과정을 자동으로 수행한 다음, Training Data를 통해 최종 가설을 세우고 Test Set에서 그 성능을 확인합니다. 그 결과 오른쪽 그래프의 빨간 선처럼 우상향 곡선을 그리게 됩니다.&lt;/p&gt;

&lt;p&gt;지금까지 봤을 때, 어느 지점에서도 Data Snooping이 일어나지 않은 것 같습니다. 하지만 분명히 이 과정에서 Data Snooping이 일어났고, 그렇기에 실제 예측 (파란색 곡선)과 큰 차이가 벌어진 것입니다.&lt;/p&gt;

&lt;p&gt;정답을 말씀드리면, Data Snooping은 Data를 Normalize 할 때 발생하였습니다. Normalize 자체가 잘못된 것은 아닙니다. Normalize를 하는 과정에서 Test Set이 포함되었기 때문에, 다시 말해 Training Set이 Test Set에 영향을 주었기 때문에 Data Snooping이 일어난 것입니다. 올바르게 Normalize를 하기 위해서는, 데이터를 먼저 Training Set과 Test Set으로 나눈 다음 Normalize를 해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data Snooping의 또 다른 이름은 Reuse of a Data set (데이터 집합의 재사용) 입니다. 만약 사용자가 어떤 데이터 집합을 가지고 이것저것 학습모델을 사용하다 보면 언젠가는 학습에 성공할 것입니다. 이 말은 다시 말하게 되면,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 당신이 데이터를 충분히 오래 고문하면, 결국에는 자백한다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;여기서 자백한다는 의미는 결과적으로 아무 의미가 없다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;왜 문제가 발생하는지 예를 들어봅시다. 만약에 사용자가 카드 발급을 승인해주는 문제를 푼다고 가정해봅시다. 사용자는 데이터를 전혀 보지 않았고, 정규화시키지도 않았습니다. 그런데 사용자는 우연히 인터넷에서 글을 보다가 카드 발급 승인 문제에서 SVM이 가장 효과가 뛰어나다는 사실을 발견했습니다. 이것을 보고 사용자가 자신도 SVM을 사용하겠다고 결정하면, 데이터를 보지 않았더라도 그 영향을 받은 사실을 사용했기 때문에 문제가 되는 것입니다.&lt;/p&gt;

&lt;p&gt;이것에 대한 핵심적인 문제는 바로 특정한 데이터 집합을 너무 잘 일치시킨다는 사실입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data Snooping에는 두 가지 해결책이 있습니다. 하나는 Data Snooping을 피하는 것[…]이고 다른 하나는 Data Snooping을 설명하는 것입니다.&lt;/p&gt;

&lt;p&gt;Data Snooping을 피하기 위해서는 엄격한 훈련이 필요하다고 합니다. 말은 정말 간단합니다. 만약 이것이 쉽지 않다면 두 번째 방법으로, 데이터가 얼마나 오염되었는지를 알아야 한다고 합니다. 물론 그냥 알기만 하면 안되고, 이전에 데이터 분포를 일치시켰던 것처럼 그에 맞는 대처를 해 주어야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 마지막 퍼즐입니다. 주식에서 장기간 &lt;strong&gt;Buy and Hold (=장기 투자)&lt;/strong&gt; 했을 때의 성능을 테스트하려고 합니다. 이를 위해 여기서는 50년간의 데이터를 사용합니다. 이를 확인하기 위해 다음과 같은 방법을 사용합니다.&lt;/p&gt;

&lt;p&gt;먼저, 현재 거래되는 모든 주식 회사를 대상으로 합니다. 만약에 50년 전에 당신이라면, 어떤 주식을 구매할 것인지 스스로 판단하고, 50년 후 (즉, 현재) 얼마가 되어있을지를 계산하는 겁니다.&lt;/p&gt;

&lt;p&gt;이 간단한 작업에도 Sampling Bias가 생겼습니다. 현재 거래되는 주식은 50년 전에 분명히 있었지만, 50년 전에 있던 주식회사 중 망한 회사는 선택에서 배제되었기 때문입니다. 문제는 이 과정은 (50년 전을 기준으로) 미래의 데이터를 보고 결정한 것이기 때문에 Sampling Bias 보다는 Data Snooping과 혼동이 생긴다는 것입니다. 여기에서는 두 가지 성질을 모두 가지고 있으므로, Snooping으로 인한 Sampling Bias라고 결론지었습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry></feed>