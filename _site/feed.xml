<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-25T16:18:45+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">Epilogue</title><link href="http://localhost:4000/studies/epilogue/" rel="alternate" type="text/html" title="Epilogue" /><published>2019-11-16T00:00:00+09:00</published><updated>2019-11-16T00:00:00+09:00</updated><id>http://localhost:4000/studies/epilogue</id><content type="html" xml:base="http://localhost:4000/studies/epilogue/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 대망의 마지막 장입니다. 이번 장에서는 지금까지 배웠던 기계학습을 정리하고 강의에서 다루지 못했던 기계학습에 대해 간략하게 설명하고 마무리합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 크게 4가지의 소주제로 이루어져 있습니다. 가장 먼저 지금까지 배운 기계학습을 간단하게 정리하고, 본 강의에서 다루지 못했던 기계학습 중 Baysian Learning과 Aggregation Methods를 간략하게 소개합니다. 마지막으로는 강의에 큰 도움을 줬던 분들에게 감사를 표한다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;the-map-of-machine-learning&quot;&gt;The Map of machine learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;기계학습&lt;/strong&gt;을 다루는 책들은 굉장히 많은 내용을 소개하고 있습니다. 이것들 중 일부분을 나열하면 위와 같이 정신이 없을 정도로 많은 주제가 있음을 알 수 있습니다. 어떤 것들이 있는지 대충 보시면 지금까지 다루었던 것들도 있지만, 그렇지 않은 것들도 있다는 것을 아실 겁니다.&lt;/p&gt;

&lt;p&gt;이렇게 보기 힘들게 주제들을 나열하면 머리만 아프고 이해도 힘드니, 강의에서는 좀 더 체계적인 방법으로 기계학습을 분류한 것을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 기계학습의 분야에는 크게 3가지 갈래가 있습니다. 첫째는 &lt;span style=&quot;color:red&quot;&gt;Theory (이론)&lt;/span&gt;, 두 번째는 &lt;span style=&quot;color:red&quot;&gt;Technique (기술)&lt;/span&gt;, 마지막으로 &lt;span style=&quot;color:red&quot;&gt;Paradigm (패러다임)&lt;/span&gt;이 있습니다. Paradigm은 학습 상황에 대한 다른 가정을 의미합니다. 수학적 가정이 아니라 Supervised Learning이나 Reinforcement Learning과 같은 다른 학습 상황을 다루는 가정이라는 뜻입니다. 이러한 가정을 할 때, 해결해야 할 문제는 기존의 기계학습의 문제와 다르기 때문에 공부해야 할 지식이 달라지게 됩니다. 그렇기에 이것을 Paradigm이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;가장 상위 개념인 Pradigm부터 이야기하면, Supervised Learning은 본 강의 대부분에서 다루는 주제였습니다. PLA부터 Support Vector Machine까지 대부분의 학습 알고리즘은 데이터에 Label이 있는 상황을 가정한 것이었기 때문입니다. 기계학습에서 가장 인기 있으면서도 유용한 주제입니다. Unsupervised Learning은 본 강의에서 많이 다루지는 않았지만, 최소한 Clustering이라는 핵심 아이디어를 배웠습니다. Reinforcement Learning은 첫 번째 강의에서만 잠깐 언급하였습니다. 좋은 행동을 하면 보상을 주고(강화하고) 나쁜 행동에 패널티를 부과하여 결국에는 좋은 해결책으로 수렴하게 만드는 방법입니다. 그 외에 Active Learning이나 Online Learning 등이 있지만 강의에서는 다루지 않았기에 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;다음으로 Theory 입니다. 기계학습에서 주요 이론은 Vapnic-Chervonenkis (VC) 이론입니다. 7장부터 시작하여 이후로도 지속적으로 기계학습의 일반화를 설명하기 위해 VC, 그리고 Bias-Variance 이론을 다루었습니다. Complextiy는 기계학습에서의 실용적인 부분입니다. 강의에서는 다루지 않았지만, 이것이 다항시간 내에 일어나는지, 혹은 그렇지 않은지를 통해 이론적인 알고리즘을 실질적으로 구현이 가능하지를 분석하는 이론입니다. 마지막으로 Bayesian은 기계학습을 확률의 한 갈래로 취급하는 이론입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Technique은 Model과 Method 2가지로 분리됩니다. Model은 지금까지 대부분의 강의에서 다루었던 부분입니다. 기본적인 Linear Model부터 시작하여 선형 분리가 되지 않는 데이터 집합에서 어떻게 처리해야하는지 Transform과 Neural Network 등을 배워나갔습니다. 그 이후로도 SVM, RBF를 포함하여 많은 영역을 다루었습니다. 이 외에도 Gaussian Process, Singular Value Decomposition (SVD), Graphical Model 등이 있지만, 강의에서 이 모든 것을 다루지는 못했습니다.&lt;/p&gt;

&lt;p&gt;Method는 Model에 관계없이 많은 영역을 다루기 때문에 매우 중요합니다. 강의에서는 Neural Network를 기점으로 발생할 수 있는 위험성인 Overfitting을 해결하기 위해 이 부분에 많은 시간을 투자하였습니다. Regularization과 Validation이 바로 대표적인 해결 방법이었습니다. Aggregation과 Input Processing은 강의에서 다루지 않은 요소입니다. 그중 Input Processing은 기계학습의 실무 과정에서 많이 다루는 실용적인 방법입니다.&lt;/p&gt;

&lt;p&gt;이번 장의 나머지 부분은 지금까지 나열했던 것 중 Bayesian과 Aggregation에 대해 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-learning&quot;&gt;Bayesian learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Bayesian Learning으로 넘어갑니다. 깊이있게 이 내용을 다루기보단, Bayesian 접근법의 기초를 다룰 것이며 언제 사용할 수 있는지, 단점은 무엇인지 정도만 짚을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오랜만에 Learning Diagram을 살펴봅시다. 이 Diagram에서 확률적인 요소는 두 가지가 있었습니다. 하나는 Data가 알 수 없는 특정한 확률분포에 의해 생성된다는 것이었고, 다른 하나는 Input $\mathbf{x}$가 주어졌을 때 Output $y$가 나올 확률이었습니다. 이것은 Noise로 인해 더 이상 Target Function이 아니라 Target Distribution으로 불리게 되었기 때문이었습니다,&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 이러한 확률적인 역할을 확장하는 개념입니다. 이전에 9장에서 Likelihood (가능도)를 잠시 떠올려보면, 가설 $h$와 Target Function $f$가 같다면 $\mathcal{D}$가 주어졌을 때 Output $y$를 얻을 확률을 의미하였습니다. 그래서 주어진 데이터를 제일 잘 표현할 수 있는 최대 확률을 계산하였습니다.&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 이와 반대로 접근하고 있습니다. 데이터가 이미 발생하였기 때문에, 수 많은 가설 중 Target Function을 가장 잘 반영하는 가설이 무엇인지를 찾는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bayesian 접근 방식은 전공자들 사이에서도 의견이 분분합니다. 어떤 사람은 종교적인 수준으로 찬양을 하고, 어떤 사람은 완전히 쓰레기 같은 방법이라고 평가하기도 합니다. 강의에서도 이 점을 언급하며 Prior가 이러한 논쟁을 불러일으키는 주요 요소라고 합니다.&lt;/p&gt;

&lt;p&gt;우리는 주어진 데이터 $\mathcal{D}$ 하에서 가설 $h$와 Target Function $f$가 일치하기를 바랍니다. 이것은 Bayes’ Theorem에 의해 가운데 식처럼 변형할 수 있습니다. 이 중 $P(\mathcal{D} \mid h=f)$는 로지스틱 회귀 등을 통해 구할 수 있습니다. 그리고 $P(h=f)$는 필요 없는 요소라고 하던데, 사실 제가 Bayesian을 잘 모르기 때문에 왜 그런지는 아직 모르겠습니다. 어쨌든 이 둘을 곱하면 Joint Probability Distribution을 얻을 수 있고, $P(h=f \mid \mathcal{D})$는 이것에 비례합니다.&lt;/p&gt;

&lt;p&gt;Bayes’ Theorem에 나오는 항 중에 $P(h=f)$는 &lt;span style=&quot;color:red&quot;&gt;Prior&lt;/span&gt;라고 부르고 데이터를 얻기 전 가설 집합에 대한 믿음이라고 합니다. 이와 비슷하게 $P(h=f \mid \mathcal{D})$는 데이터를 얻은 후의 가설 집합에 대한 믿음이기 때문에 &lt;span style=&quot;color:red&quot;&gt;Posterior&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;Bayes’ Theorem을 통해 만약에 Prior가 주어진다면, 전체 가설 집합에 대한 전체 확률 분포를 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Prior의 예를 들어봅시다. 가설 $h$를 $d$차원 Perceptron 모델의 가중치 $\mathbf{w}$로 가정합니다.&lt;/p&gt;

&lt;p&gt;가중치 $\mathbf{w}$의 Prior는 각각의 $w_i$가 독립적이고 $[-1, 1]$에서 균등하다고 정했다고 가정합니다. 이것은 모든 가중치에 대한 확률 분포를 얻을 수 있다면, 어떤 가중치가 특정 가설에 기여하는지 알 수 있음을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 Prior는 가정에 불과합니다. 아주 간단한 사례로 알 수 없는 숫자를 찾는 문제가 있다고 생각해봅시다. 내가 아는 정보는 그 숫자가 -1과 1 사이라는 것뿐입니다. 누군가가 이것을 -1과 1 사이의 Uniform Distribution으로 모델링하고 &lt;strong&gt;이것은 -1과 1 사이의 내가 모르는 숫자가 있다는 것과 동일하다&lt;/strong&gt;라고 말한다면, 얼핏 듣기에는 그럴듯해 보이지만 그것은 틀린 사실입니다. 왜냐하면 Uniform Distribution에는 보이지 않는 많은 가정이 들어가 있기 때문입니다. 예를 들어 이 상황에서 많은 숫자를 뽑았을 때 Uniform Distribution의 평균은 0이지만, 원래 문제의 평균은 -1과 1 사이의 어떤 숫자이든 가능합니다. 실제로 이 문제와 동일한 것은 우리가 모르는 그 $x$가 $a$인 Delta Function으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 실제로 Prior를 알고 있다면, 모든 가설 $h$의 Posterior를 계산할 수 있기 때문에 완벽한 방법이 될 수 있습니다. 다시 말해, VC Analysis나 Regularization 같은 것도 필요 없이 가장 가능성 있는 가설을 선택할 수 있습니다. 심지어 모든 $\mathbf{x}$에 대해 가설의 평균 $\mathbb{E}(h(\mathbf{x}))$이나 Error Bar 또한 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 말은 예를 들어 주식 시장에서, 오늘의 주식 $\mathbf{x}$을 입력하면 가격 변동의 예상치나 그 예상치의 오차율까지도 계산이 가능하다는 말입니다. 상상할 수 있는 모든 것을 얻을 수 있다는 것입니다. 그렇기 때문에 정확한 Prior을 얻을 수 없다는 현실적인 문제로 인해, Bayesian 접근 방식을 선호하지 않는 과학자들도 있습니다. (사실 저도 이 방법은 좋아하지 않습니다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bayesian Learning이 올바르게 수행되기 위해선 둘 중 하나가 필요합니다. 첫째로, 유효한 Prior를 얻었을 경우에는 Bayesian Learning이 모든 다른 방법을 압도하는 해결방법이 됩니다.&lt;/p&gt;

&lt;p&gt;둘째로, Prior가 무관하게 만드는 것입니다. Prior를 가정할 때 점점 더 많은 데이터를 얻고 Posterior를 보면, Posterior가 데이터 집합에 의해 크게 영향을 받고 Prior에 의해 점점 덜 영향을 받습니다. 그렇기 때문에 Prior가 중요하지 않은 데이터가 충분하다면, Prior를 개념적 요소가 아닌 것으로 생각할 수 있습니다. 이렇게 하면 유효한 Prior는 아니게 되지만, Prior를 가지고 데이터를 얻게 되면 Posterior 계산이 쉽습니다. 이것을 Conjugate Prior라고 하며, 전체 함수에 대해 Posterior를 다시 계산할 필요가 없습니다. 간단하게 말해, 계산 과정을 매개변수화하는 용도로만 Prior를 사용한다는 뜻입니다.&lt;/p&gt;

&lt;h2 id=&quot;aggregation-methods&quot;&gt;Aggregation methods&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 Aggregation Method입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation은 모든 모델에 적용되는 방법입니다. 기본적인 아이디어는 다른 Solution을 결합한다는 것입니다. 예를 들어 컴퓨터 비전에서 사람의 얼굴을 구별하는 학습을 한다고 가정해봅시다. 여러 사용자에게 이 문제를 준다면 어떤 사람은 눈으로, 어떤 사람은 얼굴형으로, 어떤 사람은 이목구비의 위치로 사람을 구별할 것입니다. 총 관리자는 이들의 해결책을 결합해 최종적인 결과물을 만들 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 결합하는 방법에 대해 이야기해봅시다. 의외로 방법은 간단합니다. 만약에 Regression 문제라면 그저 평균을 내면 되는 것이고, Classification 문제라면 더 많은 사람이 분류한 것으로 판단하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아이디어만 보면 Aggregation과 2-Layer Learning이 비슷해 보입니다. 하지만 이 둘은 분명한 차이가 있습니다. 먼저 2-Layer Model은 모든 Unit이 동시에 참여합니다. 예를 들어 각 Unit에 Weight를 곱해서 더하는 방식으로 합치게 됩니다. 그에 반해 Aggregation은 각 Unit이 Training Data를 사용해 각자 학습하고, 각각의 Unit의 Output만을 사용해 최종 결과를 출력하는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation에는 두 종류가 있습니다. 하나는 &lt;span style=&quot;color:red&quot;&gt;Aftter the fact&lt;/span&gt; 입니다. 이것은 이미 Solution이 있음을 의미합니다. 예를 들어 이전에 다루었던 Netflix 추천 문제는, 이미 기존의 여러 해결 방법이 존재했고 그것들을 합치는 것만을 고려하면 되었습니다.&lt;/p&gt;

&lt;p&gt;다른 하나는 &lt;span style=&quot;color:red&quot;&gt;Before the fact&lt;/span&gt; 입니다. 이것은 결합하기 위한 Solution을 만드는 것입니다. 예를 들어, 주어진 데이터 집합 $\mathcal{D}$를 여러 번 독립적으로 &lt;strong&gt;Resampling (재생산)&lt;/strong&gt;하여 모두에게 다른 Sample Data를 주는 것입니다. 그렇게 해서 각각의 Unit을 학습시키고 합치는 방법이 Before the fact가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aggregation을 하기 위한 방법으로는 Boost Algorithm이 있습니다. 각각의 가설을 순차적으로 만드는 아이디어 입니다. 위 슬라이드에 나온 그림처럼 4번째 Unit을 만드는 상황에서, Training Data를 1~3번까지 만든 가설을 참고하는 것입니다. 이렇게 되면 각 Unit이 서로 관련이 생겨버리므로, 이를 독립시키는 과정이 필요합니다.&lt;/p&gt;

&lt;p&gt;만약 몇 개의 Unit을 사용하여 Data를 60%는 올바르게, 40%는 틀리게 분류했다고 가정해봅시다. 그런데 다음 Unit에게 넘겨주는 데이터를 독립적으로 만들기 위해서 틀리게 분류한 데이터에 가중치를 부여합니다. 이 과정을 통해 올바른 결과와 틀린 분류를 50%/50% 비율로 맞춥니다. 이 방법을 사용한 가장 유명한 방법은 &lt;span style=&quot;color:red&quot;&gt;AdaBoost (Adaptive Boosting)&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 이미 모든 Unit의 학습이 끝난 상태에서 결과를 합치는 &lt;span style=&quot;color:red&quot;&gt;Blending&lt;/span&gt;을 알아봅시다. Regression 문제에서 최종 가설 $g(\mathbf{x})$를 도출하기 위해서는 각각의 Unit들의 최종 가설인 $h_t(\mathbf{x})$에 가중치 $\alpha_t$를 곱한 다음 더해야 합니다.&lt;/p&gt;

&lt;p&gt;가장 좋은 성능을 보이는 (=Error를 최소화하는) 결과를 내기 위해서는 적절한 $\alpha_t$를 정해야 합니다. Squared Error로 Measure한다고 가정한다면, Pseudo-Inverse를 통해 계산할 수 있습니다. 이 과정에서 특정 $\alpha_t$는 음수가 나올 수도 있습니다. 하지만 음수가 나왔다고 해당 가설이 Aggregation에서 쓸모가 없다는 뜻은 아닙니다. 가설이 Aggregation에서 쓸모가 있는지는 다른 방법을 통해 측정합니다.&lt;/p&gt;

&lt;p&gt;어떤 가설 $h$가 Aggregation에서 얼마나 기여했는지 평가하는 방법은 그 가설 $h$를 포함했을 때의 결과와 포함하지 않았을 때의 결과를 비교하는 것입니다. 그 둘을 비교했을 때 Out of Sample Error의 차이가 크면 클수록 가설 $h$의 기여도가 높다고 판단할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이론적인 내용은 모두 끝났고, 이 강의에 도움을 준 사람에게 감사를 표하는 시간입니다. (이 부분부터는 읽지 않으셔도 됩니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 먼저 &lt;strong&gt;Malik Magdon-Ismail&lt;/strong&gt; 교수님과 &lt;strong&gt;Hsuan-Tien Lin&lt;/strong&gt; 교수님입니다. 이 두 교수님은 교재 작성에 큰 기여를 하셨고, 그 기여도로 인해 교재에도 공동 저자로 등록되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로 &lt;strong&gt;Carlos&lt;/strong&gt;, &lt;strong&gt;Ron&lt;/strong&gt;, &lt;strong&gt;Costis&lt;/strong&gt;, 그리고 &lt;strong&gt;Doris&lt;/strong&gt;는 강의 슬라이드 및 숙제 문제를 만드는데 큰 기여를 했다고 합니다. 특히 Carlos는 이 강의에서 Q &amp;amp; A 세션의 진행을 담당했고 마지막 온라인 강의에서 이 분의 얼굴을 볼 수 있습니다. Yaser 교수님이 이들이 받는 봉급보다 많은 일을 했다는 것으로 보아 이 4명은 대학원생인 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leslie&lt;/strong&gt;와 &lt;strong&gt;Rich&lt;/strong&gt;는 강의 중 슬라이드의 크기 등을 조절할 수 있게 도와주고, 강의를 촬영하여 온라인 강의를 제작할 수 있게 도움을 주었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 강의는 모든 사람에게 무료로 열려 있습니다. 하지만 그렇게 하기 위해 많은 돈이 필요했는데, 슬라이드에 나와있는 몇몇 Caltech의 직원들이 그 비용을 마련할 수 있도록 도와주었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 외에 언급하지 않은 모든 Caltech의 TA 및 스태프, 졸업생, 동문, 그리고 Yaser 교수님의 동료들로부터도 많은 도움을 받았다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/18. Epilogue/ML 18-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 가장 큰 가르침을 얻은 &lt;strong&gt;Faiza A. Ibrahim&lt;/strong&gt;에게 감사를 표합니다. 가장 큰 글씨로 적었길래 누군가 하고 인터넷에 검색해보니 Yaser 교수님의 어머님이라고 나오네요.&lt;/p&gt;

&lt;p&gt;이번 장에서는 각 슬라이드의 내용도 많고, 특히 제가 잘 모르는 분야에 대한 내용이 많아 정리하기 쉽지 않았습니다. 그렇기에 Yaser 교수님의 말씀을 최대한 오역하지 않도록 정리했는데, 나중에 다시 읽어보며 틀린 내용이나 어색한 표현을 찾아 고치겠습니다. 댓글로도 지적해주신다면 반영하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;이로써 기계학습 관련 포스트는 여기까지입니다. 지금까지 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Three Learning Principles</title><link href="http://localhost:4000/studies/three-learning-principles/" rel="alternate" type="text/html" title="Three Learning Principles" /><published>2019-11-09T00:00:00+09:00</published><updated>2019-11-09T00:00:00+09:00</updated><id>http://localhost:4000/studies/three-learning-principles</id><content type="html" xml:base="http://localhost:4000/studies/three-learning-principles/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;17장은 기계학습에서 중요한 3가지 원칙에 대해 소개합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각각의 주제는 이번 장의 제목과 같이 3가지 원칙을 하나씩 나열하고 있습니다. Occam’s Razor는 Learning Model과 관련이 있는 주제이고 Sampling Bias는 데이터 수집(Collecting), Data Snooping은 데이터 처리(Handling)에 관련이 있는 주제입니다.&lt;/p&gt;

&lt;h2 id=&quot;occams-razor&quot;&gt;Occam’s Razor&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Occam’s Razor를 설명하기 전에, 먼저 아인슈타인의 말을 인용하면서 시작합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“데이터에 대한 설명은 가능한 한 단순해야 하지만, 더 단순해서는 안됩니다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이것이 바로 Occam’s Razor의 기본 아이디어입니다. 이것과 면도날이 왜 관련이 있는지 궁금하실 수도 있는데, 만약에 면도기로 &lt;strong&gt;설명&lt;/strong&gt;을 깎는다고 생각해봅시다. 우리가 어떤 물건을 10개의 문장으로 설명하고 있다고 합시다. 그런데 그 설명을 &lt;strong&gt;깎아&lt;/strong&gt; 5개의 문장만으로 동일한 설명이 가능하다고 하면, 그것이 더 좋은 설명이라는 논리입니다.&lt;/p&gt;

&lt;p&gt;이렇게 되면 인용구의 뒷 소절인 &lt;strong&gt;더 단순해서는 안됩니다&lt;/strong&gt;의 의미가 궁금해집니다. 이것은 만약에 설명을 더 깎을 수 있더라도, 그것이 원래의 의미를 퇴색시킨다면 그렇게는 하면 안 된다는 의미입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 아이디어를 기계학습으로 가져와봅시다. 기계학습에서 Occam’s Razor를 한 문장으로 정리하면 다음과 같습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“데이터에 맞는 가장 간단한 모델은 가장 타당하기도 합니다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;문장은 멋있지만, 이것에 대한 의미를 해석하려면 그 전에 먼저 두 가지 질문에 대답해야 합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Model이 단순하다는 것은 어떤 의미인가?&lt;/li&gt;
  &lt;li&gt;이 말이 맞다는 것을 어떻게 알 수 있나? (=성능 측면에서 단순할수록 더 좋다는 것을 어떻게 알 수 있나?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 궁금증에 대해 하나씩 풀어보도록 합시다.&lt;/p&gt;

&lt;p&gt;※ Occam’s Razor는 기계학습에서만 사용하는 용어가 아니기 때문에, 좀 더 일반적인 뜻을 알고 싶으시다면 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%98%A4%EC%BB%B4%EC%9D%98_%EB%A9%B4%EB%8F%84%EB%82%A0&quot;&gt;위키백과&lt;/a&gt;를 함께 읽어보시는 것을 추천드립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫번째 질문부터 생각해봅시다. &lt;strong&gt;단순하다&lt;/strong&gt;라는 것은 정확히 무엇을 의미할까요?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Complexity (복잡성)&lt;/strong&gt;를 측정할 때는 기본적으로 두 가지 유형이 있습니다. 첫 번째는 &lt;strong&gt;Object의 복잡성&lt;/strong&gt;입니다. 기계학습에서는 가설 $h$나 최종 가설 $g$를 의미합니다. 두 번째로는 &lt;strong&gt;Set of Object의 복잡성&lt;/strong&gt;입니다. 이것은 기계학습에서 가설 집합 $\mathcal{H}$를 의미합니다.&lt;/p&gt;

&lt;p&gt;가설 $h$의 Complexity의 예로는 Minimum Description Length (MDL), 다항식의 차수 등이 있습니다. MDL은 Object를 만들고 가능한 한 적은 Bit로 표현하는 것을 말합니다. 예를 들어, 100만에서 1을 뺀 수를 가정해보겠습니다. 이를 숫자로 표현하면 999999 입니다. 100만에서 1을 뺀 수와 999999 중에 어떤 방법이 더 간단하게 표현하는 것일까요? 당연히 전자가 더 편리한 표현임을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;다항식의 차수는 더 간단합니다. 17차 다항식과 100차 다항식이 있다고 하면 높은 차수의 다항식이 더 복잡한 모델임을 이미 알고 있습니다. 이것은 11장에서 Deterministic Noise를 통해 배웠습니다.&lt;/p&gt;

&lt;p&gt;다음으로 가설 집합 $\mathcal{H}$의 Complexity의 예로는 Entropy와 VC Dimension이 있습니다. VC Dimension은 7장에서 이미 다루었기 때문에 넘어가겠습니다. Entropy는 Information Theory에 나오는 개념으로, 정보량을 측정하는 척도를 의미합니다. 가장 유명한 식으로 &lt;span style=&quot;color:red&quot;&gt;Shannon’s Entropy&lt;/span&gt;가 있는데, 지금 중요한 부분은 아니니 여기서는 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;다시 원래의 질문으로 돌아오면, 일반적으로 &lt;strong&gt;단순하다&lt;/strong&gt;에 대해 언급할때는 첫 번째인 가설 $h$의 단순함을 일컫는 것입니다. 하지만 Occam’s Razor를 수학적으로 증명할 때 언급하는 단순함은 가설 집합 $\mathcal{H}$의 단순함을 말하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 가설 $h$의 Complexity와 가설 집합 $\mathcal{H}$의 Complexity 사이에는 어떤 관련이 있는지 알아봅시다.&lt;/p&gt;

&lt;p&gt;먼저, 가설 $h$를 특정하기 위해서는 $l$ bit가 필요하다고 가정해봅시다. 이 가정에서 가설 $h$의 복잡도는 $l$ bit가 됩니다. 이것을 가설 집합 $\mathcal{H}$에 관련지어 표현하면, 가설 $h$는 가설 집합 $\mathcal{H}$의 $2^l$개의 원소 중 하나가 된다고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이것을 기계학습과 연관지어 예를 들어보면, 17차 다항식을 생각해봅시다. 17차 다항식을 특정하기 위해서는 17개의 Parameter가 필요하기 때문에 가설 집합 $\mathcal{H}$는 무한대가 됩니다. 그렇기에 이것은 &lt;strong&gt;복잡하다&lt;/strong&gt;라고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 규칙에는 예외가 있는데, 복잡해 보이지만 실제로는 그렇지 않은 SVM이 있습니다. 오른쪽의 그림을 보시면 SVM으로 나눈 평면은 굉장히 복잡해 보이지만, 실제로는 극소수의 Support Vector로 정의되기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이와 관련해서 간단한 퍼즐을 하나 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;매주 월요일 저녁에 축구 경기가 있다고 가정합시다. 그런데 어느 월요일 아침, 당신 앞으로 편지가 한통 왔습니다. A팀과 B팀이 경기하는데 A팀이 이길 것이라는 내용입니다. 아직 경기가 있기 전이고, 대부분이 B팀의 승리를 예측했기 때문에 당신은 편지의 내용을 믿지 않았지만, 실제로 그날 경기는 A팀이 이기게 됩니다.&lt;/p&gt;

&lt;p&gt;다음 주 월요일 아침, 또 동일한 사람에게 편지가 왔습니다. 역시 그 날 저녁의 축구 경기 결과를 예측하는 내용이었으며, 또 맞춰버리고 말았습니다. 이렇게 5주 연속 편지가 왔고, 5주 내내 편지에서는 그 날의 축구 경기 결과를 정확하게 예측하였습니다.&lt;/p&gt;

&lt;p&gt;그런데 6주 째가 되었을 때, 또 편지가 왔지만 이번에는 다른 내용이었습니다. 축구 경기 예측 결과를 더 보고 싶으면 50달러를 지불하라는 내용이었습니다. 이런 상황에서, 당신은 그 가격을 지불할 것인가요?&lt;/p&gt;

&lt;p&gt;정답부터 말씀드리면 당연히 지불해서는 안됩니다. 만약에 편지를 보내는 사람이 처음엔 32명을 대상으로 절반은 A팀 승리/나머지 절반은 B팀 승리로 적어서 편지를 보내고, 맞은 쪽에만 다시 절반은 A팀 승리/나머지 절반은 B팀 승리라는 편지를 보내는 과정을 반복했을지도 모르기 때문입니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 기계학습에서는 예측 값이 의미가 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 두 번째 질문을 해결해봅시다. 왜 단순한 것이 더 좋을까요? 여기서 더 좋다는 의미는 우아해 보인다는 것이 아니라 Out of Sample에서의 성능이 더 좋다는 의미입니다.&lt;/p&gt;

&lt;p&gt;이것에 대해 더 엄밀한 증명은 이상적인 상황을 가정하지만, 여기서는 증명의 요점만을 짚고 넘어가겠습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;단순한 가설이 복잡한 가설보다 적다. (이것은 5장에서 Growth Function을 통해 배웠습니다)&lt;/li&gt;
  &lt;li&gt;단순한 가설은 주어진 데이터 셋에 맞추기 더 적합하지 않다.&lt;/li&gt;
  &lt;li&gt;그렇기 때문에 단순한 가설이 데이터 셋에 맞춰지는 일이 발생한다면, 그것이 더 중요하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;방금 전에 다루었던 우편 퍼즐의 Growth Function을 생각해보면, 편지를 받는 당신은 자신만 그러한 편지를 받았다고 생각했었지만 (일어나기 힘든 일), 현실적으로는 가능한 모든 경우를 고려해서 편지를 보낸 것 (무조건 일어나는 일)이기 때문에 의미가 없던 것이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;적합이 무의미한 과학 실험을 살펴보겠습니다. 실험의 주제는 어떤 특정한 금속의 Conductivity (전도성)이 Temperature (온도)에 선형이라는 가설을 증명하는 것입니다. 이 주제를 가지고 두 명의 과학자가 실험을 하였습니다.&lt;/p&gt;

&lt;p&gt;과학자 A는 두 지점에서 실험을 하였고, 그 둘을 잇는 선을 그렸습니다.&lt;/p&gt;

&lt;p&gt;과학자 B는 세 지점에서 실험을 하였고, 그 셋을 잇는 선을 그렸습니다.&lt;/p&gt;

&lt;p&gt;Conductivity (전도성)이 Temperature (온도)에 선형이라는 가설을 더 명확하게 밝힌 사람은 누구인가요? 오래 생각하지 않더라도 과학자 B가 더 많은 정보를 제공하는 것을 알 수 있습니다. 왜냐하면, 과학자 A가 제시한 2개의 점은 항상 선으로 연결할 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;이와 관련된 개념을 &lt;span style=&quot;color:red&quot;&gt;Falsifiable (위조 가능성)&lt;/span&gt;이라고 합니다. 과학자 A가 제시한 그래프는 사실 위 슬라이드의 3번째 그림처럼 선을 벗어난 점이 있을 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;sampling-bias&quot;&gt;Sampling Bias&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 데이터 수집에서 발생할 수 있는 문제인 Sampling Bias에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 퍼즐은 1948년에 일어났던 미국 대통령 선거입니다. 당시 후보는 Truman과 Dewey 였는데, 한 신문사가 선거가 끝난 직후 당선자를 예측하기 위해 여론조사를 실시하였습니다. 여론조사의 방법은 무작위 사람에게 전화를 걸어 누구에게 투표했는지 물어보는 것이었습니다.&lt;/p&gt;

&lt;p&gt;여론조사를 해보니 오차를 감안하더라도 Dewey가 확실하게 Truman을 이긴다는 결론을 내렸고, 사진과 같이 Dewey가 Truman을 이겼다고 신문에 실었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 문제는 실제로 Truman이 당선되었다는 것입니다.&lt;/p&gt;

&lt;p&gt;더 이상한 것은 통계의 산출 방법이 틀리지 않았다는 것입니다. 충분한 양의 표본을 모았고, 결과를 계산하는 과정도 아무런 문제가 없었습니다.&lt;/p&gt;

&lt;p&gt;단순히 운이 없어서 이런 일이 발생했다고 생각할 수도 있지만, 그렇지 않았습니다. 신문사는 데이터 표본을 10배, 100배 늘린다고 해도 똑같은 결과가 나올 것이라고 판단했기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이 여론 조사의 문제점은 무엇일까요? 바로 표본에 &lt;strong&gt;Bias (편향)&lt;/strong&gt;가 있었습니다. 지금이야 누구나 휴대폰을 갖고 있지만, 1948년에는 전화기 자체가 비싼 물건이었기 때문에 전화를 갖고 있다는 것 자체가 부유한 계층이라는 뜻이었기 때문입니다. 부유한 사람들에게만 여론 조사를 했기 때문에, 부유한 사람이 많이 지지했던 Dewey에게 투표한 사람이 많았고, 그 결과 표본 자체가 부유한 사람들의 의견만을 반영한 결과가 나온 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 데이터가 편항된 방식으로 수집된다면, 학습은 비슷하게 편향된 결과를 낳는다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;학습은 사용자가 제공한 데이터를 통해 세상을 봅니다. 사용자가 비뚤어진 데이터를 준다면, 학습 또한 사용자에게 비뚤어진 가설을 줍니다.&lt;/p&gt;

&lt;p&gt;이와 비슷한 또 하나의 예제를 보겠습니다. 재무 예측에서 기계학습은 많이 사용되는 방법입니다. 당신은 시장의 정상적인 기간을 구하려고 합니다. 실제로 사람들이 사고팔 때 특정한 패턴이 존재합니다. 만약에 실제 시장에서 일어나는 Live Trading를 데이터로 사용한다면, 이것은 데이터 편향이 존재한다고 말할 수 있습니다. 왜냐하면 Live Trading 이외의 부분은 어떠할지 전혀 알 수 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sampling Bias를 처리하는 한 가지 방법은 분포를 일치시키는 것입니다. 실제로 많이 사용하는 방법인데, Input Space에 분포가 있다고 가정하는 것입니다. Hoeffding’s Inequality와 VC Analysis에서는 Training과 Testing이 같은 분포를 갖고 있다고 가정하였습니다. 그렇기에 이 경우 Sampling Bias가 존재하면 가정에 위배되므로 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;따라서 애초부터 Training과 Testing의 분포가 동일하지 않다고 가정하는 것입니다. 그래서 Training과 Testing의 분포를 일치시키기 위해 Training Data에 가중치를 부여하거나, 또는 Resampling 할 수도 있습니다. 단순한 방법이지만, 이 방법을 사용하면 Sampling Bias를 처리할 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;하지만 만약 Training에서의 확률은 0인데, Testing에서의 확률이 0보다 큰 경우에는 사용할 수 없다고 합니다. 방금 보았던 미국의 대선이 바로 이것을 설명하는 예시인데, 전화기가 없는 사람이 실제(Testing)에서는 확률이 0보다 크지만 표본(Training)에서는 확률이 0이었기 때문입니다. 이 때는 확률이 0인 부분에서 어떤 일이 일어날 지 알 수 없기 때문에 데이터에 가중치를 부여하는 등의 작업이 불가능함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3번째 퍼즐은, 이 상황에서의 Sampling Bias를 찾아내는 것입니다.&lt;/p&gt;

&lt;p&gt;은행에서 고객의 신용카드 발급을 자동으로 승인하는 시스템을 만드려고 합니다. 이전에 신청한 고객들의 과거 기록을 기반으로 새 고객의 신용 정보 (오른쪽 표와 같은)을 입력받았을 때 이 사람이 은행에 이익을 가져다 줄지(=신용카드를 발급해줘도 괜찮은지)를 판단하는 시스템입니다.&lt;/p&gt;

&lt;p&gt;혹시 Sampling Bias가 어디서 일어나는지 찾으셨나요? 바로 &lt;strong&gt;이전에 신청한 고객들의 과거 기록&lt;/strong&gt;입니다. 이들은 이미 은행에서 신용카드를 발급해준 대상자들입니다. 그러니까, 신용카드 발급을 거절당한 사람들의 기록은 고려되지 않는 것입니다.&lt;/p&gt;

&lt;p&gt;그런데 사실 이것은 Sampling Bias가 크게 문제 되지 않는 상황이기도 합니다. 은행은 신용카드를 발급해줄 때 어느 정도의 위험성(ex. 고객이 카드를 쓰고 돈을 갚지 않는 상황)을 감수해야 하기 때문에, 다소 보수적으로 기준을 잡기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;data-snooping&quot;&gt;Data Snooping&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 주제로 Data Snooping에 대해 이야기해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 원칙을 먼저 설명한 후에 이야기가 진행됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 데이터 집합이 학습 과정의 어떤 단계라도 영향을 미쳤다면, 결과를 평가하는 능력은 손상된다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이것은 실무자들에게 가장 흔하게 발생하는 실수라고 합니다. 이전에 9장에서도 Data Snooping에 대해 이야기한 적이 있었는데, 그 때는 데이터를 먼저 보고 모델을 선택했을 때 발생하는 실수라고 언급하고 넘어갔습니다. 하지만 이것은 Data Snooping에 빠질 수 있는 경우의 수 중 한 가지에 불과하며 실제로는 이런 함정에 빠지는 방법이 많다는 것입니다.&lt;/p&gt;

&lt;p&gt;이제 Data Snooping이 일어날 수 있는 몇 가지의 예를 확인할 것입니다. 전에 보았던 예도 있지만, 그렇지 않은 것들도 있습니다. 이 예들을 통해 무엇을 피해야 하며 어떤 종류의 Data Snooping이 있는지 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전에 배웠던 Nonlinear Transform으로 시작해봅시다. 오른쪽 그림은 9장에서 Data Snooping을 처음 언급할 때 나왔던 예제입니다. 이때 2차식을 사용하여 Transform 하는 방법으로 문제를 풀었고, 그 결과 $\mathbf{z}$는 6차원의 벡터가 되었습니다.&lt;/p&gt;

&lt;p&gt;문제는 이것을 보고 더 간단하게 표현하고 싶어 $\mathbf{z}$를 직접 손댔을 때 발생했습니다. 이렇게 하면 VC Dimension이 3이기 때문에 더 좋다고 생각할 수 있습니다. 하지만 이렇게 함으로써 실제로 하는 일은 데이터가 아닌, 사용자 스스로 학습하게 일이 되어 버립니다.&lt;/p&gt;

&lt;p&gt;Data Snooping은 데이터 집합 $\mathcal{D}$와 관련이 있습니다. 그렇기 때문이 주어진 데이터 집합에서는 잘 수행될지 모르지만, 독립적으로 생성된 다른 데이터 집합에서도 잘 수행될지의 여부는 알 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또 퍼즐이 나왔습니다. 4번째 퍼즐은 재무 예측 문제에서 Data Snooping이 일어나는 곳을 찾는 것입니다.&lt;/p&gt;

&lt;p&gt;이것은 미국 달러와 영국 파운드 사이의 환율을 예측하는 문제입니다. 여기 8년 분량의 일일 거래 자료가 있습니다. 오른쪽 하단의 초록색 $\Delta r$은 오늘을 기준으로 20일 전까지 일어났던 예측 오율입니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 데이터를 평균과 단위 분산이 0이 되도록 Normalize합니다. 총 2000여 일간의 데이터 중 1500일을 Training Set으로 사용하고, 500일을 Testing Set으로 사용합니다. 물론 두 집합 모두 무작위로 추출합니다.&lt;/p&gt;

&lt;p&gt;이 과정에서 사용자는 어떤 데이터도 눈으로 보지 않았습니다. 방금까지 설명한 모든 과정을 자동으로 수행한 다음, Training Data를 통해 최종 가설을 세우고 Test Set에서 그 성능을 확인합니다. 그 결과 오른쪽 그래프의 빨간 선처럼 우상향 곡선을 그리게 됩니다.&lt;/p&gt;

&lt;p&gt;지금까지 봤을 때, 어느 지점에서도 Data Snooping이 일어나지 않은 것 같습니다. 하지만 분명히 이 과정에서 Data Snooping이 일어났고, 그렇기에 실제 예측 (파란색 곡선)과 큰 차이가 벌어진 것입니다.&lt;/p&gt;

&lt;p&gt;정답을 말씀드리면, Data Snooping은 Data를 Normalize 할 때 발생하였습니다. Normalize 자체가 잘못된 것은 아닙니다. Normalize를 하는 과정에서 Test Set이 포함되었기 때문에, 다시 말해 Training Set이 Test Set에 영향을 주었기 때문에 Data Snooping이 일어난 것입니다. 올바르게 Normalize를 하기 위해서는, 데이터를 먼저 Training Set과 Test Set으로 나눈 다음 Normalize를 해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data Snooping의 또 다른 이름은 Reuse of a Data set (데이터 집합의 재사용) 입니다. 만약 사용자가 어떤 데이터 집합을 가지고 이것저것 학습모델을 사용하다 보면 언젠가는 학습에 성공할 것입니다. 이 말은 다시 말하게 되면,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“만약 당신이 데이터를 충분히 오래 고문하면, 결국에는 자백한다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;여기서 자백한다는 의미는 결과적으로 아무 의미가 없다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;왜 문제가 발생하는지 예를 들어봅시다. 만약에 사용자가 카드 발급을 승인해주는 문제를 푼다고 가정해봅시다. 사용자는 데이터를 전혀 보지 않았고, 정규화시키지도 않았습니다. 그런데 사용자는 우연히 인터넷에서 글을 보다가 카드 발급 승인 문제에서 SVM이 가장 효과가 뛰어나다는 사실을 발견했습니다. 이것을 보고 사용자가 자신도 SVM을 사용하겠다고 결정하면, 데이터를 보지 않았더라도 그 영향을 받은 사실을 사용했기 때문에 문제가 되는 것입니다.&lt;/p&gt;

&lt;p&gt;이것에 대한 핵심적인 문제는 바로 특정한 데이터 집합을 너무 잘 일치시킨다는 사실입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data Snooping에는 두 가지 해결책이 있습니다. 하나는 Data Snooping을 피하는 것[…]이고 다른 하나는 Data Snooping을 설명하는 것입니다.&lt;/p&gt;

&lt;p&gt;Data Snooping을 피하기 위해서는 엄격한 훈련이 필요하다고 합니다. 말은 정말 간단합니다. 만약 이것이 쉽지 않다면 두 번째 방법으로, 데이터가 얼마나 오염되었는지를 알아야 한다고 합니다. 물론 그냥 알기만 하면 안되고, 이전에 데이터 분포를 일치시켰던 것처럼 그에 맞는 대처를 해 주어야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/17. Three Learning Principles/ML 17-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;드디어 마지막 퍼즐입니다. 주식에서 장기간 &lt;strong&gt;Buy and Hold (=장기 투자)&lt;/strong&gt; 했을 때의 성능을 테스트하려고 합니다. 이를 위해 여기서는 50년간의 데이터를 사용합니다. 이를 확인하기 위해 다음과 같은 방법을 사용합니다.&lt;/p&gt;

&lt;p&gt;먼저, 현재 거래되는 모든 주식 회사를 대상으로 합니다. 만약에 50년 전에 당신이라면, 어떤 주식을 구매할 것인지 스스로 판단하고, 50년 후 (즉, 현재) 얼마가 되어있을지를 계산하는 겁니다.&lt;/p&gt;

&lt;p&gt;이 간단한 작업에도 Sampling Bias가 생겼습니다. 현재 거래되는 주식은 50년 전에 분명히 있었지만, 50년 전에 있던 주식회사 중 망한 회사는 선택에서 배제되었기 때문입니다. 문제는 이 과정은 (50년 전을 기준으로) 미래의 데이터를 보고 결정한 것이기 때문에 Sampling Bias 보다는 Data Snooping과 혼동이 생긴다는 것입니다. 여기에서는 두 가지 성질을 모두 가지고 있으므로, Snooping으로 인한 Sampling Bias라고 결론지었습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Radial Basis Functions</title><link href="http://localhost:4000/studies/radial-basis-functions/" rel="alternate" type="text/html" title="Radial Basis Functions" /><published>2019-11-02T00:00:00+09:00</published><updated>2019-11-02T00:00:00+09:00</updated><id>http://localhost:4000/studies/radial-basis-functions</id><content type="html" xml:base="http://localhost:4000/studies/radial-basis-functions/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;16장은 Radial Basis Function을 배우게 됩니다. 이것으로 데이터에 Label이 없는 Unsupervised Learning을 해결하는 방법을 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장에서 다룰 내용은 총 4가지인데, 첫 번째 주제를 제외한 나머지는 짧게 다루기 때문에 분량은 다른 장들과 비슷합니다. 첫 번째로 Radial Basis Function 표준 모델을 배우고 Nearest Neighbors Algorithm과 비교합니다. 두 번째로는 Neural Network와 간단하게 비교, 세 번째로 RBF와 Kernel Method와의 비교, 그리고 마지막으로 Regularization과의 비교를 다룰 것입니다. 여기서의 비교는 서로의 연관성과 차이점을 찾는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Radial Basis Function&lt;/span&gt;의 기본 아이디어는 데이터 집합의 모든 점이 가설에 영향을 준다는 것입니다. 그런데 잠깐 생각해보면 데이터 집합을 통해 가설을 만들기 때문에 데이터 집합이 가설에 영향을 주는 것은 당연한 것이 아니냐고 생각할 수 있습니다. 하지만 RBF는 ‘거리’라는 특별한 방법으로 영향을 받습니다. 다시 말해, 데이터 집합의 한 점이 근처에 있는 다른 점에 영향을 미친다는 것입니다.&lt;/p&gt;

&lt;p&gt;이해를 돕기 위해, 슬라이드 오른쪽에 나와있는 산 모양의 그림을 봅시다. 산 꼭대기에 데이터 집합 중 한 점 $\mathbf{x}_n$이 있다고 가정해보면, 이 그림은 $\mathbf{x}_n$이 이웃에 대한 영향력이 크다는 것을 나타내고 있습니다. RBF는 거리에 비례하기 때문에, 이 그림은 모든 방향에 대해 대칭적인 구조라는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;RBF의 기본 형태는 복잡해보이지만, 구조를 이해한다면 생각보다 간단한 개념임을 알 수 있습니다. 먼저 데이터 집합 중 임의의 데이터 점 하나를 $\mathbf{x}$로 정합니다. 가설은 이 점을 기반해서 도출되기 때문에, $h(\mathbf{x})$라고 부릅니다.&lt;/p&gt;

&lt;p&gt;그다음 다른 모든 데이터가 이 $\mathbf{x}$와 얼마나 떨어져 있는지를 계산합니다. 여기서는 Gaussian Function을 사용합니다. 이것이 $\text{exp}( \cdot )$ 부분입니다. 그런 후 데이터 집합의 각 점 $\mathbf{x}_n$이 얼마나 중요한지를 나타내는 가중치 $w_n$이 붙습니다. 나중에 다시 나오겠지만, 이 가중치 $w_n$은 $y_n$과 관련이 있습니다.&lt;/p&gt;

&lt;p&gt;이 과정을 $N$개의 모든 데이터에 수행해주면 RBF 표준 모델 $h(\mathbf{x})$을 만들 수 있습니다. 이것이 Radial Basis Function이라고 부르는 이유는 $\lVert \mathbf{x} - \mathbf{x}_n \rVert$이 Radial Function이고, $\text{exp}( \cdot )$이 Basis Function이기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델을 알았으니, 이제 RBF의 학습 알고리즘이 무엇인지 알아봅시다. RBF의 기본 형태를 보시면 데이터 집합은 이미 주어졌으니 가중치 $w$만 구하면 됩니다. 그런데 기본 형태를 보시면 가중치의 개수는 $N$개이고, 데이터를 하나하나 대입해서 나오는 식 또한 $N$개입니다. 고등학교 수학에서 미지수의 개수와 식의 개수가 같으면 정확히 1개의 해답이 나오는 것을 배웠습니다. 그렇기 때문에 모든 가중치 $w$를 구하게 되면 In Sample Error가 정확히 0이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 보았듯이 학습 알고리즘은 매우 간단합니다. 결과적으로 이 문제는 $N$개의 방정식이 주어지고 $N$개의 미지수를 구하는 산수 문제이기 때문입니다. 다만 미지수의 개수가와 방정식의 수가 주어진 데이터의 수와 같기 때문에 그것이 매우 귀찮은 일일 뿐입니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 여기서는 행렬을 사용해서 미지수를 한번에 계산합니다. 선형대수학에서 방정식의 미지수를 구할 때 역행렬을 사용해 계산하는 방법을 배웠으므로 그 방법을 사용하면 간단하게 모든 가중치를 구할 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Exact Interpolation (정확한 보간)&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 RBF에서 $\gamma$의 값이 어떻게 영향을 주는지 알아봅시다. 먼저 $\gamma$의 값이 작다고 가정해봅시다. Gaussian Distribution (정규분포)의 그래프를 생각해보시면 비교적 분포가 고른 완만한 모양이 될 것입니다. 반대로 $\gamma$의 값이 크다면 평균에 대부분이 몰려있는 모양이 됩니다. RBF에서는 이러한 Gaussian Function의 합으로 이루어져 있기 때문에 $\gamma$의 값에 따라 어떤 모양이 될지 대략적으로 상상할 수 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 데이터 집합이 단 3개의 점으로 이루어져 있다고 가정해봅시다. $\gamma$의 값이 큰 경우에는 각각의 데이터 점에 몰려있는 Gaussian Function의 합이기 때문에 데이터 점에서 조금만 거리가 멀어져도 그 영향력이 확 줄어버리게 됩니다. 결과적으로 오른쪽 그림처럼 뾰족뾰족한 모양이 됩니다.&lt;/p&gt;

&lt;p&gt;이번에는 $\gamma$의 값이 작다고 가정해봅시다. 이 경우에는 각각의 데이터 점이 거리가 떨어져도 그 영향력이 완만하게 줄어들기 때문에 3개의 Gaussian Function이 합쳐져 왼쪽 그림과 같이 하나의 산봉우리와 같은 모양이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 다루었던 RBF 모델은 Regression을 위한 모델이었습니다. 3장에서도 배웠듯이 Regression을 사용해 Classification을 할 수도 있습니다. 이번에는 RBF를 사용해 Classification을 하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;방법 자체는 매우 간단합니다. 그저 기존의 식에 $\text{sign}$만 넣어서 Output이 $+1/-1$로만 나오게 만드는 것입니다. 다만 이번에는 Error Measure도 $(s-y)^2$로 달라지기 때문에 더 이상 In Sample Error가 0으로 고정되지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;rbf-and-nearest-neighbors&quot;&gt;RBF and nearest neighbors&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 RBF 모델과 비슷한 아이디어로 시작된 모델인 Nearest-neighbor method와 어떤 관계가 있는지 살펴봅시다. Nearest-neighbor method는 기존에 분류가 완료된 Training Set을 기반으로 새로운 입력이 들어왔을 때 가장 가까운 데이터와 동일하게 분류하는 간단한 분류 방법입니다. 왼쪽의 그림을 보시면 Training Set의 거리를 기반으로 정확히 중간 지점을 선으로 표현하였습니다. 새로운 데이터가 들어오면 어느 영역에 있는지를 알게 되면 즉각적으로 분류할 수 있습니다.&lt;/p&gt;

&lt;p&gt;RBF 모델 또한 이와 비슷한 개념이기 때문에 RBF를 사용해서 Nearest-neighbor method를 구현할 수 있습니다. RBF로 Nearest-neighbor method를 표현한다면 오른쪽 그림과 같이 실린더 모양이 됩니다. 이것이 의미하는 것은 간단한데, 일정 거리까지는 최대의 영향력을 발휘하지만, 그 범위를 벗어나면 0의 영향력을 가지게 됩니다.&lt;/p&gt;

&lt;p&gt;모양과 의미를 확인해보면 모델 자체가 굉장히 불안정한 것을 알 수 있습니다. 특히 경계선에 위치한 데이터들은 약간의 차이로 +1과 -1이 갈리게 됩니다. 이 문제를 보완하기 위해 보통은 K-Nearest-neighbor method를 사용하는데, 이것은 가장 가까운 K개의 데이터를 찾은 다음 +1이나 -1이 더 많은 쪽으로 분류하는 방법입니다. 이렇게 경계선에서 불안정성이 발생할 때 이를 보완하는 작업을 &lt;span style=&quot;color:red&quot;&gt;Smoothing&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 K-Nearest-neighbor method를 RBF로는 어떻게 구현하는지 알아봅시다. 기존 RBF 모델은 모든 데이터가 가설에 영향을 끼쳤기 때문에 $N$개의 데이터를 모두 사용해서 가설을 만들었고, 그렇기에 $N$개의 가중치 $w$가 있었습니다.&lt;/p&gt;

&lt;p&gt;이제는 주어진 데이터와 가장 가까운 $K$개의 Center만 영향을 끼치게 만들어야 합니다. 그들을 각각 $\mu_1, …, \mu_K$라고 정의합니다. $\mu$는 데이터 집합의 일부일 수도 있지만, 그렇지 않고 사용자가 직접 선정한 특별한 지점일 수도 있습니다.&lt;/p&gt;

&lt;p&gt;$\mu$를 사용해서 RBF를 수정하는 것은 간단합니다. 기존의 RBF에서 $\Sigma$에 있던 $N$을 $K$로 바꿔주고 Basis Function $\text{exp}( \cdot )$에 있던 $\mathbf{x}_n$을 $\mu_k$로 바꿔주면 됩니다.&lt;/p&gt;

&lt;p&gt;하지만 여기에는 두 가지 문제가 있습니다. 첫째로, Center $\mu_k$를 어떻게 선택해야 할까요? 둘째로 그 때 $w_k$는 어떻게 선택해야 할까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Center $\mu_k$를 정하는 방법을 알아봅시다. 하나 떠오르는 방법은 전체 데이터 $\mathbf{x}_1, … \mathbf{x}_N$를 $K$개의 클러스터로 나누어 각 클러스터의 Center에서 각각의 데이터와 평균 제곱 오차를 최소화하게 만드는 것입니다.&lt;/p&gt;

&lt;p&gt;이 방법의 장점은 이것이 Unsupervised Learning이라 $y_n$에 관계없이 구할 수 있다는 것입니다. 하지만 이 방법이 &lt;strong&gt;NP-Hard&lt;/strong&gt;라는 단점이 있습니다. 쉽게 말해 시간복잡도가 다항함수로 표현될 수 없다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 NP-Hard 문제를 푸는 Iterative Algorithm으로는 &lt;span style=&quot;color:red&quot;&gt;Lloyd’s Algorithm&lt;/span&gt;이 있습니다. 아이디어는 간단합니다. 먼저 무작위로 클러스터를 정한 다음 그 클러스터를 기반으로 Center $\mu_k$를 계산합니다. 그 후에는 그 Center들을 기준으로 클러스터를 다시 만듭니다. 이 과정을 반복하면 결국에 Center들은 특정한 점들로 수렴하게 됩니다. 한 가지 문제는 이것이 &lt;strong&gt;Global Minimum&lt;/strong&gt;이 아니라 &lt;strong&gt;Local Minimum&lt;/strong&gt;으로 수렴하게 된다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lloyd’s Algorithm을 간단하게 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;데이터 점들을 얻습니다.&lt;/li&gt;
  &lt;li&gt;지금은 $y$를 배제하고 $\mathbf{w}$만 갖고 수행합니다.&lt;/li&gt;
  &lt;li&gt;무작위로 Center를 정합니다. 몇 개의 Center를 정하는 것도 문제이지만, 여기서는 Support Vector와 비교하기 위해 Support Vector와 똑같은 수인 9개로 정했습니다.&lt;/li&gt;
  &lt;li&gt;주어진 Center를 기반으로 클러스터링을 하고, 그 클러스터링에서 새로운 Center를 찾습니다. 이 과정을 반복합니다.&lt;/li&gt;
  &lt;li&gt;더 이상 Center가 이동하지 않는다면 바로 그 점이 최종 $\mu$ 입니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 방법을 통해 데이터들을 분류한다면 위 슬라이드의 오른쪽 그림과 같습니다. 예제에서 좋지 않은 분류가 하나 나왔는데, 바로 가장 왼쪽 아래에 있는 Center입니다. 이 Center를 포함한 클러스터는 $+1$과 $-1$을 모두 갖고 있기 때문에 RBF가 제 역할을 하지 못하지만, 강의에서는 이 정도는 Unsupervised Learning을 사용할 때 감수해야 한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;14장에서 배운 Support Vector도 데이터 집합을 대표하는 점이었고, RBF의 Center도 데이터를 대표하는 점입니다. 이 둘을 한번 비교해봅시다. 이 둘은 똑같이 데이터를 대표하는 점이지만 구하는 방법부터 하는 역할까지 모두 다릅니다. 차이점을 간단하게 정리하면 아래와 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Support Vector는 Supervised Learning으로 구하지만, RBF Center는 Unsupervised Learning으로 구한다.&lt;/li&gt;
  &lt;li&gt;Support Vector는 무조건 데이터 집합의 점 중 하나이지만, RBF는 그럴 수도 있고, 아닐 수도 있다.&lt;/li&gt;
  &lt;li&gt;Support Vector는 Separating Surface (분리 평면)을 표현하지만, RBF Center는 데이터 입력을 표현한다.&lt;/li&gt;
  &lt;li&gt;각각의 Support Vector는 $+1/-1$로 구분되지만 RBF Center는 Label이 없다. ($=y_n$을 사용하지 않는다)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 RBF의 Center $\mu$를 모두 구했으니, 남은 것은 가중치 $w$를 구하는 것만 남았습니다.&lt;/p&gt;

&lt;p&gt;그런데, 아까와는 다른 문제가 생겼습니다. 처음 모든 데이터 집합을 사용했을 때는 방정식이 $N$개, 미지수가 $N$개로 동일했기 때문에 In Sample Error를 정확하게 0으로 만드는 가중치 $w$를 찾았지만, 이제는 방정식이 똑같이 $N$개인 상황에서 미지수가 $K$개로 줄어든 상황이 되었습니다.&lt;/p&gt;

&lt;p&gt;고등학교 때 배운 기억을 되살려보면, 방정식의 개수가 미지수의 개수보다 많을 때는 풀지 못한다고 배웠습니다. 그렇기 때문에 여기서는 식을 약간 변형시킵니다. 아까는 데이터 $(\mathbf{x}_n, y_n)$를 대입시켰을 때 정확하게 일치했기 때문에 $=$로 표현했지만, 이제는 약간의 In Sample Error를 감수해야 하므로 $\approx$로 표현합니다.&lt;/p&gt;

&lt;p&gt;이렇게 하고 나면 나머지는 이전과 똑같이 행렬로 표현할 수 있습니다. 다만 이제는 $\Phi$가 정사각행렬이 아니기 때문에 역행렬이 존재하지 않습니다. 그렇기 때문에 $w$를 구하려면 3장에서 배운 &lt;strong&gt;의사역행렬(Pseudo-Inverse)&lt;/strong&gt;를 사용해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 RBF를 Graphic Network로 표현해 보겠습니다. 이 그림은 Basis Function $\text{exp}(-\gamma \lVert \mathbf{x} - \mu_k \rVert^2)$을 $\phi$로 표현한 그림입니다.&lt;/p&gt;

&lt;p&gt;아래쪽부터 본다면 데이터 점 $\mathbf{x}$가 각각의 Center $\mu_1, …, \mu_K$와의 거리를 계산하는 것부터 시작합니다. 그리고 나서 각각의 거리를 제곱한 후, $-\gamma$를 곱해 Exponential $e$의 지수에 넣습니다. 최종적으로 이것들을 전부 더해주고 나면 $h(\mathbf{x})$가 도출됩니다. (이 더하는 과정에서 Bias Term $b$ 혹은 $w_0$가 더해질 수도 있습니다.)&lt;/p&gt;

&lt;p&gt;그런데 이 구조는 어디서 많이 본 것 같은 구조입니다. 이 RBF Network를 가로로 눕혀서 본다면 Neural Network와 비슷한 모양이 됨을 유추할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;rbf-and-neural-networks&quot;&gt;RBF and neural networks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 RBF Network와 Neural Network를 서로 비교해봅시다. 비교를 쉽게 하기 위해, 오른쪽의 Neural Network는 고의적으로 RBF Network와 유사한 구조로 디자인하였습니다.&lt;/p&gt;

&lt;p&gt;역시 비교를 위해서는 아래쪽 부분부터 살펴봅시다. RBF Network의 $\lVert \mathbf{x} - \mu \rVert$ 부분은 데이터와 Center의 거리가 멀다면 $\phi$를 지날 때 0에 수렴할 것입니다. 즉, Network Output의 기여도가 0에 수렴할 것입니다. 반면 Neural Network에서는 $\mathbf{w} \mathbf{x}$의 값이 크든 작든 Sigmoid 함수를 통과할 것이고, Sigmoid 그래프의 특성상, 이 값은 언제나 Network Output에 일정 부분 기여를 한다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 선택할 마지막 변수는 Gauss Function에서의 $\gamma$ 입니다. 슬라이드 6에서 보았듯이 $\gamma$의 크기에 따른 변화가 적지 않기 때문에 적절한 $\gamma$를 찾는 것은 매우 중요합니다. $\gamma$를 구할 때는 일반적으로 Gradient Descent를 사용해서 In Sample Error가 가장 낮게끔 선택합니다.&lt;/p&gt;

&lt;p&gt;여기에 또 문제가 있습니다. 왜냐하면 Pseudo Inverse로 $w$를 구할 때는 $\gamma$ 값을 알고 있다는 가정 하에 계산하기 때문입니다. 따라서 반복적인 방법으로 $\gamma$를 계산합니다. 여기에 사용하는 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;Expectation Maximazation (EM) Algorithm&lt;/span&gt; 이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;이 알고리즘의 아이디어는 Lloyd’s Algorithm과 유사합니다. 먼저 $\gamma$를 임의의 값으로 고정한 후, 가중치 $w_1, … ,w_K$를 계산합니다. 그 다음에는 반대로 가중치 $w_1, … ,w_K$를 고정한 후 In Sample Error를 최소화하는 $\gamma$를 계산합니다. 여기서는 $\gamma$를 단 1개의 매개변수로 가정했지만, 각 Center 별로 $\gamma$의 값을 다르게 하는 경우도 동일하게 처리할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;rbf-and-kernel-methods&quot;&gt;RBF and kernel methods&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 RBF와 Kernel Method, 그리고 Regularization과의 연관성을 빠르게 짚고 마치겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;15장에서 Kernel Method를 배울 때 잠깐 RBF Kernel을 언급했었습니다. 이번에는 SVM Kernel과 RBF를 사용한 Classification이 어떻게 다른지를 살펴보겠습니다. (RBF 식에서 $\text{sign}$과 $b$가 파란색 글씨로 나온 이유는 없어도 되는 부분이기 때문입니다.)&lt;/p&gt;

&lt;p&gt;결과만 놓고 보았을 때 오른쪽 그림을 보시면 SVM이 RBF보다 더 잘 Classification 한 것을 볼 수 있습니다. 하지만 이것만 놓고 보았을 때 SVM이 RBF보다 우수하다고 판단할 수는 없습니다. 왜냐하면 이 문제에서의 Support Vector는 9개가 나오는데, RBF가 동등한 조건에서 경쟁하기 위해 $K$를 9로 놓았기 때문입니다. 이전 문제에서도 보았듯이, Center를 9개로 정하는 것이 이 문제에서 최선의 방법이 아니기 때문에 RBF 입장에서는 조금 불공평한 경쟁이라고 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;rbf-and-regularization&quot;&gt;RBF and regularization&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/16. Radial Basis Functions/ML 16-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 RBF와 Regularization의 연관성을 정리하겠습니다. Regularization을 통해 RBF를 유도할 수 있습니다. 간단한 예시를 만들기 위해, 데이터는 1차원 입력이라고 가정하겠습니다. 이 데이터로 가설 $h$를 만들고, Squared Error를 계산하면 $(h(x_n) - y_n)^2$ 이 됩니다. 이것을 모든 데이터에 대해 수행해 준 다음 더하게 되면 In Sample Error가 나오는데, 이 자체만 Minimize 하게 되면 Overfitting이 발생하여 $\lambda$를 포함한 추가 항을 더한 다음 Minimize 하는 방법이 Regularization이었습니다.&lt;/p&gt;

&lt;p&gt;여기서는 $\lambda$ 항에 가설 $h$를 입력 $x$에 대해 $k$번 미분한 것의 크기(제곱)를 $-\infty$부터 $\infty$까지 적분해준 값에 상수 $a_k$를 곱해 $k=0$부터 $k=\infty$까지 더한 항을 곱해주었습니다. 왜 이런 식을 해야 하는지는 강의에서도 너무 복잡한 과정이라서 생략하였는데, 이런 Regularization을 수행하면 그 결과가 바로 RBF와 정확하게 일치한다고 설명하였습니다.&lt;/p&gt;

&lt;p&gt;이것은 &lt;span style=&quot;color:red&quot;&gt;Smoothest Interpolation (가장 매끄러운 보간)&lt;/span&gt;이라고 하며 RBF에 대한 또 다른 해석이라고 합니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Kernel Methods</title><link href="http://localhost:4000/studies/kernel-methods/" rel="alternate" type="text/html" title="Kernel Methods" /><published>2019-10-26T00:00:00+09:00</published><updated>2019-10-26T00:00:00+09:00</updated><id>http://localhost:4000/studies/kernel-methods</id><content type="html" xml:base="http://localhost:4000/studies/kernel-methods/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;15장은 Kernel Method를 배우게 됩니다. 지난 시간에 배웠던 Support Vector Machine에서 이어지는 내용입니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 크게 두 주제로 나뉘어 있습니다. 비선형 변환을 처리하기 위한 Kernel Trick이 무엇인지를 먼저 배우고, SVM의 방법 중 하나인 Soft-margin SVM을 배우게 됩니다. 두 주제가 서로 연관이 되어있지는 않지만, 두 주제 모두 비선형 문제를 해결하기 위한 시도라고 생각하시면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;the-kernel-trick&quot;&gt;The Kernel trick&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 비선형 문제를 풀기 위해서는 비선형 문제를 선형 문제로 변환시키기 위해 $\mathcal{Z}$ 공간으로 데이터를 Transform 시키는 방법을 사용하였습니다. 그러나 일반적으로 적당한 Transform Function $\Phi$를 찾는 것은 쉽지 않기 때문에 Transform을 시키지 않은 채로 비선형 문제를 해결하는 방법이 필요합니다. Kernel의 아이디어는 바로 거기서부터 시작합니다.&lt;/p&gt;

&lt;p&gt;먼저 지금까지 $\mathcal{Z}$ 공간에서 어떤 일을 했었는지를 떠올려봅시다. 지난 시간에 배웠던 라그랑지안 $\mathcal{L}$ 식에서 맨 뒷부분을 보시면, 보라색으로 표시된 $\mathbf{z}^{\sf T}_{n}$과 $\mathbf{z}_m$의 내적 부분이 바로 유일하게 $\mathcal{Z}$ 공간이 사용되는 부분입니다. 라그랑지안에서의 Constraints 부분은 $\mathcal{Z}$ 공간과 관련이 없습니다.&lt;/p&gt;

&lt;p&gt;라그랑지안을 풀고 찾은 가설 $g$에도 $\mathbf{z}$가 쓰입니다. 그런데 $g$에서의 Weight인 $\mathbf{z}$가 쓰이므로, 결과적으로 가설 $g$에는 $\mathbf{z}$ 자체보다는 $\mathbf{z}_n$과 $\mathbf{z}$의 내적만이 필요합니다. $b$ 또한 식을 살펴보면 $\mathbf{z}_n$과 $\mathbf{z}_m$의 내적만 알고 있다면 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이로써 알 수 있는 것은 라그랑지안을 통해 $\mathcal{Z}$ 공간에서 가설 $g$를 구하기 위해서는 $\mathbf{z}$를 직접 구할 필요 없이 $\mathbf{z}$ 간의 내적만 알 수 있으면 된다는 것입니다. 만약에 $\mathcal{Z}$ 공간까지 가지 않더라도 $\mathbf{z}$ 간의 내적을 구할 수 있다면, 굳이 힘들게 데이터들을 $\mathcal{Z}$ 공간으로 Transform 시키는 수고를 하지 않아도 될 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주어진 데이터는 $\mathcal{X}$ 공간에 있는 임의의 두 점 $\mathbf{x}$와 $\mathbf{x}’$이라고 가정합시다. 이 두 점을 $\mathcal{Z}$ 공간으로 Transform 시킨 점을 $\mathbf{z}$와 $\mathbf{z}’$이라고 하면, 우리가 원하는 것은 주어진 데이터만으로 $\mathbf{z}$와 $\mathbf{z}’$의 내적을 구하는 것입니다. 이 내적을 $K(\mathbf{x}, \mathbf{x}’)$으로 표현하고, &lt;span style=&quot;color:red&quot;&gt;Kernel&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;이해를 돕기 위해 주어진 데이터가 $\mathbf{x} = (x_1, x_2)$라고 가정해 보겠습니다. 만약 Transform Function $\Phi$이 2차 다항식으로 주어진다면 Transform을 한 결과는 $\mathbf{z} = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)$가 됩니다. $K(\mathbf{x}, \mathbf{x}’)$를 계산하는 과정은 아래와 같습니다.&lt;/p&gt;

\[\begin{align}
K(\mathbf{x}, \mathbf{x}&apos;) &amp;amp;= \mathbf{z}^{\sf T} \mathbf{z}&apos; \\&amp;amp;=(1, x_1, x_2, x_1^2, x_2^2, x_1 x_2) \cdot (1, {x&apos;}_1, {x&apos;}_2, {x&apos;}_1^2, {x&apos;}_2^2, {x&apos;}_1 {x&apos;}_2) \\&amp;amp;=1 + x_1 {x&apos;}_1 + x_2 {x&apos;}_2 + x_1^2 {x&apos;}_1^2 + x_2^2 {x&apos;}_2^2 + x_1 {x&apos;}_1 x_2 {x&apos;}_2
\end{align}\]

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 두 점 $\mathbf{x}$와 $\mathbf{x}’$을 Transform을 하지 않고 $K(\mathbf{x}, \mathbf{x}’)$를 구하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;이번에는 Kernel Function을 $K(\mathbf{x}, \mathbf{x}’) = (1 + \mathbf{x}^{\sf T} \mathbf{x}’)^2$로 가정해봅시다. 이 식을 전개하면 $1 + x_1^2 {x’}_1^2 + x_2^2 {x’}_2^2 + 2 x_1 {x’}_1 + 2 x_2 {x’}_2 + 2 x_1 {x’}_1 x_2 {x’}_2$가 되는데, 잘 살펴보면 두 벡터 $(1, x_1^2, x_2^2, \sqrt{2} x_1, \sqrt{2} x_2, \sqrt{2} x_1 x_2)$와 $(1, {x’}_1^2, {x’}_2^2, \sqrt{2} {x’}_1, \sqrt{2} {x’}_2, \sqrt{2} {x’}_1 {x’}_2)$의 내적을 수행한 결과라는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 $(1 + \mathbf{x}^{\sf T} \mathbf{x}’)$의 제곱의 형태인 Kernel을 &lt;span style=&quot;color:red&quot;&gt;Polynomial Kernel (다항식 커널)&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 상황을 고려하기 위해 $d$차원의 주어진 데이터를 $Q$차 다항식으로 Transform을 하게 된다면, 이와 동등한 Kernel은 $K(\mathbf{x}, \mathbf{x}’) = (1 + \mathbf{x}^{\sf T} \mathbf{x}’)^Q$로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그런데 문제는 만약 $d$와 $Q$가 커지게 되면 $K(\mathbf{x}, \mathbf{x}’)$를 구하기가 너무 어렵다는 것입니다. 슬라이드에 나온대로 $d=10$, $Q=100$인 상황만 가정하더라도 10차 다항식을 100제곱 하는 결과를 구해야 하는데, 계산량이 너무 많아 전개하는 것이 거의 불가능합니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 Polynomial Kernel에서는 $\mathbf{x}^{\sf T} \mathbf{x}’$를 전개하지 않고 $a \mathbf{x}^{\sf T} \mathbf{x}’ + b$ 의 형태로 만들어 이항정리를 사용해 $\mathbf{x}^{\sf T} \mathbf{x}’$의 계수만을 구하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kernel이 두 점 $\mathbf{x}$와 $\mathbf{x}’$의 내적으로 정의된 문제는 해결되었으나, 그 외에 상황을 고려할 필요가 있습니다. 이번 예제에 나온 Kernel은 $\mathbf{x}$와 $\mathbf{x}’$의 내적으로 표현되지 않고 &lt;strong&gt;Euclidean Norm&lt;/strong&gt;으로 표현되고 있습니다. 이런 식의 Kernel이 $\mathcal{Z}$ 공간에 존재하는 지를 보여야 합니다.&lt;/p&gt;

&lt;p&gt;결론부터 말하면 이 Kernel은 무한차원의 $\mathcal{Z}$ 공간에 존재합니다. 간단한 예를 들면 주어진 점 $\mathbf{x}$를 1차원이라 가정합니다. 그렇다면 $\mathbf{x}$와 $\mathbf{x}’$는 모두 스칼라로 표현이 되므로 $x$와 $x’$으로 대체합니다. 그리고 $\gamma$를 간단하게 1로 놓습니다.&lt;/p&gt;

&lt;p&gt;그런 후에 이 식을 &lt;strong&gt;Taylor Series (테일러 급수)&lt;/strong&gt;로 표현한다면 좀 더 복잡한 식이 되긴 합니다. 하지만 보기 쉽게 $x$와 $x’$를 갖고 있는 것을 각각 분리한다면 두 무한 차원의 벡터의 내적으로 이루어진 식임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;참고로 이런 Kernel을 &lt;span style=&quot;color:red&quot;&gt;Radial Basis Function Kernel&lt;/span&gt;이라고 합니다. 다음 장에서 이를 더 자세하게 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 Kernel을 실제 비선형 문제에서 적용하는 방법을 살펴봅시다. 주어진 $\mathcal{X}$ 공간의 데이터들을 무한 차원인 $\mathcal{Z}$ 공간에 Transform 할 필요 없이, 이전 슬라이드에서 배운 Kernel만을 사용할 것입니다. Kernel을 Quadratic Programming에 입력하면 알아서 Support Vector가 구해지기 때문에 직접적인 계산은 필요가 없습니다.&lt;/p&gt;

&lt;p&gt;그림이 작아서 잘 보이진 않지만, 파란색의 점과 빨간색의 점이 양쪽의 Support Vector입니다. 그리고 그 Support Vector를 따라 그린 검은색 곡선이 학습 결과가 됩니다. Support Vector인데 왜 Margin이 크지 않는지 궁금해하실 수도 있는데, 이 검은색 곡선은 $\mathcal{Z}$ 공간에서 Margin이 최대인 직선으로 그린 것이기 때문에 지금 보는 $\mathcal{X}$ 공간과는 관련이 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Support Vector Machine을 계산할 때, 위와 비슷한 Quadratic Programming 문제를 풀었던 것을 기억하실 겁니다. 그 당시에는 $\mathcal{X}$ 공간의 문제를 풀었기 때문에 저 자리에 $\mathbf{x}$와 $\mathbf{x}’$의 내적이 들어가 있었습니다. 비선형 문제를 풀기 위해서는 저 자리에 $\mathbf{z}$와 $\mathbf{z}’$의 내적이 있어야 하지만, 그것이 싫어 $K(\mathbf{x}, \mathbf{x}’)$로 대체한 것이 지금까지 한 내용입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 Kernel에서 Final Hypothesis를 어떻게 구하는지를 알아봅시다.&lt;/p&gt;

&lt;p&gt;이전에도 보았듯이, Hypothesis를 $\mathcal{Z}$ 공간에서 구했기 때문에 $g$는 $\mathbf{z}$가 포함된 식으로 표현이 가능했습니다. 하지만 지금까지 우리는 $\mathbf{z}$를 배제해왔으니, 이것 대신 Kernel $K( - , - )$로 표현할 수 있도록 식을 바꾸어봅시다.&lt;/p&gt;

&lt;p&gt;이번 장의 앞부분에서 했던 것처럼 $\mathbf{w}$를 풀어서 전개하면 $\mathbf{z}$ 간의 내적으로 표현할 수 있게 되고, 그 부분을 $K(\mathbf{x}_n, \mathbf{x})$로 대체할 수 있습니다. 시그마가 포함된 앞부분은 더이상 정리할 수 없지만, $b$는 또다시 Kernel로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 배운 Kernel의 유일한 문제, 스스로 만든 임의의 Kernel이 유효한지를 알 수 없다는 것입니다. 다시 말해, 우리가 제시한 Kernel이 어떤 $\mathcal{Z}$ 공간에서 나온 Kernel임을 보여야 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;크게 3가지 접근방법이 있는데, 첫 번째는 Polynomial Kernel과 같이 개념적인 방법으로 접근하여 Kernel을 만드는 방법입니다. 두 번째는 다음 슬라이드에서 설명할 Kernel의 수학적인 특성을 이용하는 것입니다. 세 번째는 저자분이 선호하는 방법이라고 하는데 그냥 신경 쓰지 않는 방법이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 말한 Kernel의 수학적인 특성을 이용해 Kernel을 디자인해봅시다. 임의의 Kernel이 유효하기 위한 필요충분조건은 첫째로 Symmetric 해야 한다는 조건이 있습니다. 이것은 $\mathbf{x}$와 $\mathbf{x}’$의 위치를 서로 바꿔도 원래의 식과 동일해야 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;두번째는 모든 데이터 점을 사용해 만든 Kernel Matrix가 &lt;span style=&quot;color:red&quot;&gt;Positive Semi-Definite&lt;/span&gt;여야 합니다. Matrix가 Positive Semi-Definite라는 것은 해당 Matrix의 모든 Eigenvalue가 음수가 아니라는 것인데, 일반적으로는 영벡터가 아닌 $x$에 대해 $x^{\sf T} M x \ge 0$이라면 Matrix $M$이 Positive Semi-Definite라고 부릅니다.&lt;/p&gt;

&lt;p&gt;이 수학적 특성을 &lt;span style=&quot;color:red&quot;&gt;Mercer’s Condition&lt;/span&gt;이라고 부르는데, 보시다시피 두 번째 조건은 데이터의 많은 경우 계산이 어렵기 때문에 실제로 이를 보이는 것은 쉽지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;soft-margin-svm&quot;&gt;Soft-margin SVM&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kernel에 대한 이야기는 끝났고, 다음으로는 Soft-margin SVM에 대해 알아봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전과 마찬가지로 비선형 문제를 다룰 것이지만, 비선형 문제도 두 가지 종류로 나눌 수 있습니다. 하나는 왼쪽 그림처럼 몇 개의 데이터만 무시한다면 선형으로 나눌 수 있는 경우고, 다른 하나는 오른쪽 그림처럼 아예 선형 분류를 시도조차 할 수 없는 경우입니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림과 같은 경우는 Kernel로 처리하면 되지만, 왼쪽 그림과 같은 경우는 Kernel보다 더 나은 방법이 있을 것 같습니다. 이제 배울 &lt;span style=&quot;color:red&quot;&gt;Soft-Margin SVM&lt;/span&gt;으로 이런 경우를 처리할 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Support Vector Machine의 Error Measure를 다시 한번 살펴보겠습니다. 현재 문제가 되는 것은 Support Vector 사이에 있는 Margin 영역에 &lt;strong&gt;Violation (침범)&lt;/strong&gt;하는 데이터입니다. 이 Violation 데이터 때문에 Support Vector를 구하는 식인 $y_n (\mathbf{w}^{\sf T} \mathbf{x}_n + b) \ge 1$가 성립하지 않습니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 Violation 데이터를 정량화할 필요가 있고, 원래의 최적화 식을 변형해야 합니다. 오른쪽 항에 0보다 큰 $\xi_n$를 빼주는데, $\xi_n$의 총 합이 Violation이 일어나는 총 합이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Support Vector를 구하는 식을 바꾸었으니 원래의 최적화 식 또한 변경할 필요가 있습니다. 원래의 최적화 식에 $\xi_n$의 총 합과 그 가중치를 나타내는 상수 $C$를 곱한 값을 빼주어야 합니다. Augment Error의 개념과 비슷하다고 생각하시면 됩니다. 최적화 식의 조건은 이전 슬라이드에서 변경했던 식이 그대로 들어왔습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최적화 식이 바뀌었으니, 그것을 계산하는 라그랑지안 또한 바뀔 것임을 쉽게 알 수 있습니다. 라그랑지안에서는 최적화 식과 조건을 하나의 식으로 만들어주므로 대부분의 항은 크게 문제 될 것이 없습니다. 마지막 항을 보시면 $\beta_n \xi_n$의 합을 빼는 항이 추가된 것이 보이는데, 이것은 $\xi_n$이 0보다 크다는 조건을 라그랑지안으로 표현한 것입니다.&lt;/p&gt;

&lt;p&gt;라그랑지안에서 변수 $\xi_n$가 추가되었으니 라그랑지안을 $\xi_n$에 대해 편미분한 식을 구해야 합니다. 라그랑지안이 복잡해 보이지만, 자세히 보시면 라그랑지안이 $\xi_n$에 대해 죄다 1차식으로만 이루어졌다는 것을 알 수 있습니다. 따라서 남는 계수는 $C$와 $\alpha_n$, 그리고 $\beta_n$ 뿐입니다. 식은 간단하지만 의외로 이 편미분의 결과가 의미하는 바는 큰데, $\alpha_n$이 $C$보다 절때 클 수 없다는 것입니다. 만약에 $\alpha_n$가 $C$ 보다 크다면 편미분 식을 만족하는 $\beta_n$값이 없어지기 때문입니다. 게다가 이제 $\beta_n$을 $\alpha_n$과 $C$로 표현할 수 있으니, 추가된 새로운 변수인 $\beta_n$을 소거할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 라그랑지안을 살펴보면 당연히 $\beta_n$는 이제 없어지고, $\alpha_n$이 $C$보다 크지 않다는 조건이 추가되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 Support Vector의 종류를 살펴보도록 하겠습니다. 이제 이전과 달리 모든 Support Vector가 Magin을 나타내는 것이 아닙니다. 변경된 Support Vector를 구하는 식에서 $\xi_n$가 0인 경우에는 이전과 마찬가지로 Margin의 경계선에 위치하는 Support Vector가 나오지만, 그렇지 않은 경우에는 Support Vector가 전혀 엉뚱한 위치에 있을 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/15. Kernel Methods/ML 15-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 두 가지 기술적 관점을 살펴보겠습니다. 원래의 Support Vector Machine은 데이터가 선형으로 분리가 가능한 경우를 가정해서 Hard Margin을 계산하는 것이었습니다. 그러나 데이터가 선형 분리가 되지 않을 때는 다른 방법을 찾아야 하는데, 쉬운 방법으로는 방금과 같이 Soft Margin을 구하는 것입니다. 이때 변수가 $\alpha$ 하나에서 $\beta$가 추가되었기 때문에 Dual Problem으로 바뀌게 됩니다.&lt;/p&gt;

&lt;p&gt;두 번째로는 $\mathcal{Z}$ 공간에서의 문제입니다. 기존에 Threshold를 의미하는 $w_0$이 있었다는 것을 기억하실 겁니다. 그런데 SVM에서는 이에 대한 언급이 없고, 이와 비슷한 기능을 하는 $b$가 있었습니다. 같은 역할을 하는 서로 다른 변수가 있기 때문에 개념이 꼬일 수 있지만, 결과적으로 이 둘이 전부 0으로 수렴하기 때문에 실제 계산에서는 신경 쓸 필요가 없습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Support Vector Machines</title><link href="http://localhost:4000/studies/support-vector-machines/" rel="alternate" type="text/html" title="Support Vector Machines" /><published>2019-10-19T00:00:00+09:00</published><updated>2019-10-19T00:00:00+09:00</updated><id>http://localhost:4000/studies/support-vector-machines</id><content type="html" xml:base="http://localhost:4000/studies/support-vector-machines/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;14장은 새로운 기계학습의 방법인 Support Vector Machine을 배우게 됩니다. 지금처럼 Neural Network가 유명해지기 전까지는 가장 널리 쓰이던 방법이었습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 Classification 문제에서 Margin을 최대화하는 것부터 시작해서, 그것을 찾는 해법, 그리고 Nonlinear Transform에 응용하는 방법까지를 다루게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;maximizing-the-margin&quot;&gt;Maximizing the margin&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저, 선형 분리가 가능한 데이터 집합이 있다고 가정해봅시다. 간단하게 표현하기 위해, 상단의 그림처럼 4개의 데이터만 존재하고, 빨간색과 파란색으로 분류하는 문제를 예로 들어보겠습니다. 이 문제를 해결하기 위해 두 데이터 종류를 분리하는 선을 그어야 하는데, 빨간색과 파란색을 분리하기만 하면 되므로 해답은 무수히 많습니다. 슬라이드에는 그 중 3가지의 해답을 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;위의 그림에 나온 분리선을 위 아래로 평행이동시킨다고 가정해봅시다. 어느 방향으로 움직이든 처음으로 데이터를 만나는 곳까지를 Margin으로 정의합니다. Margin의 의미는 데이터 분리에 오류가 발생하기 전까지 움직일 수 있는 영역을 의미합니다. 이를 3개의 그림에서 노란색 영역으로 표시한 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;세 그림에서 분리선은 모두 주어진 데이터를 올바르게 나누고 있습니다. 하지만 만약에 이 중 하나를 고르라고 하면 많은 분들이 3번째 그림을 고를 것입니다. 왜냐하면 3번째 그림의 Margin이 가장 크다는 것을 알기 때문입니다. 여기서 두 가지 의문이 생깁니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;왜 Margin이 클 수록 좋은가?&lt;/li&gt;
  &lt;li&gt;Margin을 크게 만들기 위해서 $\mathbf{w}$를 어떻게 설정해야 하는가?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;첫 번째 의문에 대한 답은 Margin이 크다면 데이터에 Noise가 있는 경우 Error를 일으킬 확률이 줄어들기 때문입니다. 첫 번째와 같이 나누게 되면, 빨간 점의 노이즈로 인해 분리선을 넘는 데이터가 발생할 확률이 상당히 높게 발생할 수 있습니다. 그에 반해 세 번째 그림의 분리선은, 데이터와 멀리 떨어져 있기 때문에 그럴 위험이 상대적으로 적음을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;잠시 5장에서 배운 Growth Function을 떠올려봅시다. 만약에 데이터가 3개이고, 선으로 데이터를 나누는 예제가 있다고 가정합니다. 이 예제에서는 3개의 점으로 가능한 모든 경우인 $2^3=8$개의 경우의 수가 나왔습니다. 이 말은 Growth Function이 크다는 것이고, Growth Function이 크다는 것은 일반화가 쉽지 않다는 말과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반화 문제를 해결하기 위해서, 먼저 똑같은 예제에서 8개의 경우를 각각 Margin이 가장 큰 방법으로 나누어보겠습니다. 데이터가 무작위로 분포해있기 때문에, 그림을 보시면 아시겠지만 나누었을 때 Margin이 큰 경우도 있고, 작은 경우도 있습니다. 만약에 우리가 Margin이 &lt;strong&gt;Fat&lt;/strong&gt;한 경우만 허용한다면, 위의 3번째 그림과 같은 경우는 사용할 수 없으므로, 경우의 수가 줄어들게 됩니다. 즉, &lt;strong&gt;Fat Margin은 Dichotomy의 수를 줄인다&lt;/strong&gt;라는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이제는 선형 분류에서, 두 종류의 데이터를 나누는 작업에 Margin이 커야 한다는 조건이 추가된 것입니다. 그렇게 만들기 위해 $\mathbf{w}$를 어떻게 찾아야하는지를 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;Margin은 단순하게 나누는 평면에서 데이터 점까지의 거리입니다. 그 거리를 계산하기 위해, 먼저 데이터를 나누는 평면 $\mathbf{w}^{\sf T} \mathbf{x} = 0$ 과 가장 가까운 점 $\mathbf{x}_n$를 가정합니다. 데이터를 나누는 것을 &lt;strong&gt;선&lt;/strong&gt;이 아니라 &lt;strong&gt;평면&lt;/strong&gt;이라고 지칭하는 이유는 2차원이 아닌 일반적인 차원을 기준으로 계산해야하기 때문입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 이제는 평면과 점 사이의 거리를 계산해야 합니다. 그 전에 먼저 알아야 할 것은 첫째로 $\mathbf{w}$를 정규화하는 것입니다.&lt;/p&gt;

&lt;p&gt;둘째로 $w_0$를 식에서 따로 분리하는 것입니다. 기존의 선형 분류에서 $w_0$는 Threshold를 위해 만든 가중치였습니다. 그렇기 때문에 $w_0$의 역할은 $w_1 ~ w_d$까지의 역할과 완전히 다른 역할을 하므로 $\mathbf{w}$에서 따로 분리하는 것이 추후 계산에서 좀 더 편합니다. 이제 $\mathbf{w}$는 $w_1 ~ w_d$으로만 이루어진 벡터라고 가정하고, $w_0$는 $b$로 바꾸어서 더해줄 것입니다. 즉, 이제 평면의 방정식은 $\mathbf{w}^{\sf T} \mathbf{w} + b = 0$으로 바뀐 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 점 $\mathbf{x}_n$과 평면 $\mathbf{w}^{\sf T} \mathbf{x} + b = 0$ 사이의 거리를 구하는 문제를 해결해야 합니다. $w_0$이 $b$로 바뀌고 $\mathbf{w}$ 밖으로 나왔기 때문에, 1로 정규화시킨 식 또한 약간 바뀐 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이해를 돕기 위해 위 슬라이드의 오른쪽 그림을 참고하면서 계산해봅시다. 고등학교 때 배운 평면의 방정식을 떠올려보면, $\mathbf{w}$는 평면의 법선벡터이기 때문에 평면과 직교하게 됩니다. 이를 보이려면 평면의 임의의 점 $\mathbf{x}^{\prime}$과 $\mathbf{x}^{\prime\prime}$을 잡습니다. 두 점은 평면 위의 점이기 때문에, 평면의 방정식에 대입해도 식이 변하지 않습니다. 식 $\mathbf{w}^{\sf T} \mathbf{x}^{\prime} + b = 0$에서 식 $\mathbf{w}^{\sf T} \mathbf{x}^{\prime} + b = 0$를 뺀다면 $\mathbf{w}^{\sf T} (\mathbf{x}^{\prime} - \mathbf{x}^{\prime\prime})$ 이 됩니다. $\mathbf{w}$와 $(\mathbf{x}^{\prime} - \mathbf{x}^{\prime\prime})$ 두 벡터의 내적이 0이 나오므로, 두 벡터는 직교하는 것이고 $\mathbf{x}^{\prime}$과 $\mathbf{x}^{\prime\prime}$는 평면 위의 임의의 점이므로, 평면과 $\mathbf{w}$는 직교한다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 평면 밖의 점 $\mathbf{x}_n$과 평면 사이의 거리를 계산해봅시다. 평면위에 임의의 점 $\mathbf{x}$를 잡고, 벡터 $\mathbf{x}_n - \mathbf{x}$를 $\mathbf{w}$에 사영(Projection)합니다. 이 길이를 구하기 위해, 먼저 $\mathbf{w}$의 단위벡터를 $\hat{\mathbf{w}}$로 정의합니다. 그렇다면 구하려는 거리는 $\hat{\mathbf{w}}^{\sf T} (\mathbf{x}_n - \mathbf{x})$ 벡터의 크기가 됩니다.&lt;/p&gt;

&lt;p&gt;이 식에서 단위벡터를 풀고 정리하면, $b$를 한번 더해주고 빼주는 테크닉을 통해 평면의 방정식과 비슷한 형태로 정리할 수 있습니다. 여기서 $\mathbf{w}^{\sf T} \mathbf{x} + b$는 평면의 방정식 정의로 인해 0이고, $\mathbf{w}^{\sf T} \mathbf{x}_n + b$는 1로 정규화를 시켰었기 때문에, 결국 구하려는 거리를 표현한 식은 $\frac{1}{\lVert \mathbf{w} \rVert}$만 남게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국 최적화해야하는 문제는 $\frac{1}{\lVert \mathbf{w} \rVert}$를 최대화하는 것입니다. Margin을 최대화하기 위해서는 분리 평면과 데이터 점 사이의 거리를 멀게 만들어야 하기 때문입니다. 여기에 조건으로, 분리 평면과 가장 가까운 점에서 $\mathbf{w}$를 $1$로 정규화하는 것을 추가해야 합니다.&lt;/p&gt;

&lt;p&gt;그러나 조건에 Minimize가 들어있으면 최적화 문제를 풀기 어렵기 때문에, 문제를 조금 변형시킬 필요가 있습니다. $\lvert \mathbf{w}^{\sf T} \mathbf{x}_n + b\rvert$는 $y_n (\mathbf{w}^{\sf T} \mathbf{x}_n + b)$로 대체할 수 있습니다. 왜냐하면, 이 문제에서는 선형 분리가 가능한 데이터 집합을 가정했기 때문에, 모든 데이터가 올바르게 분류되는 상황만을 따지기 때문입니다. 만약에 데이터가 $+1$로 분류된다면 $y_n = 1, -1$로 분류된다면 $y_n = -1$이 되므로 절대값과 동일한 기능을 가짐을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 목적 함수도 분모에 Euclide Norm이 불편하기 때문에 식을 Maximize로 수정해주고, $\mathbf{w}$에 대한 2차식으로 바꾸어줍니다. (이 부분은 아직 이해가 덜 되어 추후 그 이유를 추가하겠습니다)&lt;/p&gt;

&lt;p&gt;최종적으로 바꾼 식을 보시면 조건 부분이 $1$과 같은 것이 아니라 $1$ 이상으로 바뀌었습니다. 만약에 모든 조건식이 $1$보다 크다면, 단순히 $\mathbf{w}$와 $b$를 조절함으로써 제일 $1$에 가까운 식을 $1$로 맞춰줄 수 있기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-solution&quot;&gt;The solution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이전 슬라이드에서 유도한 최적화 식을 해결하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최적화 문제에서 많이 사용되는 라그랑주를 사용하고자 합니다. 그런데 조건에 부등식이 들어가 있습니다. 이렇게 조건에 부등식이 들어간 라그랑지안을 &lt;span style=&quot;color:red&quot;&gt;KKT (Karush Kuhn Tucker)&lt;/span&gt; 라고 합니다. KKT는 대학원 수업에서 다루는 Convex Optimization에 나오는 내용인데, 이 강의에서 구체적으로 KKT를 풀지는 않으니 일단 이런 것이 있다고만 생각하고 넘어가도록 합시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 쉽게 풀기 위해 이전에 정규화에서 배웠던 방법을 생각해봅시다. 왜냐하면 이 때도 조건에 부등식이 들어간 목적 함수를 최소화하는 문제를 풀었기 때문입니다. 단지 차이는 목적 함수가 $E_{in}$이 $\mathbf{w}^{\sf T} \mathbf{w}$으로 바뀌고, 조건이 $\mathbf{w}^{\sf T} \mathbf{w}$에서 $E_{in}$으로 바뀌었을 뿐입니다. 갑자기 SVM의 조건에 $E_{in}$을 언급하는 이유는, 모든 데이터가 올바르게 분류된다고 가정했기 때문에 $E_{in}$이 0이여만 하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;라그랑주 승수법(Lagrange Multiplier)에 의해, 조건으로 붙은 $y_n(\mathbf{w}^{\sf T} + b) - 1$은 목적 함수로 들어갑니다. 이 라그랑지안을 $\mathbf{w}$와 $b$에 대해 풀어야 합니다.&lt;/p&gt;

&lt;p&gt;그 후 주어진 라그랑지안을 최소화하는 해법을 찾기 위해, $\mathbf{w}$에 대해 기울기(Gradient)가 0이 되는 지점을 구해야 합니다. 다행히도 라그랑지안의 첫번째 항은 $\mathbf{w}$에 대한 2차식, 두번째 항은 $\mathbf{w}$에 대한 1차식이기 때문에 기울기는 간단하게 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;마찬가지로 라그랑지안을 $b$로 편미분을 하게 되면 첫번째 항은 $b$가 포함되지 않았으므로 0, 두번째 항은 $b$에 대한 1차식이므로 역시 간단하게 구할 수 있습니다. 이 편미분 식 역시 0이 되는 지점을 찾아야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전에 구한 식들을 정리해보겠습니다. 정리 자체는 매우 간단합니다. 기존의 라그랑지안 식에서, 이전에 구한 $\mathbf{w}$와 $\sum_{n=1}^{N} \alpha_n y_n$만 대입하면 라그랑지안 식에서 $\mathbf{w}$와 $b$는 모두 사라지게 되고, 라그랑지안 식은 $\alpha$에 관한 식으로 변하게 됩니다.&lt;/p&gt;

&lt;p&gt;(추후에 자세한 정리 과정을 여기에 적겠습니다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국 최종적으로 풀어야 하는 식은 Quadratic Programming 문제가 됩니다. 여기서 직접 QP 문제를 풀 필요는 없습니다. 왜냐하면 MATLAB, Python 등의 라이브러리에서 QP를 풀어주는 기능이 이미 존재하기 때문입니다. 지금까지 유도한 QP 문제만 넣어주면 알아서 답이 나오기 때문에, 여기에서는 QP 문제 해결 방법은 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;다만 주어진 문제는 Maximize를 해야 하는데, QP 문제는 일반적으로 Minimize를 시키게 되어있습니다. 따라서 식의 부호를 반대로 하고 Maximize를 Minimize하는 문제로 바꾸겠습니다.&lt;/p&gt;

&lt;p&gt;그리고 앞의 식에서 $\alpha$와 $\mathbf{x}$, $y$를 구분해주기 위해 식을 변형시켜 $\alpha$를 앞으로 따로 빼고 나머지는 Matrix로 표현했습니다.&lt;/p&gt;

&lt;p&gt;식 자체가 매우 복잡해보이지만, 단지 프로그램에 넣기 편하게 만들기 위해 식을 풀어쓴 것입니다. 간단하게는 아래처럼 표현할 수 있습니다.&lt;/p&gt;

\[\min \frac{1}{2} \alpha^{\sf T} \mathbf{Q} \alpha - \mathbf{1}^{\sf T} \alpha\]

\[\text{subject to }\mathbf{y}^{\sf T} \alpha = 0; \alpha \ge 0\]

&lt;p&gt;QP 문제를 제시하는 것까지는 성공적이었으나, QP 문제를 컴퓨터가 풀어준다고 해도 여전히 문제점이 있습니다. 특히, Quadratic Coefficients로 표현된 Matrix는 $N \times N$ 크기이기 때문에 데이터가 많이 주어질수록 푸는데 시간이 오래 걸린다는 문제가 있습니다. 그렇기에 현재 이런 문제를 쉽게 풀기 위한 경험적인 방법이 연구되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;계산상의 문제는 별개의 문제이기 때문에 여기서는 일단 성공적으로 $\alpha$를 구했다고 가정해봅시다. 그 다음에는 당연히 $\mathbf{w}$를 구해야하는데, 이것은 굉장히 쉽습니다. $\mathbf{x}$와 $y$는 주어진 데이터이기 때문에 당연히 알고있는 것이고, 여기에 방금 구한 $\alpha$만 대입한다면 자연스레 $\mathbf{w}$가 계산되기 때문입니다.&lt;/p&gt;

&lt;p&gt;둘째로 볼 것은 KKT 조건에서 Slack이라고 불렸던 식입니다. 이 식이 0이 되므로 $\alpha$ 또는 $(y_n ( \mathbf{w}^{\sf T} \mathbf{x}_n + b) - 1 )$이 0이 되어야 합니다. 이것은 이전에 정규화에서 배웠던 것과 비슷하게 볼 수 있습니다. $\alpha$가 0이라는 것은 라그랑지안이 0이 되었다는 뜻이므로 데이터 점이 아예 Margin 바깥에 있음을 의미합니다. 즉, 이것은 신경을 쓸 필요가 없습니다.&lt;/p&gt;

&lt;p&gt;중요한 점은 $(y_n ( \mathbf{w}^{\sf T} \mathbf{x}_n + b) - 1 )$이 0이 되는 지점입니다. 이 말은 데이터 점이 정확하게 Margin의 경계선에 있다는 것을 의미하기 때문입니다. 이 때의 점 $\mathbf{x}_n$은 Margin의 범위를 &lt;strong&gt;도와&lt;/strong&gt;주기 때문에 Support Vector라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Support Vector를 직관적으로 이해하려면, 위 슬라이드의 오른쪽 그림을 참고하시면 됩니다. 보라색 선은 데이터 집합을 구분하는 분리 평면이 되고, 노란색 영역은 Margin이 됩니다. 노란색 영역의 경계선을 보시면 경계에 걸쳐있는 점이 표시되어 있는데, 이 점들이 바로 Support Vector입니다. 이전 슬라이드에서 $(y_n ( \mathbf{w}^{\sf T} \mathbf{x}_n + b) - 1 )$이 0이 되는 점이었습니다. 그리고 바로 이전 슬라이드에서 $\alpha$가 0이 되는 점들이 Margin 밖에 있는 데이터입니다.&lt;/p&gt;

&lt;p&gt;지금까지 $\alpha$와 $\mathbf{w}$만 구했지, $b$를 아직 구하지 않았습니다. $b$를 구하는 방법도 간단한데, Support Vector가 아닌 데이터에서는 $\alpha$가 0이 되므로 $b$를 구하는데 쓸 수 없습니다. 그렇기에 Support Vector를 먼저 구한 다음, 아무 Support Vector 데이터를 가져오면 $y_n ( \mathbf{w}^{\sf T} \mathbf{x}_n + b) = 1$이란 간단한 식이 나오므로, $\mathbf{w}$만 대입하면 $b$를 구할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-transforms&quot;&gt;Nonlinear transforms&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 선형 분리가 가능한 경우를 가정했지만, 이제 Support Vector를 Nonlinear Transform에 응용해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적이 Nonlinear Transform은 기존의 데이터 $\mathbf{x}$를 Transform Function을 통해 $\mathbf{z}$으로 바꾸어줬습니다. $\mathbf{x}$는 선형 분리가 불가능한 데이터이지만, Transform을 통해 선형 분리가 가능한 $\mathbf{z}$로 바꾼 것입니다.&lt;/p&gt;

&lt;p&gt;데이터를 Transform 해도 라그랑지안은 그렇게 크게 바뀌지 않다는 걸 눈치채셨을 겁니다. $\alpha$를 푸는 QP 문제는 어차피 데이터의 “수”에만 영향을 받기 때문입니다. 즉, 데이터를 Transform 하는 것은 Support Vector를 구하는 것을 어렵게 만들지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/14. Support Vector Machines/ML 14-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\mathcal{Z}$ 공간에서 $\alpha$와 Support Vector를 구한 다음 데이터 집합을 다시 $\mathcal{X}$ 공간으로 Transform하게 되면 위 그림과 같은 분리 평면이 나오게 됩니다. $\mathcal{Z}$ 공간에서는 분리 평면이 직선과 같은 형태지만, $\mathcal{X}$ 공간에서는 이렇게 곡선 형태로 나오게 됩니다. Support Vector는 그림상에 특별하게 표시가 되어있습니다.&lt;/p&gt;

&lt;p&gt;분리 평면을 언뜻 보면 Overfitting을 일으킬 것처럼 생겼습니다. 하지만 이 문제에서는 Support Vector가 4개로 나오는데, 이 말은 $\mathcal{Z}$ 공간에서 $\mathbf{w}$는 4개의 Parameter만 있다는 뜻입니다. 그렇기에 Support Vector는 일반화에 매우 적합한 특징을 갖고 있다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;Support Vector에서의 일반화를 명확하게 정리하면, 평균적인 Out of Sample Error는 Support Vector의 수의 평균을 $N-1$로 나눈 것보다 작습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Validation</title><link href="http://localhost:4000/studies/validation/" rel="alternate" type="text/html" title="Validation" /><published>2019-10-12T00:00:00+09:00</published><updated>2019-10-12T00:00:00+09:00</updated><id>http://localhost:4000/studies/validation</id><content type="html" xml:base="http://localhost:4000/studies/validation/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;13장에서는 Overfitting을 해결하는 방법 중 Validation (검증)이라는 방법을 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 Validation Set (검증 집합), Model Selection (모델 선택), Cross Validation (교차 검증) 순서대로 배우게 됩니다. Validation이 무엇인지 알지 못하는 지금은 소제목을 봐도 이해가 쉽지 않으니 일단 넘어가도록 합시다.&lt;/p&gt;

&lt;h2 id=&quot;the-validation-set&quot;&gt;The validation set&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적으로 Out of Sample Error는 In Sample Error에 Overfit penalty $\Omega$를 더한 식으로 표현되었습니다. 이 Overfit penalty를 없애기 위해 Regularization (정규화)에서는 이 penalty의 수치를 추정하는 것을 목표로 하였고, 이를 Augmented Error로 호칭하였습니다.&lt;/p&gt;

&lt;p&gt;이것과 다르게 &lt;span style=&quot;color:red&quot;&gt;Validation (검증)&lt;/span&gt;은, Out of Sample Error를 추정하고 오류를 최소화합니다. 이것을 측정하는 방법은 Test Set과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Out of Sample Data $(\mathbf{x}, y)$에 대해, Error는 $\mathbf{e}(h(\mathbf{x}, y))$로 정의하는 것을 이미 이전에 배웠습니다. 그 Error을 측정하는 대표적인 두 가지 방법인 Squared Error와 Binary Error도 이미 배웠습니다.&lt;/p&gt;

&lt;p&gt;검증에서 하려고 하는 것은 이 Error의 추정치이기 때문에, 기대값인 $\mathbb{E}[\mathbf{e}(h(\mathbf{x}, y))]$라 표현할 수 있습니다. 이것은 Bias가 없는 Out of Sample Error 입니다. Variance는 따로 표현해주어야 하기 때문에 Error의 Variance를 $\sigma^2$로 정의하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Out of Sample Error를 추정하는데에는 Out of Sample과 별개의 데이터인 Validation Set을 사용합니다. Validation Set은 $K$개의 데이터를 보유하고 있다고 가정합니다. 이 때의 Error 측정은 In Sample Error를 계산하는 방법과 동일하지만, 호칭을 구분하기 위하여 $E_{val}$ (Validation Error)로 표현합니다. 기존의 방법과의 차이점은 Sample Data와는 별개의 데이터로 수행된다는 점입니다.&lt;/p&gt;

&lt;p&gt;Validation Error의 평균을 계산하기 위해서는 간단하게 $E_{val}$에 기대값만 취해주면 되고, $\mathbb{E}$는 시그마 합 안쪽으로 넣을 수 있습니다. 그러고 나면 이것은 바로 이전 슬라이드에서 했던 Out of Sample Error와 동일함을 알 수 있습니다. 즉, Validation Set만으로 Out of Sample Error를 측정할 수 있다는 것을 보인 것입니다.&lt;/p&gt;

&lt;p&gt;이번엔 Validation Error의 Variation을 계산해보겠습니다. Variance 역시 Validation Error를 통해 계산하는데, 이때 Variance와 Co-Variance이 계산과정에서 같이 나옵니다. 하지만 운이 좋게도 데이터를 독립적으로 뽑았기 때문에, Co-Variance는 모두 0이 되고, Variance만 남게 됩니다. 분모는 이 과정에서 $K^2$으로 바뀌게 됩니다. 결과적으로, Validation Error의 Variation은 Out of Sample Error를 Validation Set의 원소의 수로 나눈 것이 됩니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Validation Error와 Out of Sample Error 사이의 관계를 정리하면, Validation Error는 Out of Sample Error에 $\frac{1}{\sqrt{K}}$차수의 식을 더하거나 뺀 수치가 됩니다. $K$가 크면 클 수록 이 부분은 0에 가까워지므로, Validation Set이 클수록 Out of Sample Error를 더 정확하게 추정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 Validation Set을 늘리는 데에는 한계가 있습니다. 기본적으로 Validation Set은 Traning에 사용되는 Data Set인 $\mathcal{D}$에서 일부를 가져와야하기 때문입니다. 전체의 $N$개의 데이터에서 $K$개 만큼의 데이터를 Validation을 위해 사용하게 되면 Training에 사용되는 데이터의 수는 $N-K$개가 됩니다.&lt;/p&gt;

&lt;p&gt;만약에 $K$가 작다면, $O( \cdot )$이 커지기 때문에 Validation Error와 Out Of Sample Error의 차이가 커지므로 제대로 된 측정을 할 수 없는 문제가 생깁니다. 반대로 $K$가 너무 크다면 Traning의 데이터 수가 적으므로 오른쪽 그림처럼 In Sample Error와 Out of Sample의 차이가 커지는 문제가 생기는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이렇게 생각해볼 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Validation에 사용한 $K$개의 데이터를 Training에 사용하면 되지 않을까?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 질문에 답하기 위하여, 처음부터 차근차근 정리해봅시다. 먼저 주어진 전체 데이터 집합은 $\mathcal{D}$입니다. $\mathcal{D}$는 Training set $\mathcal{D}_{train}$과 Validation set $\mathcal{D}_{val}$의 합집합니다. $N$개의 데이터 중 $N-K$개의 데이터로 학습하고, $K$개의 데이터로 Validation을 하였습니다.&lt;/p&gt;

&lt;p&gt;전체 데이터 집합 $\mathcal{D}$를 사용하여 Training 하게 되면, 최종 가설인 $g$를 얻습니다. 그런데 $\mathcal{D}_{train}$을 사용해서 Training 하게 되면, 최종 가설로 $g$가 아니라 $g^{-}$를 얻게 됩니다.&lt;/p&gt;

&lt;p&gt;정리하자면, Validation Set을 포함하여 Training을 하게 된다면, 최종 가설은 $g$인데 Validation Error의 결과는 $g^{-}$에 대해 나오게 됩니다. 만약 $K$가 작다면 $g$에 대한 Validation Error나 $g^{-}$에 대한 Validation Error나 비슷하기 때문에 큰 문제가 되지 않습니다. $K$가 크다면 $g$와 $g^{-}$ 사이의 차이가 더욱 커지기 때문에 Validation Error 자체의 의미가 사라지게 됩니다.&lt;/p&gt;

&lt;p&gt;결론적으로 Validation Set 크기만 잘 조정한다면 Validation에 사용한 데이터를 Training에 사용해도 문제가 없습니다. 강의에서는 대략적으로 전체 데이터의 20%를 Validation Set으로 사용하는 것이 &lt;strong&gt;Rule of Thumb (경험적 법칙)&lt;/strong&gt;이라고 제시하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가만히 생각해보면 Validation에서 사용한 방법은 Test를 할 때 쓰는 방법과 차이가 없습니다. 그렇다면 왜 이것을 Validation 이라고 부르는 것일까요? 강의에서는 이렇게 말하고 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“We call it validation, because we use it to make choices.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;선택하기 위해 사용하므로 Validation이라고 부른다고 합니다. 무엇을 선택하는 걸까요? 오른쪽 그림을 보시면 답이 나옵니다. 이 그림은 11장에서 Overfitting을 배울 때 나왔습니다. 이 그림에서 가장 좋은 성능을 보이는 지점은 Early Stopping 부분입니다. 하지만 이 당시에는 Out of Sample Error를 예측할 수 없었기 때문에 Early Stopping 지점에 도달하더라도, 이 다음에 Out of Sample Error가 증가할지, 감소할지를 알 수 없었기 때문에 계속 Training을 진행하였습니다. 결과적으로 이것이 Overfitting을 일으키는 요소였던 것입니다. Validation을 함으로써 Out of Sample Error를 예측할 수 있으니, Early Stopping 부분에 도달한다면 이 지점을 기준으로 Out of Sample Error가 증가할 것이라는 것을 알고 멈출 수 있습니다. 따라서 이것이 Validation이라고 부르는 이유입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Test Set과 Validation Set의 차이점을 알아보겠습니다. Test Set은 Bias가 없지만 Validation Set은 Optimistic Bias를 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Optimistic&lt;/strong&gt;의 의미를 이해하기 위해 간단한 예를 들어보겠습니다. Out of Sample Error가 0.5로 같은 2개의 Hypothesis $h_1$과 $h_2$를 가정해봅시다. 두 개의 Hypothesis에서 각각 Error를 예측하는데, $h_1$의 Error 추정값을 $\mathbf{e}_1$, $h_2$의 Error 추정값을 $\mathbf{e}_2$라 정의합니다. 단순하게 하기 위해, 두 Error 추정이 모두 0과 1 사이에서 균일(Uniform)하다고 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;여기서 $\mathbf{e}_1$과 $\mathbf{e}_2$ 모두 Bias가 없는 Out of Sample Error의 추정입니다. 여기서, 두 개의 가설 $h_1$과 $h_2$ 중 하나를 고르는 것을 $h$라 부르겠습니다. 고르는 방법은 $\mathbf{e}_1$과 $\mathbf{e}_2$ 중 작은 것입니다. 그렇다면 $\mathbf{e}$의 기대값은 얼마일까요?&lt;/p&gt;

&lt;p&gt;$\mathbf{e}_1$과 $\mathbf{e}_2$ 모두 기대값이 0.5이므로 $\mathbf{e}$ 또한 0.5라고 단순하게 생각할 수도 있으나, 실제로는 0.5보다 작을 수밖에 없습니다. 왜냐하면 2개의 확률 변수 중 작은 것을 고르는데, 두 확률 변수의 평균이 0.5 이므로, 항상 작은 것만 고르게 된다면 확률 분포가 0과 1 사이에서 균일하게 있다는 가정 하에서 0.5보다 작은 것을 뽑을 확률이 75%나 되기 때문입니다. 따라서 대부분의 경우 $h$의 Error가 0.5보다 낮기 때문에 Optimistic Bias라고 부르는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;model-selection&quot;&gt;Model selection&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로, Validation의 모델을 선택하는 방법에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 M개의 모델 $\mathcal{H}_1, …, \mathcal{H}_M$이 존재한다고 가정해봅시다. Validation Set을 제외한 데이터 집합인 $\mathcal{D}_{train}$을 사용하여 학습하면 최종 가설로 $g_m^{-}$이 각 모델마다 생성됩니다. 마찬가지로 $\mathcal{D}_{val}$ 집합으로 Validation Error를 각 모델별로 측정한 결과도 $E_1, \ldots, E_M$로 나오게 됩니다. 이 중에 가장 좋은 결과를 가진 Validation Error을 골라 $E_{m^*}$라고 하는데, 이전에 설명한대로 그 과정에서 Optimistic Bias가 포함되어 있음을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;최선의 가설 $\mathcal{H}_{m^*}$을 찾는 것도 이와 크게 다르지 않습니다. 이 때의 학습은 $\mathcal{D}_{train}$이 아니라 $\mathcal{D}$를 사용합니다. 이렇게 고른 $E_{m^*}$과 $\mathcal{H}_{m^*}$이 최종 가설 $g_{m^*}$이 되는 것입니다. 오른쪽의 그림에서 이를 잘 표현하고 있으니 참고하시면 이해가 쉬울 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 Bias에 대해 조금 더 자세하게 다뤄보겠습니다. 모델 선택은 이전 슬라이드에서 언급했듯이 $\mathcal{D}$가 아닌 $\mathcal{D}_{val}$을 기준으로 합니다. 그렇기에 $g_{m^*}$에 대한 Validation Error는 Out of Sample Error로부터 Bias됩니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림은 Validation Set Size에 따른 Validation Error와 Out of Sample Error의 변화를 나타내고 있습니다. Validation Set Size가 커질수록 학습에 사용할 데이터의 수가 적어지므로 두 Error가 커지는 것은 이전에 배웠습니다. 하지만 여기서 주목할 것은 Validation Set Size이 커질수록 두 Error의 그래프가 가까워진다는 것입니다. 왜 그런지는 다음 슬라이드에서 그 이유를 보여드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;주어진 데이터 중 Validation Set을 제외한 데이터 집합 $\mathcal{D}_{train}$을 사용하여 $M$개의 모델 $\mathcal{H}_1, …, \mathcal{H}_M$로 최종 가설을 도출하면, 그 결과는 각각 $g_1^{-}, …, g_M^{-}$이 됩니다. 이것들을 원소로 하는 새로운 모델을 $\mathcal{H}_{val}$이라 부르겠습니다. 최종 가설은 이 모델의 원소 중 Validation Error가 가장 작은 것이 됩니다.&lt;/p&gt;

&lt;p&gt;이전에 Hoeffding Inequality와 VC Bound에서 배웠던 내용을 이용하여, Validation Error와 Out of Sample Error의 관계를 유도합니다. 여기서 Validation Error는 VC Bound에서 사용했던 In Sample Error의 역할을 하게 되고, 여러 모델의 합은 그냥 간단하게 Union Bound로 가정합니다.&lt;/p&gt;

&lt;p&gt;이 관계식을 유도하고 나면 $O( \cdot )$ 부분이 중요합니다. Validation Set Size $K$가 커질수록 $O( \cdot )$ 가 작아짐을 쉽게 알 수 있습니다. $O( \cdot )$가 매우 작다면 Out of Sample Error와 Validation Error가 거의 같아짐 또한 알 수 있습니다. 이로써 이전 슬라이드에서 Validation Set Size가 커질수록 두 그래프가 가까워지는 이유를 알 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 지금까지 발생했던 데이터 오염을 정리해보겠습니다. Out of Sample Error은 방금 전까지 다루었듯이 Optimistic Bias로 오염되었습니다. 이 외에 Error를 추정하는데 사용했던 3가지 집합은 어떤지 보겠습니다. 첫째로, In Sample Error를 측정하는데 사용했던 Training Set은 애초에 이것으로 학습을 했기 때문에 완전히 오염되었다고 판단합니다. 둘째로, Test Error를 측정하는데 사용했던 Test Set은 학습에 관여하지도 않고 어떠한 의사결정도 하지 않았기 때문에 완전히 깨끗한 집합니다. 마지막으로 Validation Error를 측정하는데 사용했던 Validation Set은 Out of Sample Error를 추정하고 어디서 학습을 멈춰야 하는지 사용되었기 때문에 약간 오염되었다고 판단할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;cross-validation&quot;&gt;Cross validation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;span style=&quot;color:red&quot;&gt;Cross Validation (교차 검증)&lt;/span&gt;에 대해 배우겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Validation Set Size $K$의 딜레마를 다시 짚고 넘어가겠습니다. 일단 우리가 원하는 것은 Validation Error가 $\mathcal{D}_{train}$으로 학습한 최종 가설의 Out of Sample Error와 최대한 비슷한 것과 이것이 $\mathcal{D}$의 Out of Sample Error와 최대한 비슷한 것을 원합니다. 그런데 첫 번째로 원하는 것을 만족시키려면 $K$가 최대한 커야하고, 두 번째로 원하는 것을 만족시키려면 $K$가 최대한 작아야합니다.&lt;/p&gt;

&lt;p&gt;혹시 두 가지 요구사항을 모두 만족시키는 방법은 없을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 이를 해결하기 위해 Cross Validation의 한 방법인 &lt;strong&gt;Leave one out&lt;/strong&gt; 이라는 방법을 소개합니다. 약간의 수학적인 트릭을 이용한 것인데 방법을 보시면 굉장히 머리를 잘 썼다는 것을 느낄 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 방법은 Validation으로 단 1개의 데이터만 사용합니다. 나머지 $N-1$개의 데이터는 모두 Training에 사용합니다. 전체 데이터에서 1개만을 제외한 데이터이기 때문에 $g^{-}$는 $g$에 매우 가깝다는 것을 알 수 있습니다. 우선 여기서 딜레마의 두 번째 조건을 만족함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;편의상 제외한 데이터를 $(\mathbf{x}_n, y_n)$이라 부르고, 나머지 $N-1$개의 데이터를 사용하여 만든 최종 가설을 $g^{-}_n$이라 부르겠습니다. 그렇다면 Validation Error는 오직 이 1개의 점에서의 Error이므로, $\mathbf{e}(g^{-}_n(\mathbf{x}_n, y_n))$이 됨을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;Cross Validation은 이 과정을 모든 데이터에 대해 실시하는 것입니다. 그 후 이들의 평균을 구하면 Cross Validation이 완성된 것입니다. 결과적으로 모든 데이터를 Validation에 사용했기 때문에 Validation Set이 데이터 $N$개 전체가 되었습니다. 이로써 딜레마의 첫 번째 조건을 만족했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cross Validation을 이해하기 쉽게 하나의 예를 들어보겠습니다. 3개의 데이터가 존재하고, 선형 모델을 사용할 것입니다. 편의상 왼쪽부터 데이터를 한번씩 빼고, 나머지 2개로 만든 가설을 만듭니다. 그 후, 만든 가설과 제외한 데이터 사이의 Error를 측정합니다. 이렇게 나온 Error는 각각 $\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3$이 됩니다. Cross Validation Error는 이 3개에 대한 평균을 구하면 간단하게 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이제 Cross Validation을 사용하여 모델을 선택하는 것을 보겠습니다. 상수 모델을 하나 추가하여 이전 슬라이드에서 계산한 선형 모델과 비교해봅시다. Cross Validation Error의 정확한 값은 나와있지 않지만, 눈으로 대충 비교해봐도 상수 모델의 Cross Validation Error가 더 낮음을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좀 더 복잡한 예제를 보도록 합시다. 이 예제는 예전에 했던 손글씨를 분류하는 학습입니다. 원래의 문제를 조금 단순화 시켜 이 글씨가 1인지 1이 아닌지만을 판단하는 Classification입니다. 이 문제는 Nonlinear Transform을 사용하였습니다.&lt;/p&gt;

&lt;p&gt;이 경우에는 5차식을 사용하여 Transform을 하였는데, Transform Vector의 차원의 수는 총 20입니다. 오른쪽 그림은 Transform 차원을 1부터 20까지 사용하며 In Sample Error, Out of Sample Error, Cross Validation Error의 변화를 보여주고 있습니다. 어떤 Transform을 사용했는지에 따라 모델이 달라지므로, 총 20개의 모델이 있는 것과 같습니다. 우리는 가장 낮은 Cross Validation Error를 가진 모델을 선택해야 하는데, 그래프를 보면 5 또는 7 차원을 모델을 선택해야 함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제에서 Validation을 사용하지 않은 것과 사용한 것이 어떤 차이가 있는지 비교해 봅시다. 왼쪽은 Validation을 사용하지 않은 경우인데, Overfitting이 일어난 것을 쉽게 알 수 있습니다. In Sample Error는 0이지만, 주어진 데이터에 과도하게 맞추다보니 빨간색 영역 중간에 파란색 영역이 일부 존재함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;오른쪽은 Validation을 사용한 것인데, In Sample Error는 조금 증가했지만, Out of Sample Error가 사용하지 않은 것에 비해 확연히 떨어진 것을 알 수 있습니다. 퍼센트로는 단순히 1%의 차이지만, 실제 학습 성능은 40%나 향상된 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/13. Validation/ML 13-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다만 실제 학습에서 Leave one out을 사용하기는 조금 곤란합니다. 데이터의 수가 매우 많을 때는 Cross Validation을 계산하는 시간이 너무 많이 걸리기 때문입니다. 그 때문에 실제로는 1개의 데이터만을 빼기 보단 몇 개의 데이터를 그룹화합니다.&lt;/p&gt;

&lt;p&gt;많이 사용하는 방법 중에 &lt;span style=&quot;color:red&quot;&gt;10-fold Cross Validation&lt;/span&gt;이라는 것이 있습니다. 이 방법은 전체의 데이터를 10등분하여 Validation을 10번 계산하여 평균을 내는 방법입니다. 비록 Leave one out보다 $g$와 $g^{-}$의 Out of Sample Error 차이는 더 벌어지겠지만, 계산의 편의를 위해 이 방법이 더 권장된다고 합니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Regularization</title><link href="http://localhost:4000/studies/regularization/" rel="alternate" type="text/html" title="Regularization" /><published>2019-10-05T00:00:00+09:00</published><updated>2019-10-05T00:00:00+09:00</updated><id>http://localhost:4000/studies/regularization</id><content type="html" xml:base="http://localhost:4000/studies/regularization/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;12장은 지난 시간에 배웠던 문제점인 Overfitting을 해결하는 방법 중 Regularization (정규화)에 대해 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장에서는 먼저 직관적인 접근 방식으로 정규화가 무엇인지 알아보고, 그 후에 수학적인 방법으로 정규화가 정확하게 무엇인지 알아봅니다. 그 후 정규화에서 중요한 Weight Decay가 무엇인지 배운 다음, Regularizer를 선택하는 방법을 공부하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;regularization---informal&quot;&gt;Regularization - informal&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정규화에 접근하는 방식은 두 가지 방법이 있습니다. 먼저 수학적으로 문제점을 분석해 해결하는 방법을 유도하는 방법이 있고, 다른 방법으로는 단지 $E_{in}$을 최소화하는 것을 방해하는 요소가 무엇이었는지를 따져가며 해결하는 방법을 찾는 방법입니다. 여기서는 두 번째 방법을 먼저 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 8장에서 사용했던 예제입니다. Target Function이 $y=\sin(\pi x)$이고 가설 모델이 1차 함수인 상황에서 무작위로 2개의 데이터가 주어졌을 때 발생하는 가설을 그린 것입니다. 이 당시에는 Variance가 너무 컸기 때문에 상수 함수보다 나쁜 가설 모델임을 밝혔습니다. 그런데 오른쪽 그림처럼 이 가설 모델에 정규화를 도입하고 나니 가설들이 발생 분포가 조금 작아진 것처럼 보입니다. 여기서는 가설의 기울기에 제한을 두어 기울기가 너무 가파른 직선은 가설에서 제외하는 방법을 사용하였다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예상대로 정규화를 거친 경우 그렇지 않은 경우에 비해 성능이 좋아진 것을 볼 수 있습니다. 빨간색 선의 위치는 동일한 것처럼 보이지만, 실제로는 모델의 분포가 줄어들었기 때문에 살짝 틀어져 Bias는 약간 높게 나오지만, Target Function의 범위를 초과하는 영역은 모두 사라졌기 때문에 Variance는 급격하게 줄어들었습니다.&lt;/p&gt;

&lt;h2 id=&quot;regularization---formal&quot;&gt;Regularization - formal&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이런 정규화를 수학적으로 유도해볼 차례입니다. 이번에도 예를 들어 설명할텐데, 조금 복잡한 방법으로 Legendre Polynomials (르장드르 다항식)을 사용한 Transform을 예제로 다루게 됩니다.&lt;/p&gt;

&lt;p&gt;Legendre Polynomial은 Legendre Differential Equation (르장드르 미분 방정식)의 해가 되는 함수들을 일컫는데, 여기서는 구체적으로 Legendre Polynomial이 무엇인가까지 알 필요는 없습니다.&lt;/p&gt;

&lt;p&gt;다만 Legendre Polynomial은 최고차항이 1, 2, 3, … 일 때 각각 고유한 함수를 갖는데, 그 고유한 함수를 각각 $L_1, L_2, L_3, …$로 정의합니다. 위의 슬라이드에서는 $L_1$부터 $L_5$까지의 르장드르 다항식이 어떤 함수인지를 나타내고 있습니다.&lt;/p&gt;

&lt;p&gt;가설 모델 $\mathcal{H}_Q$는 최고차항이 $Q$인 다항식이라 가정합니다. 비선형 변환 $\mathbf{z}$는 1(=$L_0$)부터 $L_Q$까지를 원소로 갖는 벡터입니다. 이것을 사용하여 구체적인 $\mathcal{H}_Q$를 유도하면 각 Legendre Polynomial에 Weight $w_q$를 곱한 후 더한 형태로 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 주어진 데이터 $(x_1, y_1), … ,(x_N, y_n)$을 Transform을 통해 $(\mathbf{z}_1, y_1),$ $…, (\mathbf{z}_N, y_n)$를 만들면 $\mathbf{z}$에 대한 새로운 In Sample Error $E_{in}$을 유도할 수 있습니다. 이후의 전개는 3장에서 배운 Pseudo Inverse (유사 역행렬)을 구하시면 됩니다.&lt;/p&gt;

&lt;p&gt;즉, 임의의 일반적인 데이터 $\mathbf{x}$가 주어진다면, 이를 Legendre Polynomial을 벡터로 갖는 $\mathbf{z}$로 Transform을 하고 Pseudo Inverse를 통해 가중치 벡터 $\mathbf{w}$를 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 가중치 벡터가 제한된 상황에서는 어떻게 변하는지 알아봅시다. 지난 시간에 가설 모델 중 2차 방정식 모델 $\mathcal{H}_2$와 10차 방정식 모델 $\mathcal{H}_{10}$을 비교하였습니다. 그런데 생각해보면, $\mathcal{H}_2$는 $\mathcal{H}_{10}$의 제한된 버전, 즉 3차 이상의 항의 계수를 0으로 한 모델임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 것과 비슷하게 각 가중치를 제곱한 합을 일정 값 $C$ 이하로 제한하는 방법을 생각해볼 수 있습니다. 제곱이 되는 이유는 In Sample Error를 계산하는 식에 가중치 벡터 $\mathbf{w}$의 제곱이 들어가기 때문입니다. 이 제한이 들어갔을 때 나오는 해답을 이전의 해답과 비교하기 위해 $\mathbf{w}_{reg}$라고 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;제한 조건이 없는 In Sample Error $E_{in}$을 상수값과 같다고 식을 놓고 2차원으로 표현하면 오른쪽과 같은 파란색 타원으로 표현할 수 있습니다. 원의 중심으로 갈수록 $E_{in}$의 값이 낮아지기 때문에 제한조건이 없을 때의 해답 $\mathbf{w}_{lin}$은 원의 중심이 됩니다. 마찬가지로 제한 조건만을 2차원으로 표현한다면 빨간색 원으로 그릴 수 있습니다. 제한조건을 만족하면서 $E_{in}$을 최소화하는 점은 파란색 타원과 빨간색 원이 겹치는 영역에 포함됨을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;$E_{in}$을 최소로 만드는 점을 찾기 전에, 영역에 포함되는 점 중 하나인 $\mathbf{w}$를 하나 가정해봅시다. 이 점에서의 기울기(Gradient)를 구하게 되면 파란색 원의 바깥쪽을 향하게 됩니다. (경사하강법에서 목적지에 도달하기 위해 기울기의 반대 방향으로 갔던 것을 떠올리면 됩니다) 그리고 빨간색 원을 기준으로 $\mathbf{w}$에서 법선 벡터(Normal Vector)를 구하게 되면 역시 원 바깥으로 향하는 벡터가 생성됨을 쉽게 알 수 있습니다. 그렇다면 여기서 $E_{in}$을 최소화하려면 $\mathbf{w}$를 어느 방향으로 움직여야 할까요? 당연히 빨간색 원을 따라 움직여서 $\mathbf{w}_{lin}$에 최대한 가까운 점으로 움직이면 될 것입니다. 그 방향을 $\mathbf{w}$를 기준으로 표현한다면, $E_{in}$ 기울기의 반대 방향과 제한조건의 법선벡터 반대 방향으로 움직여야 함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이를 수학적으로 표현하면 $E_{in}$의 변화량 $\nabla E_{in}(\mathbf{w}_{reg})$은 $-\mathbf{w}_{reg}$에 비례한다고 말할 수 있습니다. 정확한 값으로는 $-2 \times \frac{\lambda}{N}\mathbf{w}_{reg}$로 표현하는데, 왜 갑자기 이 식이 나왔는지는 강의를 들어도 잘 이해가 되지 않습니다. 알게 되면 글을 수정하겠습니다. 혹시 아시는 분은 댓글로 알려주시면 감사하겠습니다.&lt;/p&gt;

&lt;p&gt;어쨌든 $\nabla E_{in}(\mathbf{w}_{reg})$의 오른쪽 항을 이항하면 오른쪽 항에는 0벡터만 남게 됩니다. 그런데 이 식은 어떤 식을 미분한 식처럼 보입니다. 쉽게 말해, $E_{in}(\mathbf{w}) + \frac{\lambda}{N}\mathbf{w}^{\sf T}\mathbf{w}$을 최소화하기 위하여 이를 미분했을 때의 결과로 보입니다.&lt;/p&gt;

&lt;p&gt;여기서 문제는 제한이 들어갔을 때의 상수 $C$가 이 식에는 사라져 있다는 것입니다. 대신 다른 상수 $\lambda$가 존재합니다. 다행히도 이 두 상수간에는 관계가 존재하는데, $C$가 증가할수록 $\lambda$가 줄어든다는 것이고 그 반대도 성립한다는 겁니다. 즉, 우리가 $C$의 값을 조절하고 싶다면 그 대신 $\lambda$를 조절함으로써 동일한 결과를 얻을 수 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 Augmented Error가 무엇인지 살펴보겠습니다. 이전 슬라이드에서 유도한 $E_{in}(\mathbf{w}) + \frac{\lambda}{N}\mathbf{w}^{\sf T}\mathbf{w}$식을 간단하게 $E_{aug}(\mathbf{w})$로 정의합니다. 여기서 $E_{in}$을 우리가 아는 식으로 대입하고, 그 식을 정리하면 이전 슬라이드에서 처음 시작했던 식과 동일한 결과가 나옵니다. (전개 과정은 추후에 글을 수정함으로써 보충하도록 하겠습니다)&lt;/p&gt;

&lt;p&gt;이 식은 가설 집합을 명시적으로 제한하고 있기 때문에 7장에서 배웠던 VC Analysis에 적합하다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 정의했던 $E_{aug}(\mathbf{w})$을 최소화하는 계산을 시도해봅시다. 식을 정리하여 미분값이 0이 되는 점을 찾으려면 $Z^{\sf T}(Z\mathbf{w}-\mathbf{y})+\lambda\mathbf{w})=\mathbf{0}$을 계산하면 됩니다. 이를 정리하여 $\mathbf{w}$를 계산하면 $(Z^{\sf T}Z + \lambda\mathbf{I})^{-1}Z^{\sf T}\mathbf{y}$을 됩니다. 정규화를 적용하기 전에 구했던 $\mathbf{w}_{lin}$과는 $\lambda\mathbf{I}$ 만큼의 차이가 있다는 것이 쉽게 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이번에는 $\lambda$의 값을 변화시킴으로써 학습 결과가 어떻게 변하는 지 알아보겠습니다. $\lambda=0$일 때는 정규화를 사용하지 않는 것과 같으므로 Overfitting이 일어납니다. 그런데 $\lambda$를 0.0001로 아주 약간만 올리게 되면 Target Function과 거의 비슷한 결과가 나오게 됩니다. $\lambda$를 조금 더 증가시켜 0.01로 설정하면 오히려 이전보다 Target Function과 더 멀어지지만, 그래도 Overfitting이 일어났을 때보다는 결과가 좋습니다. $\lambda$를 1까지 증가시키게 되면 오히려 학습을 제대로 하지 못하는 Underfitting이 일어나버립니다. $\lambda$가 커질수록 학습 결과는 점점 더 평평해지고, 너무 크게 정한다면 Underfitting이라는 새로운 문제가 생기기 때문에 적절한 $\lambda$의 값을 찾는 것이 중요하지만, 안타깝게도 최선의 $\lambda$를 찾을 수 있는 방법이 존재하지 않기 때문에 아주 작은 값부터 값을 올려가며 $\lambda$을 찾는 경험적인 방법을 사용할 수밖에 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 설명했던 방법을 Weight Decay라고 부릅니다. 왜 그런지 이유를 알아보기 위해 인공신경망 모델에서 경사하강법을 사용하는 상황을 가정해봅시다. 원래의 경사하강법의 수식에서 정규화를 적용한다면 $-2\eta\frac{\lambda}{N}\mathbf{w}(t)$ 항이 추가됩니다. 이 항이 포함된 식을 정리한다면 $\mathbf{w}(t)$가 $(1-2\eta\frac{\lambda}{N})$ 로 묶이게 됩니다. 즉, $t$가 한 단계 증가할 때마다 $\mathbf{w}(t)$는 조금씩 작아지게 됩니다. 이것이 바로 Weight Decay라고 불리게 되는 이유입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay의 변형에는 어떤 것들이 있는지 알아보겠습니다. 가중치 벡터의 각 Element $w_q^2$를 제곱한 값에 $\gamma_q$를 곱해준 것을 Regularizer라고 가정해봅시다. 여기서 $\gamma_q$를 어떻게 정하느냐에 따라 가중치 벡터의 Element의 크기를 정해줄 수 있습니다. 시그마 수식의 전체 총합은 일정한 상수 $C$ 보다 작아야 하기 때문인데요, 만약에 $\gamma_q$가 크다면 제한 사항을 만족해야 하기 때문에 가중치 벡터의 Element들은 크기가 작을 수밖에 없고, 반대로 $\gamma_q$가 작다면 Element를 그보다 크게 정할 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;여기에서는 두 가지 반대 케이스를 보여주는데, $\gamma_q=2^q$인 경우에는 함수의 차수를 작게 맞추려고 합니다. 예를 들어 가설 모델이 고차 다항식인 경우에는 낮은 차수의 다항식과 맞추려는 역할을 하게 된다는 뜻입니다. 반대로 $\gamma_q=2^{-q}$인 경우라면 좀 더 고차인 다항식에 맞추려고 하기 때문에 Target Function이 복잡한 함수로 예상되는 경우에 좋은 퍼포먼스를 보일 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 각 레이어마다 다른 $\gamma$의 값을 정해준다고 합니다. 가장 유명한 식은 Tikhonov Regularizer라는 것인데, 처음 주어졌던 식과는 다르게 완전한 2차식으로 표현됩니다. 그냥 이런 것이 있구나 정도만 이해하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay를 배웠으니 그 반대의 경우도 떠올려볼 수 있습니다. 만약에 Weight를 반대로 크게 만든다면 어떻게 될까요?&lt;/p&gt;

&lt;p&gt;우선 Weight Decay의 효과를 알기 위해 오른쪽 그림처럼 $\lambda$의 크기를 증가시킬 때 예상되는 Out of Sampel Error $E_{out}$이 어떻게 변하는지 보면, 일정 지점까지는 $\lambda$를 증가시킬 때 $E_{out}$이 감소하지만, 그 이후부터는 오히려 증가함을 알 수 있습니다. 만약에 Weight growth를 시키는 상황에서 $\lambda$를 증가시킨다면? 그냥 $E_{out}$이 바로 수직 상승해 버립니다. 이렇게 되면 Weight Growth를 하게 되면 엄청나게 큰 문제가 생길 것 같지만, 실제로 계산을 해보면 Weight Growth에서는 $\lambda$를 0으로 유도하기 때문에 계산의 낭비만을 제외하고는 정규화를 사용하지 않는 것과 차이는 없다고 합니다.&lt;/p&gt;

&lt;p&gt;또한 여기서 실용적인 규칙을 하나 알려주는데, Stochastic Noise는 High-frequency이고 Deterministic Noise는 Non-smooth라고 합니다. (강의에서 정말 간단하게 알려주고 넘어가는데, 저는 강의만 보고서는 이게 무슨 의미인지는 알지 못하겠습니다. 혹시라도 아시는 분은 댓글로 부탁드립니다.) Regularize는 더 Smooth 한 가설을 선택하는 경향이 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay의 Regularizer를 $\Omega$라고 정의해 봅시다. $E_{aug}$는 $\Omega$의 형태로 표현할 수 있습니다. 여기서 $h$는 $\mathbf{w}$에 영향을 주는 변수라고 생각하시면 됩니다. 그런데 이 식을 조금만 변형시키면, 7장에서 보았던 Generalization Bound와 유사함을 알 수 있습니다. 물론 이 때는 $E_{aug}$가 아니라 $E_{out}$이었습니다. 그다음 문장이 조금 이해하기 어려운데, $E_{aug}$는 $E_{out}$에 대한 대용물로써 $E_{in}$보다 낫다고 합니다. 그 이유를 인터넷 강의에서 자세하게 설명해주지 않아서 이 부분은 추후에 공부한 후 보강하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;choosing-a-regularizer&quot;&gt;Choosing a regularizer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 문제별로 어떻게 빠르게 Regularizer를 선택하는지 그 방법을 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완벽한 Regularizer가 무엇일까요? Target Function은 알 수 없지만 그 방향으로 갈 수 있게 만드는, 즉 Target Function과 최대한 가까워질 수 있게 만드는 Regularizer일 것입니다. Regularization은 기본적으로 Overfitting을 완화시키는 목적으로 사용되기에, Noise를 손상시키게 됩니다. Perfect Regularizer는 Closed Form을 구하는 것이 아닌, 경험적인 방법으로 유도하게 됩니다. 여기에서는 가이드라인으로 Smooth 하거나 간단한 방향으로 움직여야 한다고 말합니다. 그 이유는 Noise가 Smooth 하지 않기 때문입니다. 그 방향으로 가게 되면 Noise를 더 손상시킬 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;만약에 나쁜 $\Omega$를 선택한다면 어떻게 될까요? 다행히도 $\lambda$의 값을 잘 조절한다면 나쁜 $\Omega$를 선택한다고 해도 Overfitting을 해결할 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 특별한 예제로 인공신경망 모델에서의 Regularizer를 직관적으로 선택해보겠습니다. 인공신경망 모델에서 큰 가중치와 작은 가중치를 사용하는 경우를 각각 따져봅시다. 만약에 매우 작은 가중치를 선택한다면, 가중치는 원점 근처, 즉 Linear 함수와 다르지 않은 부분에 분포해 있을 것입니다. 결과적으로 매우 작은 가중치에서는 간단한 선형 함수를 구현하는 것입니다. 반대로 가중치가 큰 경우라면 선형 부분을 넘어 Logical 한 부분으로 가게 됩니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 Overfitting을 해결하는 방법 중 하나로 Weight Elimination이 있습니다. 이름 그대로 몇 개의 Weight를 아예 0으로 만들어 버리는 방법입니다. Weight의 수가 적어진다면 그만큼 VC Dimension이 줄어들기 때문입니다. 다만 여기서는 완전히 0으로 만들어버리기보다는 Soft 한 방법을 제시하고 있는데, $\gamma$를 $\frac{\beta^2}{(w_{ij}^{(l)})^2}$로 정의하는 것입니다. 이렇게 되면 $w$가 작은 경우에는 $\frac{1}{\beta^2}$에 가깝게 될 것이고 $w$가 큰 경우라면 1에 가깝게 될 것입니다. 결과적으로 덜 중요한 Weight들은 0에 가깝게 되지만 중요한 Weight들은 그 값이 그대로 유지되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지난 장에서 살짝 언급하였던 Early Stopping 또한 Regularizer의 한 형태입니다. 이 방법은 특이하게 목적 함수를 변화시키지 않습니다. 학습을 언제 멈춰야 할지를 선택하는 방식이기 때문입니다. 이런 해결 방법을 Validation이라고 부르는데, 이것은 다음 장에서 더 자세하게 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 최적의 $\lambda$를 찾는 방법을 알아봅시다. 왼쪽의 그래프는 Stocahstic Noise가 정규분포를 따른다고 가정했을 때 Variation 별 $\lambda$와 예상되는 Out of Sample Error를 나타낸 것입니다. 만약에 Variation이 0이라면(=Stochastic Noise가 없다면) 정규화 자체를 할 필요가 없음을 알 수 있습니다. Variation이 0.25일 때와 0.5일 때를 비교하면 Variation이 클수록 $\lambda$의 값 또한 커져야 함이 보입니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그래프는 Deterministic Noise인 Target Function의 차수에 따른 $\lambda$와 Out of Sample Error의 상관관계를 나타내고 있습니다. 이 역시 Stochastic Noise 때와 비슷하게, 차수가 높을수록 높은 $\lambda$값으로 정해야 함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Overfitting</title><link href="http://localhost:4000/studies/overfitting/" rel="alternate" type="text/html" title="Overfitting" /><published>2019-09-28T00:00:00+09:00</published><updated>2019-09-28T00:00:00+09:00</updated><id>http://localhost:4000/studies/overfitting</id><content type="html" xml:base="http://localhost:4000/studies/overfitting/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11장은 Overfitting에 대해 배우게 됩니다. 단어의 뜻만 봐도 이게 무엇을 의미하는지 대충 알 수 있습니다. 예를 들면 옷 가게에서 Fitting Room은 옷이 자기한테 잘 맞는지를 확인하는 곳입니다. Fit의 의미는 &lt;strong&gt;맞다&lt;/strong&gt;이고 Over는 무언가 과다한 상태의 형용사이기 때문에, Overfit은 &lt;strong&gt;과다하게 맞다&lt;/strong&gt;라고 해석이 가능합니다. 기계학습에서 Overfitting도 이와 비슷한 의미입니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 4개 부분으로 나뉘어 있습니다. 먼저 Overfitting이 무엇인지부터 배우고, 4장에서 배웠던 Noise가 무슨 역할을 하는지 알아본 뒤, Deterministic Noise를 배우고 마지막으로 Overfitting 문제를 해결하기 위한 방법을 알아봅니다.&lt;/p&gt;

&lt;h2 id=&quot;what-is-overfitting&quot;&gt;What is overfitting?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Overfitting이 일어나는 상황부터 한번 살펴보겠습니다. 간단한 예제로, 2차함수와 비슷하게 생긴 Target Function (파란색 선)이 있습니다. 데이터는 물론 이 Target Function으로부터 나오는데, 이 데이터가 완벽하게 Target Function 위에 있지 않고 어느정도의 Noise가 끼어 있는 상태로 주어집니다. 5개의 점이 주어졌으니, 이를 완벽하게 커버하기 위해서는 4차함수가 필요합니다. 그렇게 해서 4차함수를 사용해 In Sample Error가 0이 되도록 함수를 구한다면, 주어진 데이터는 완벽하게 커버할지 몰라도, Out of Sample Error는 엄청나게 커지는 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 “Overfitting”과 “Bad Generalization”의 차이가 무엇인지 알아봅시다. Noise가 있는 데이터를 인공신경망 모델로 학습했을 때의 In Sample Error와 Out of Sample Error를 구합니다. 가로축은 학습횟수를 나타내고 세로축은 에러를 나타냅니다.&lt;/p&gt;

&lt;p&gt;가로축의 0과 1000 사이를 보시면 In Sample Error와 Out of Sample Error 모두 에러가 매우 높게 나온 것이 보입니다. 이렇게 두 Error가 모두 높은 상황은 아직 모델이 데이터를 커버하지 못하는, 즉 일반화가 나쁜 상황이 됩니다.&lt;/p&gt;

&lt;p&gt;반대로 가로축이 6000을 넘긴 시점을 보시면, 학습을 하면 할수록 In Sample Error가 줄어드는데, Out of Sample Error는 반대로 증가하는 모습을 보입니다. 이 때는 모델이 Sample 데이터에 과도하게 맞추고 있다는 것을 보여주므로, 이런 상황을 &lt;span style=&quot;color:red&quot;&gt;Overfitting&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Overfitting이 일어나는 이유는 데이터를 보장된 만큼보다 더 맞추려고 하기 때문입니다. 이 말은, 데이터에 Noise가 있다면 그것까지 굳이 맞추려고 할 필요가 없는데도 불구하고 완벽하게 맞추려고 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;슬라이드 3을 예로 들면 데이터에 Noise가 있는 것을 알았을 시, In Sample Error가 어느정도 발생하는 것을 감수하고 2차함수로 모델링을 한다면 Overfitting이 일어나지 않지만, Noise까지 완벽하게 맞추려고(fit) 4차함수로 모델링을 하기 때문에 Overfitting이 발생하는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-role-of-noise&quot;&gt;The role of noise&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;조금 더 복잡한 상황에서 살펴봅시다. 왼쪽의 예제는 Target Function이 10차 방정식인데, 주어진 데이터에 Noise가 끼어있는 상황이고, 오른쪽의 예제는 Target Function이 50차 방정식인데, 주어진 데이터에 Noise는 없는 상황입니다. 데이터는 두 모델 각각 15개씩 주어졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 두 예제는 2가지의 모델로 해결해 봅시다. 하나는 2차 방정식 모델이고, 다른 하나는 10차 방정식 모델입니다. 각 예제의 그림에서 초록색 선이 2차 방정식 모델로 해결한 결과이고, 빨간색 선이 10차 방정식 모델로 해결한 결과입니다. 놀랍게도 2차 방정식 모델이 두 예제에서 모두 10차 방정식 모델보다 훨씬 좋은 결과가 나왔습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드의 결과를 보면 의문이 듭니다. 첫번째 예제의 Target Function은 10차 방정식이였기 때문에 학습 모델을 10차 방정식으로 잡는 것은 상식적으로 생각해봤을때 전혀 과도하게 잡은 모델이 아닙니다. 오히려 2차 방정식으로 모델을 잡은 것이 너무 낮은 차수로 잡은 것이 아닌가 하는 생각을 들게 만들죠. 그렇다면 왜 이러한 결과가 나온 것일까요?&lt;/p&gt;

&lt;p&gt;그 이유는 바로 데이터의 수가 적었기 때문입니다. 7장에서 배웠듯이, 올바른 학습을 위해서는 VC Dimension의 10배 만큼의 데이터가 필요하다고 하였습니다. 물론 2차 함수 모델의 VC Dimension은 3이므로 30개가 필요한데 비해 데이터는 그것의 절반밖에는 주어지지 않았지만, 10차 함수 모델의 필요한 데이터의 수인 110개보다는 가까웠기 때문에 더 좋은 성능이 나온 것입니다. 즉, 학습 모델을 잡을 때는 Target Function 보다 주어진 데이터의 수에 맞추는 것이 훨씬 더 좋은 성능이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리는 이미 이 사실을 이전에 배웠습니다. 데이터의 수가 충분하다면 물론 Target Function의 차수와 가까운 10차 방정식 모델 $\mathcal{H}_{10}$이 $\mathcal{H}_{2}$보다 성능이 좋게 나올 것입니다. 그러나 주어진 상황에서는 데이터가 매우 적은 상황이었기 때문에 회색 영역이 현재의 상황을 나타내는 부분이 되고, 데이터가 적은 상황에서도 In Sample Error와 Out of Sample Error의 간격이 적은 2차 방정식 모델이 그렇지 않은 10차 방정식 모델 보다 &lt;strong&gt;아직은&lt;/strong&gt; 좋은 퍼포먼스를 보이게 되는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Target Function이 50차 방정식이고 Noise가 없었던 두 번째 예제로 넘어가 봅시다. Noise가 없는 상황이기 때문에 Overfitting은 일어나지 않을 것이라 생각되어 복잡한 모델인 $\mathcal{H}_{10}$이 이길 수 있을 것처럼 보이지만, 실제로는 여전히 $\mathcal{H}_{2}$가 더 나은 성능을 보여주고 있습니다. 왜 그렇게 되는지 알아보기 전에, 과연 이런 상황을 Noise가 정말로 없다고 부를 수 있을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 질문에 답하기 위해 좀 더 구체적인 실험을 해봅시다. Input을 간단하게 1차원 $x$라 하고 Output을 $y$라고 한다면 Noise가 없는 Input과 Output의 관계는 $y=f(x)$가 됩니다. 여기에 Noise Function $\epsilon(x)$를 더해주면, Noise가 추가된 Output $y$의 식이 됩니다. Noise는 일반적으로 가우시안 분포를 따르기 때문에 Noise의 정도를 나타내기 위해 이를 $\sigma^2$으로 표현합니다.&lt;/p&gt;

&lt;p&gt;문제를 구체적으로 하기 위해 Target Function은 다항함수라고 가정하고, 최대 차수를 $Q_f$라 정의합니다. 그렇게 되면 $x$의 각 항에 계수가 붙는 다항함수 꼴을 시그마로 표현할 수 있습니다. 그리고 데이터의 수를 $N$으로 정의합니다.&lt;/p&gt;

&lt;p&gt;방금 정의한 기호를 사용하여 하단 왼쪽에 있는 예제를 예로 들면, $Q_f=10$인 Target Function에서 $N=15$개의 데이터가 $\sigma^2$의 Noise를 따르도록 나타낸 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 가지고 Overfit을 측정해봅시다. $N$개의 데이터 $(x_1, y_1), \ldots, (x_N, y_N)$를 사용하여 2개의 모델 $\mathcal{H}_{2}$과 $\mathcal{H}_{10}$로 학습한 결과를 비교해야 합니다. 비교는 각각의 모델에서 가설을 가지고 Out of Sample을 계산한 뒤, 그 차이를 구하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 슬라이드는 그 결과를 표현하고 있습니다. 이 그래프는 무지개색으로 그 값이 어느정도인지를 표현하고 있는데, 색이 빨간색에 가까울 수록 Overfit Measure가 커지고(즉, $E_{out}(g_{10}) \gg E_{out}(g_2)$) 파란색에 가까울수록 Overfit Measure가 작아지는(즉, $E_{out}(g_{10}) \ll E_{out}(g_2)$) 상황입니다. 초록색인 상황은 두 가설에서의 Out of Sample이 동일하다는 뜻(즉, $E_{out}(g_{10}) = E_{out}(g_2)$) 입니다. 여기서 알아야 할 것은 빨간색이 짙을 수록 Overfitting이 심하게 일어난다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;먼저 왼쪽의 Noise Level의 변화에 따른 그래프를 봅시다. Noise Level이 클수록, Overfitting을 해결하기 위한 데이터의 수가 더 많이 필요한 것을 알 수 있습니다. 예를 들어, Noise Level이 0일 때는 100개의 데이터가 주어졌을 때 Overfitting이 일어나지 않지만, Noise Level이 2일 때는 똑같이 100개의 데이터가 주어졌을 때 Overfitting이 일어나기 때문입니다.&lt;/p&gt;

&lt;p&gt;왼쪽의 그래프는 단순해보이지만, 오른쪽의 그래프는 조금 복잡해보입니다. 오른쪽의 그래프는 Target Function의 복잡도(즉, 다항함수의 차수)가 Overfitting에 미치는 영향을 나타냅니다. 먼저, $Q_f$가 커지면 커질수록 Overfitting이 심해지는 것은 간단하게 알 수 있습니다. 특이한 점은 $Q_f$가 일정 수치 이하일 때는 복잡도가 높아질수록 Overfitting이 일어나지 않는데, 뒤에 나올 슬라이드 17에서 그 이유가 나옵니다.&lt;/p&gt;

&lt;p&gt;여담으로 이 두 그래프는 교재의 표지 하단 왼쪽에 나와있는 그림과 같습니다. 또한 매 강의 슬라이드 첫장 하단 왼쪽에도 동일한 그림이 나와있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 실험으로 알게 된 것은 Noise Level과 Target Complexity 모두 Overfitting을 증가시킨다는 사실입니다. Noise Level $\sigma^2$은 어떤 확률 분포를 따르는 Noise이므로 이를 &lt;span style=&quot;color:red&quot;&gt;Stochastic Noise&lt;/span&gt;라고 부르고, Target Complexity는 확률적인 부분이 아니기 때문에 이를 &lt;span style=&quot;color:red&quot;&gt;Deterministic Noise&lt;/span&gt;라고 부릅니다. 이것을 Noise라고 부르는 이유는 실제 기계학습에서는 Target Complexity를 알 수 없기 때문에 마치 Noise처럼 보이기 때문입니다.&lt;/p&gt;

&lt;p&gt;요약하자면, 데이터의 수를 증가시킬 수록 Overfitting은 감소하고 Stochastic Noise나 Deterministic Noise를 증가시키면 Overfitting 또한 증가한다는 결론을 낼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;deterministic-noise&quot;&gt;Deterministic noise&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 Deterministic Noise에 대해 좀 더 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deterministic Noise는 Stochastic Noise와 마찬가지로 가설 집합 $\mathcal{H}$가 알 수 없는 요소입니다. 알 수 없다는 것은 학습을 하면서 “이것이 Noise다!” 라고 분별할 수 없음을 뜻합니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림은 Target Function $f$와 그 $f$를 최대한 가까운 근사 함수 $h^{*}$를 구한 모습인데, 보시다시피 어느정도의 차이가 있습니다. 그 이유는 그림에서 알 수 있다시피 $h^{*}$의 차수가 $f$보다 낮기 때문입니다.&lt;/p&gt;

&lt;p&gt;만약에 $\mathcal{H}$가 더 큰 차수의 모델이었다면, 그 오차가 줄어듬을 알 수 있습니다. 이를 통해, Deterministic Noise는 $\mathcal{H}$에 따라 달라진 다는 것을 알 수 있습니다. 또한 학습을 주어진 데이터로 하기 때문에, 데이터가 동일하다면 학습 결과와 $h^{*}$ 또한 동일하므로, Deterministic Noise 또한 변하지 않는다는 특징이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Deterministic Noise가 Overfitting에 끼치는 영향을 살펴보겠습니다. 오른쪽의 그림은 이전 슬라이드에서 본 그림인데, 그때 느낀 이상한 점은 $Q_f$가 일정 이하(여기서는 10)일 때는 Target Complexty가 커져도 Overfitting이 일어나지 않는 것이었습니다.&lt;/p&gt;

&lt;p&gt;저 일정 이하의 부분에서는 Target Function보다 낮은 차수이기 때문에 Target Complexity를 높일 수록 Overfitting을 피할 수 있는 것입니다. (물론 일정 이상의 데이터가 필요하다는 것도 그래프를 통해 알 수 있습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림을 통해 분석해 보았으니, 이제 수식을 통해 좀 더 정밀하게 분석해 보겠습니다. 8장에서 배웠던 Bias와 Variance를 기억하시나요? 이 분석 방법은 샘플에서 벗어난 오차를 Bias와 Variance로 각각 분류하였습니다. 이 당시 분석에서는 $f$에 Noise가 있다고 생각하지 않았습니다. 만약에 $f$에 Noise를 추가하게 된다면 식이 어떻게 바뀔 지 알아봅시다. Noise는 Gaussian Distribution (정규분포)을 따르기 때문에 그 평균은 0이라고 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$y = f(\mathbf{x}) + \epsilon (\mathbf{x})$로 놓고 나머지는 8장에서 했던 전개 과정을 그대로 수행하였습니다. 식은 다행히 그렇게 어렵지 않은데, 그 이유는 Noise의 평균이 0이기 때문에 Noise와 다른 항을 곱한 Cross Term의 평균 또한 모두 0이 되어버리기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 최종적으로는 기존의 Variance와 Bias에 $\epsilon(\mathbf{x})$를 제곱한 평균만 추가되는 꼴이 됩니다. 그 마지막 항이 바로 Stochastic Noise가 됩니다. 그런데 Bias는 최선의 가설 $\bar{g}$가 Target Function과의 차이를 뜻했는는데 이것이 바로 Deterministic Noise의 정의와 같기 때문에, 여기서 이를 같은 용어로 치환합니다.&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-overfitting&quot;&gt;Dealing with overfitting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이제 Overfitting을 해결하는 방법을 알아보겠습니다. 불행하게도, Overfitting은 쉽게 해결되는 문제가 아니기 때문에, 이번 장에서는 어떤 방법들이 있는지만 소개하고, 그 방법들은 다음 2개의 장에서 하나씩 자세하게 설명합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째로 &lt;span style=&quot;color:red&quot;&gt;Regulatization (정규화)&lt;/span&gt;라는 방법이 있습니다. 이는 학습 중간에 제동을 거는 방법인데, 구현 방법은 목적 함수 자체를 약간 변형시키는 방법을 사용합니다.&lt;/p&gt;

&lt;p&gt;두 번째 방법은 &lt;span style=&quot;color:red&quot;&gt;Validation (검증)&lt;/span&gt;입니다. 위의 슬라이드 4를 보시면 Early Stopping 이라는 부분에서 멈춘다면 Overfitting이 일어나지 않음을 알 수 있습니다. 이 부분을 찾아서 멈추는 것이 바로 Validation이라는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 장에서 바로 Regulatization을 배우게 될 텐데, 대충 어떤 것인지 감을 잡기 위해 이번 장의 첫 예제를 다시 한번 보여드리겠습니다. 왼쪽 그림이 바로 Overfitting의 문제를 알게 된 그 그림인데, 이 때는 모델을 4차함수로 잡았기 때문에 Target Function과 큰 차이가 벌어졌었습니다.&lt;/p&gt;

&lt;p&gt;그런데 4차함수의 모양은 꼭 저런 모양만 있지 않습니다. 4차함수의 변수를 약간 변형시킨다면, 오른쪽 그림과 같이 2차 함수와 비슷한 모양으로 변해 4차함수로 모델을 잡더라도 Overfitting 문제를 해결할 수 있습니다. 바로 그 방법이 무엇인지 구체적으로 배우게 될 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Neural Networks</title><link href="http://localhost:4000/studies/neural-networks/" rel="alternate" type="text/html" title="Neural Networks" /><published>2019-09-21T00:00:00+09:00</published><updated>2019-09-21T00:00:00+09:00</updated><id>http://localhost:4000/studies/neural-networks</id><content type="html" xml:base="http://localhost:4000/studies/neural-networks/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;10장은 인공신경망(Neural Network)에 대해 배우게 됩니다. 현재 인공신경망 모델은 기계학습의 대세가 되어 많은 관심을 받고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 3개로 나뉘어 있습니다. 지난 장에서 배운 경사하강법(Gradient Descent)의 변형인 확률적 경사하강법(Stochastic Gradient Descent)를 배우고, 본격적인 인공신경망 모델에 대해서 배운 다음, 마지막으로 인공신경망 모델의 학습 알고리즘인 역전파(Backpropagation) 알고리즘을 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic gradient descent&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 간단하게 지난 장에서 배운 경사하강법을 복습해보면, 처음에 무작위의 $w$를 정한 다음 기울기를 계산하여 어떤 방향으로 움직일 것인가를 정하여 $\mathbf{w}$를 단계적으로 값을 수정하여 In Sample Error가 최소가 되도록 만들어주는 방법이었습니다.&lt;/p&gt;

&lt;p&gt;분명 경사하강법은 최적의 $\mathbf{w}$를 정하기 위한 좋은 방법이지만, 문제는 한 단계를 거칠 때마다 모든 데이터를 이용하여 방향을 결정해야 한다는 것입니다. 즉, 데이터의 개수가 $N$개라면, 매번 $N$개의 데이터의 $e(h(\mathbf{x}_n, y_n))$를 계산해야 한다는 것입니다. 데이터의 개수가 적다면 크게 문제 될 사항은 아니지만, 일반적으로 기계학습에서는 수많은 데이터를 보유하여 그것을 기반으로 문제를 해결하기 때문에, 계산량이 많다는 것은 결코 반가운 사항은 아닙니다.&lt;/p&gt;

&lt;p&gt;이런 식으로 한번의 움직임을 위해 모든 데이터를 일괄적으로 처리하는 방식을 &lt;span style=&quot;color:red&quot;&gt;Batch&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 확률적으로 계산하기 위해 조금 다른 방법을 사용할 것입니다. 한 단계에서 오직 1개의 데이터 $(\mathbf{x}_{n}, y_{n})$ 만을 무작위로 추출하는 것입니다. 그리고 오직 그 1개의 데이터만을 사용해 경사하강법을 사용하는 것입니다.&lt;/p&gt;

&lt;p&gt;언뜻 보면 전체의 데이터를 기준으로 방향을 정하던 때와 달리 한 개의 데이터만을 기준으로 방향을 정하기 때문에 원하는 결과가 나오지 않을 것이라고 생각이 들지만, 사실 이 방법은 이미 이전에 PLA에서도 사용한 방법입니다. 이런 방식으로 방향을 정하는 것이 일반적인 경사하강법과 같다는 것을 보이기 위해 “평균적인” 방향을 계산해보면, 경사하강법의 방향 계산 식과 동일하다는 것을 알 수 있습니다. 물론 하나하나의 단계에서는 경사하강법의 방향과 차이가 있을 수 있지만, 많은 횟수를 반복하게 되면 결과적으로는 원래의 경사하강법과 동일한 방향이 된다는 것입니다. 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Stochastic Gradient Descent (확률적 경사하강법)&lt;/span&gt; 이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;확률적 경사하강법의 장점으로는 먼저 경사하강법에 비해 계산이 빠르다는 것입니다. 두 번째 장점으로는 무작위성으로 인해 이득을 볼 수 있다는 것으로, 직관적으로 이해는 쉽지 않기 때문에 오른쪽 그림을 참고해봅시다. 지난 장에서 경사하강법을 설명했을 때 봤던 U자 모양의 그래프인 경우는 사실 일반적으로 잘 발생하지 않고, 보통은 중간의 그림처럼 울퉁불퉁한 모양의 그래프가 더 많이 발생합니다. 이런 울퉁불퉁한 모양의 그래프의 가장 큰 문제는, &lt;strong&gt;전역 최솟값(Global Minimum)&lt;/strong&gt;을 찾다가 &lt;strong&gt;지역 최솟값(Local Minimum)&lt;/strong&gt;을 찾는 일이 발생한다는 것입니다. 그림상으로 봤을 때는 어느 부분이 지역 최솟값인지 쉽게 구분이 가능하지만, 실제 문제를 해결하는 과정에서는 그림이 아닌 수치로만 확인하기 때문에 지금 내가 찾은 답이 지역 최솟값인지, 전체 최솟값인지 구분이 되지 않는 문제가 있습니다. 만약에 방금처럼 무작위 하게 데이터를 선택하여 방향을 정한다면, (데이터가 고루 퍼져있다는 전제 하에) 지역 최솟값에 빠지더라도 그곳을 빠져나갈 수 있는 기회를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;세 번째 장점으로는 간단하다는 것인데, 확률적 경사하강법의 경우 Learning Rate $\eta$를 보통 간단하게 0.1로 놓는다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1장에서 나왔던 영화 추천 문제를 다시 가져왔습니다. 유저의 영화 선호도는 코미디, 액션, 등장하는 배우 등으로 이루어진 벡터로 이루어져 있고, 영화 또한 영화의 장르, 등장하는 배우 등으로 이루어진 벡터가 있습니다. 이것을 이용하여 유저가 어떤 영화를 좋아할지 추천하는 시스템을 만들어야 하는데, 넷플릭스에서 기존의 방법보다 10% 향상시키는 방법에 대해 100만 달러의 상금을 걸었다고 언급했습니다. 이 예제가 왜 갑자기 이 곳에 다시 언급되었나 궁금했는데, 온라인 강의에서 확률적 경사하강법을 사용한 방법이 실제로 10%의 성능 향상을 이루어내 100만 달러의 상금을 탔다고 합니다.&lt;/p&gt;

&lt;p&gt;100만 달러의 상금을 받은 방법을 간단하게 설명하자면, 유저와 영화의 각 요소를 곱한 값을 더해 영화에 매긴 평점과의 Squared Error를 계산한 것을 $\mathbf{e}_{ij}$로 놓고 확률적 경사하강법을 사용했다고 합니다. 방법 자체는 크게 어렵지 않아 보이지만, 마치 콜럼버스의 달걀처럼 보고 나면 쉬운데 막상 이걸 떠올리지는 쉽지 않았나 봅니다.&lt;/p&gt;

&lt;h2 id=&quot;neural-network-model&quot;&gt;Neural network model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째로 이번 장의 핵심인 인공신경망 모델에 대해 배워봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망은 이름에서 알 수 있듯이 신경망의 생물학적인 구조에서 영감을 얻어 만들어진 모델입니다. “배운다” 라는 생물학적인 기능을 구현하기 위해 생물학적인 구조를 모방한 것입니다. 물론 구조만 그렇게 만들고 끝나는 것이 아니라 생물학적으로 동작하게끔 유사한 시스템까지 구현해야 합니다.&lt;/p&gt;

&lt;p&gt;신경망은 &lt;strong&gt;Synapse&lt;/strong&gt;로 연결된 &lt;strong&gt;Neuron&lt;/strong&gt;으로 구성되어 있습니다. 각 뉴런들은 입력을 받은 자극으로부터 간단한 연산을 한 후 그 결과를 내보냅니다. 마치, 퍼셉트론(Perceptron)과 유사하다고 생각하시면 됩니다. 인공신경망은 다수의 뉴런으로 구성되어 있는 신경망과 비슷하게 다수의 퍼셉트론으로 이루어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1장에서 배운 퍼셉트론은 간단하고도 꽤 유용한 도구였지만, 지금까지 문제가 꾸준히 있었습니다. 특히 선형 분류가 되지 않은 문제를 해결하기 위해 여러 장에 걸쳐 꽤 많은 노력을 기울여왔습니다. 그러나 이런 노력에도 불구하고 퍼셉트론으로 아예 해결할 수 없는 문제도 있습니다. 위 슬라이드의 첫 번째 그림을 보면 +와 -가 서로 대각선 영역으로 나뉘어 있습니다. 이를 정확하게 나누기 위해서는 최소한 2개의 직선이 필요합니다. 그러나 퍼셉트론은 1개의 직선으로 이루어진 방법입니다. 따라서 기존의 퍼셉트론을 사용해서는 이 문제를 해결할 수 없는데, 만약에 이 문제를 두 번째와 세 번째 그림과 같이 $h_1, h_2$ 2개의 선으로 각각 나눈 다음 합칠 수는 없을까 라는 새로운 방법이 제시되었습니다.&lt;/p&gt;

&lt;p&gt;두 개의 서로 다른 도구를 합치기 위한 방법은 기본적으로 OR 논리회로와 AND 논리회로인데, 슬라이드 아래의 그림은 이것을 퍼셉트론으로 구현한 것입니다. OR은 2개의 Input 모두 -1일 경우에만 Output이 -1이 나오고 그 외에는 모두 +1이 나오는 논리회로이고, AND는 2개의 Input 모두 1일 경우에만 Output이 1이 나오고 그 외에는 모두 -1이 나오는 논리회로입니다. 왼쪽 그림을 보시면 기본적으로 1.5의 값이 들어오기 때문에 $x_1, x_2$ 모두 -1이 들어와야만 -1이 나오고, 오른쪽 그림에서는 기본적으로 -1.5의 값이 들어오기 때문에 $x_1, x_2$ 모두 1이 들어와야만 1이 나오는 구조가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 배운 내용을 토대로, 위 슬라이드의 왼쪽 그림은 이전 슬라이드의 $h_1, h_2$를 가지고 이전 슬라이드의 위쪽 첫 번째 그림과 같은 분류가 되도록 논리회로를 구성한 결과입니다. Input이 조금 복잡한 모양을 가지고 있지만, 눈썰미가 좋으신 분은 XOR 논리회로를 구현한 퍼셉트론인 것을 아실 수 있을 것입니다. 이 표현방법이 틀린 것은 아니지만, Input이 너무 복잡하게 나와있기 때문에 $h_1, h_2$로만 Input이 구성될 수 있게끔 전개한 것이 위 슬라이드의 오른쪽 그림입니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림에서 가장 왼쪽의 단계에서는 $h_{1}\bar{h}_{2}$와 $\bar{h}_{1}h_{2}$를 구현하였습니다. Threshold가 -1.5로 들어가고 있는 것을 보면 $h_1, h_2$의 입력을 각각 AND회로로 구성한 것을 알 수 있습니다. XOR를 구현하기 위해서는 $h_1$과 $h_2$가 서로 한번씩 NOT회로를 거쳐야 하는데, 오른쪽 그림에서는 그것을 구현하기 위해 가중치에 -1을 부여하는 것으로 해결하였습니다. 두번째 단계에서는 $h_{1}\bar{h}_2$와 $\bar{h}_{1}h_{2}$를 더하는 OR 논리회로를 구현함으로써 왼쪽의 그림과 동일한 Output이 나오도록 만들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 문제에 맞게 그림을 수정하긴 했지만, 원래의 문제는 Input으로 $x_1, x_2$가 들어가지 $h_1, h_2$가 들어가는 게 아니었습니다. 따라서 $x_1, x_2$를 이용하여 $h_1, h_2$를 구현하는 것 또한 수행해 주어야 합니다. 퍼셉트론 $h_1, h_2$를 구현하기 위한 가중치 $\mathbf{w}_1$과 $\mathbf{w}_2$은 이미 구했다고 가정하고, 여기서는 그 구조만 표현하도록 합시다.&lt;/p&gt;

&lt;p&gt;결과적으로 이 문제는 총 3단계를 거쳐 해결할 수 있게 되었습니다. 인공신경망에서 이 각각의 단계를 &lt;span style=&quot;color:red&quot;&gt;Layer&lt;/span&gt;라고 부릅니다. 또한 이 인공신경망의 구조는 Input에서 Output까지 다음 레이어로만 이동하고 이전 레이어로 돌아가지 않는데, 이러한 구조를 &lt;span style=&quot;color:red&quot;&gt;Feedforward&lt;/span&gt; 구조라고 합니다. 따라서 이 문제는 3개의 레이어로 구성된 피드포워드 인공신경망이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망 모델을 이용한다면 기존에 퍼셉트론으로는 해결할 수 없었던 많은 문제를 해결할 수 있습니다. 위 슬라이드의 첫 번째 그림은 퍼셉트론으로 풀 수 없는 예제입니다. 이전보다 조금 더 어려워 보이는 원(Circle)이 주어졌습니다. 직선으로 원을 표현하기는 힘드니 두 번째 그림처럼 8개의 퍼셉트론을 이용하여 8각형으로 처리하는 방법이 있습니다. 그러나 Target과 동일한 모양이 아니기 때문에 연두색 부분처럼 어느 정도의 오류가 발생하게 됩니다. 이 오류를 줄이기 위해 세 번째 그림처럼 8개의 퍼셉트론을 추가해 더 원에 가까운 16각형으로 처리할 수도 있습니다. 우리가 원한다면 더 많은 퍼셉트론을 이용해 최대한 원과 가까운 모양을 만들면서 오류를 줄여갈 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 이렇게 퍼셉트론을 많이 사용할수록 또 다른 문제가 생기게 됩니다. 많은 퍼셉트론을 사용할수록 필요한 가중치의 수와 자유도, VC Dimension이 늘어나므로 일반화가 어려워지게 됩니다. 이보다 더 큰 또 다른 문제는 모델이 복잡해질수록 최적의 가중치를 찾는 최적화까지 어려워진다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망의 일반적인 구조는 Input $\mathbf{x}$으로 이루어진 &lt;span style=&quot;color:red&quot;&gt;Input Layer&lt;/span&gt;, 그리고 로지스틱 함수 $\theta$로 이루어진 &lt;span style=&quot;color:red&quot;&gt;Hidden Layer&lt;/span&gt;, 결괏값을 내보내는 &lt;span style=&quot;color:red&quot;&gt;Output Layer&lt;/span&gt;로 이루어져 있습니다. 이 중 핵심은 Hidden Layer인데, 사용자가 일반적으로 어떤 값인지 알 수 없기 때문에 Hidden이란 이름이 붙었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 인공신경망을 구현하기 위해서는 약간의 변형이 필요합니다. 이전 장에서 배운 로지스틱 함수는 ${e^s}/{(1+e^s)}$ 였지만, 이는 0과 1 사이의 값을 갖는 비선형 곡선이었습니다. 그러나 우리는 -1과 1 사이의 값을 갖는 것이 필요하므로, 함수를 조금 아래로 내려야 합니다. 로지스틱 함수는 입력이 0일 때 0.5의 값을 가졌으므로, 로지스틱 함수에 0.5를 빼주면 정확하게 원점을 지나게 됩니다.&lt;/p&gt;

\[\frac{e^s}{1+e^s}-\frac{1}{2}\]

&lt;p&gt;문제는 이렇게 되면 함수의 크기상 -0.5와 0.5 사이만을 지나게 되는 문제가 있습니다. 우리가 원하는 것은 -1과 1 사이의 값을 갖는 것이니, 함수에 2를 곱해주어야 합니다.&lt;/p&gt;

\[2 \times \left( \frac{e^s}{1+e^s}-\frac{1}{2} \right)\]

&lt;p&gt;이 식을 정리하면 우리가 많이 보던 쌍곡 탄젠트 함수($\tanh$)와 유사한 함수가 나오게 됩니다.&lt;/p&gt;

\[2 \times \left( \frac{e^s}{1+e^s}-\frac{1}{2} \right) = \frac{2e^s-e^s-1}{1+e^s} = \frac{e^s-1}{e^s+1}\]

&lt;p&gt;입력만 $s$에서 $2s$로 바꾸어주면 쌍곡 탄젠트 함수 $\tanh$가 되는데, 왜 여기에 2를 곱해주는지는 사실 잘 모르겠습니다. 혹시 이 부분을 아시는 분은 댓글로 알려주시기 바랍니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 퍼셉트론에 비해 가중치의 수가 많이 늘어났기 때문에 이 가중치가 어디에 연결되어있는지 좀 더 정교한 표기가 필요합니다. 이제는 $w^{(l)}_{ij}$라는 방식으로 표기하는데, 이것은 레이어 $l$의 가중치 중 이전 레이어의 $i$번째 뉴런과 현재 레이어 $l$의 $j$번째 뉴런이 연결되어있다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;또한 입력에 대한 표기법도 정교해졌는데, 이전 레이어의 Output이 다음 레이어의 Input이 되기 때문입니다. 입력의 표기는 $x^{(l)}_j$로 하는데, 이것은 레이어 $l$의 $j$번째 뉴런의 Input이라는 뜻입니다.&lt;/p&gt;

&lt;h2 id=&quot;backpropagation-algorithm&quot;&gt;Backpropagation algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 인공신경망에서 각각의 가중치를 학습하기 위해 사용하는 &lt;span style=&quot;color:red&quot;&gt;Backpropagation Algorithm (역전파 알고리즘)&lt;/span&gt;을 배워봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적으로는 이번 장 앞부분에서 배운 확률적 경사하강법을 사용합니다. 모든 가중치 $\mathbf{w}$를 학습할 때 한번에 1개의 데이터 $(\mathbf{x}_n, y_n)$을 사용하는데, 이 때의 오류를 $\mathbf{e}(\mathbf{w})$로 정의합니다. 확률적 경사하강법을 구현하기 위해서는 $\mathbf{e}(\mathbf{w})$의 기울기를 구해야 하는데, 모든 가중치의 값을 변화시켜야 하므로 모든 레이어 $l$, 모든 $i$, $j$를 잇는 뉴런에 대해서 수행해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;문제를 간단하게 접근하기 위해 $\nabla\mathbf{e}(\mathbf{w})$를 차근차근 분석해봅시다. 이 강의에서는 효율적인 계산을 위해 연쇄 법칙(Chain Rule)을 사용하였습니다. 오류와 가중치 간의 직접적인 기울기를 계산하는 것이 어렵기 때문에 오류 $\mathbf{e}(\mathbf{w})$와 로지스틱 함수를 거치기 전의 값인 $s^{(l)}_j$, $s^{(l)}_j$와 가중치 $w^{(l)}_{ij}$의 기울기를 각각 계산하는 방법을 이용합니다.&lt;/p&gt;

&lt;p&gt;연쇄 법칙을 이용하게 되면 문제가 약간 간단해집니다. 일단, 입력 $x^{(l-1)}_i$와 가중치 $w^{(l)}_{ij}$를 곱한 것이 $s^{(l)}_j$이므로, $s^{(l)}_j$를 $w^{(l)}_{ij}$로 편미분하게 되면 $x^{(l-1)}_i$만 남기 때문입니다. 아쉽게도 오류 $\mathbf{e}(\mathbf{w})$를 $s^{(l)}_j$로 편미분한 것은 간단하게 정리하지 못하지만, 표기만이라도 간단히 하기 위해 이를 $\delta^{(l)}_j$로 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 경사하강법에서 가중치를 계산할 때는 출력을 기준으로 계산했기 때문에, 이번에도 최종 출력인 마지막 레이어에서 먼저 $\delta^{(l)}_j$를 계산해봅시다. 마지막 레이어 $L$은 출력을 위해 단 1개의 뉴런만 존재하므로 $j$는 1이 됩니다. 이곳에서 $\mathbf{e}(\mathbf{w})$는 $\mathbf{e}(x^{(L)}_1, y_n)$이고, 이를 풀어쓰면 $(x^{(L)}_1 - y_n)^2$가 됩니다. $x^{(L)}_1$은 $s^{(L)}_1$가 쌍곡 탄젠트 함수 $\theta$를 거친 값이므로 최종적으로 $\theta$의 미분값을 계산하게 되면 $1-\theta^2(s)$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 레이어에서의 $\delta$를 계산했으니, 이제는 그보다 이전 레이어의 $\delta$를 구하기 위해 두 $\delta$간의 관계를 찾아보도록 하겠습니다. 이번에도 계산에는 연쇄 법칙을 사용하는데, 이전보다는 조금 더 복잡하게 2번을 사용하게 됩니다. 두 번째 단계에서 시그마가 갑자기 튀어나오는 것이 이해가 안 가실 수도 있는데, 다변수함수에서 편미분을 하게 되면 아래처럼 시그마가 나오게 됩니다.&lt;/p&gt;

\[\frac{\partial}{\partial t}f(x_1, x_2, \ldots, x_N) = \sum_{i=1}^{N}\frac{\partial f}{\partial x_i} \times \frac{\partial x_i}{\partial t}\]

&lt;p&gt;이렇게 연쇄 법칙으로 3덩이로 식을 나누게 되면, 각각의 덩이를 간단하게 표현할 수 있으므로 문제가 간단해집니다. 최종적으로는 맨 아래의 식과 같이 이전 레이어의 $\delta^{(l-1)}_i$와 이후 레이어의 $\delta^{(l)}_j$ 사이의 관계를 찾았으므로, 첫 번째 레이어의 $\delta$부터 마지막 레이어의 $\delta$까지 모두 계산할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 배운 내용을 알고리즘으로 정리하였습니다. 하나 특이한 점은, 가중치의 갱신을 위해서는 마지막 레이어부터 맨 앞의 레이어까지 반대방향(Backward)으로 계산하지만, 마지막 레이어에서 오류를 계산하기 위해서는 주어진 가중치로 첫번째 레이어부터 마지막 레이어까지 정방향(Forward)으로 계산해야 한다는 것입니다. 즉, 매 단계에서 앞으로 한번 계산하고, 뒤로 한번 계산하는 과정을 거쳐야만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Networks/ML 10-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 이전에 배운 비선형 변환(Nonlinear Transform)과 인공신경망 모델과는 어떤 차이가 있는지 알아봅시다. 비선형 변환과 인공신경망 모델 모두 선형 분리가 되지 않는 데이터를 제대로 분리하기 위한 방법입니다. 차이가 있다면 비선형 변환의 경우 직접 특정한 함수를 찾아 데이터를 다른 차원으로 변환하였지만, 인공신경망 모델에서는 가중치를 학습시킴으로써 데이터를 분류하는 것이므로, 학습된 비선형 변환으로 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 제대로 학습하기 위해 히든 레이어의 수나 뉴런의 수는 어떻게 조정해야 할까요? 그 문제는 다음 장에서 다루게 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Linear Model II</title><link href="http://localhost:4000/studies/linear-models-2/" rel="alternate" type="text/html" title="Linear Model II" /><published>2019-09-13T00:00:00+09:00</published><updated>2019-09-13T00:00:00+09:00</updated><id>http://localhost:4000/studies/linear-models-2</id><content type="html" xml:base="http://localhost:4000/studies/linear-models-2/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;9장은 3장에 이어서 선형 모델을 배우게 됩니다. 교재에서는 3장과 9장의 내용이 하나의 장으로 구성되어 있지만, 인터넷 강의에서는 이론적인 내용을 연속해서 다루기보다 중간에 구체적인 예시를 추가하기 위하여 두 장으로 나누었다고 합니다. 따라서 이번 장을 공부하기 전에 3장을 다시 한번 복습하시는 것을 추천합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3장에서 다루었던 것과 이번 장에서 다룰 것을 정리해봅시다. 3장에서 선형 분류(Linear Classification), 선형 회귀(Linear Regression)가 무엇인지 배웠고, 어떤 알고리즘을 사용해 학습하는지까지 간단히 소개하였습니다. (물론 3장에서 배운 학습 알고리즘이 전부는 아닙니다.)&lt;/p&gt;

&lt;p&gt;또한 3장 마지막에는 비선형 문제를 해결하는 방법인 Transform을 소개하였습니다. 다만 그 당시에는 간단하게 넘어갔기 때문에 이번 장에서 Nonlinear Transform을 먼저 더 공부하고, 이전에 다루지 않았던 Logistic Regression에 대해 본격적으로 공부하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-transforms&quot;&gt;Nonlinear transforms&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3장에서 배운 비선형 문제를 푸는 방법을 다시 한번 복습하고 넘어갑시다. 주어진 Input Data $\mathbf{x}$는 $d$차원의 벡터입니다. ($x_0$은 Threshold 역할을 하기 때문에 $d$에 포함시키지 않았습니다.) 그러나 $\mathbf{x}$의 분포가 선형 분리가 되지 않는 경우, 이를 선형 분리가 가능한 데이터로 만들어주기 위해 새로운 함수 $\Phi$를 정의하여 $\mathbf{z}$로 바꾸어 주었습니다. 이 때, $\mathbf{x}$와 차원이 달라질 수 있기 때문에 $\mathbf{z}$는 $\tilde{d}$차원을 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;여기서 주의할 점은, 변환을 하기 전과 변환을 하고 난 데이터의 Output은 동일하기 때문에, 굳이 변환한 학습 결과를 원래 차원으로 되돌릴 필요가 없다는 것입니다. 즉, 최종 가설 $g$는 선형 분류 문제의 경우 $\text{sign}(\tilde{\mathbf{w}}\Phi(\mathbf{x}))$, 선형 회귀 문제의 경우에는 $\tilde{\mathbf{w}}\Phi(\mathbf{x})$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 비선형 문제를 Transform을 통해 문제를 해결하게 되면 새로운 문제가 생기게 됩니다. 3장에서 이것을 처음 배울 때는 단순히 $\Phi$함수를 찾기가 쉽지 않다는 단점만을 생각할 수 있었지만, VC Dimension을 배우고 나니 Transform을 하게 되면 VC Dimension 또한 바뀌게 된다는 사실도 알게 되었습니다. 일반적으로 선형 데이터로 만들기 위해 Transform을 하는 것은 차원이 늘어날 수밖에 없기 때문에 VC Dimension 또한 늘어나게 됩니다. VC Dimension이 늘어난다는 것은 그만큼 일반화하기 힘들다는 말과 같습니다.&lt;/p&gt;

&lt;p&gt;원래의 데이터 $\mathbf{x}$의 VC Dimension은 Threshold를 포함하여 $d+1$인데, $\mathbf{z}$의 VC Dimension은 최대 $\tilde{d}+1$이 됩니다. 왜 정확히 $\tilde{d}+1$이 아니라 최대 $\tilde{d}+1$이 되냐면 $\mathbf{z}$는 $\mathbf{x}$를 통해 만들게 되기 때문입니다. 이전 슬라이드의 Example 처럼 $\mathbf{z}$ 벡터의 각 원소는 $\mathbf{x}$의 원소를 사용해 만들기 때문에, 각 원소가 독립적이지 않는 경우가 존재하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 비선형 예제 2개를 살펴보겠습니다. 첫번째 예제는 왼쪽 그림처럼 거의 선형이지만 선형으로 나눌 경우 약간의 에러가 발생하는 경우이고, 두번째 예제는 오른쪽 그림처럼 데이터들이 처음부터 비선형이라 선형으로는 절때 나눌 수 없는 경우입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 예제는 두 가지 선택을 할 수 있습니다. 약간의 In Sample Error를 감수하고 선형으로 나눌 것인지, 그 Error조차 용납하지 못하고 높은 차원으로 Transform을 시켜 In Sample Error를 0으로 만들게 할 수도 있습니다. 말할 필요도 없이 어떤 방법이 더 좋은지 우리는 이미 알고 있습니다. 굳이 VC Dimension을 따져보지 않더라도 Transform을 시켜서 푸는 방법이 일반화가 더 어렵기 때문에 더 좋지 않은 해결방법이라는 직관적인 판단을 하셨다면, 훌륭합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 예제는 Transform을 해야만 문제를 풀 수 있기 때문에 Transform을 한다고 가정합시다. 원래의 데이터는 $x_1, x_2$로 이루어진 2+1차원의 데이터였으나, 차수를 늘려 5+1차원의 데이터로 바뀌게 되었습니다. 그렇게 Transform을 하고 학습을 성공적으로 마쳤을 때의 상황이 바로 오른쪽 그림입니다. 가만히 생각해보면, 나눈 결과를 보아하니 원 모양인데, 원의 방정식에 필요한 2차항만 남겨서 차수를 줄일 수 있지 않을까하는 느낌이 들 수도 있습니다. $x_1^2, x_2^2$만 남기고 2+1차원을 데이터로 바꿀 수도 있고, 혹은 그보다 줄이는 방법도 생각해 볼 수 있습니다. 물론 초기 상태인 5+1차원과 동일한 결과가 나옵니다. 하지만 이 방법은 뭔가 이상하다는 직관적인 느낌이 듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 전의 두 번째 예제에서 차원을 줄이는 과정을 다시 한번 복기해봅시다. 먼저, 데이터를 Transform 시키고 학습이 끝난 다음 결과를 확인하였습니다. 그런데, 학습 결과를 보고 원과 비슷한 모양인 것을 확인한 후, &lt;strong&gt;의도적으로&lt;/strong&gt; Transform 시킨 데이터의 차원을 줄였습니다. 이 부분에서 잘못된 판단을 한 것입니다. 데이터를 눈으로 먼저 보고 모델을 고르는 것을 &lt;span style=&quot;color:red&quot;&gt;Data Snooping&lt;/span&gt;이라고 하는데, 이렇게 모델을 정할 경우 학습 결과가 일반화와는 거리가 멀어지기 때문에 Out of Sample Error에 큰 악영향을 주게 됩니다. 방금 전에 “원”의 모양을 눈으로 보고 그 모양이 나오도록 학습 결과를 의도적으로 수정했기 때문에 In Sample Error에는 영향이 없을 수도 있지만, 실제 Out of Sample Data가 어떻게 분포되어 있을지 모르는 상황이기 때문에 대부분의 경우에는 Out of Sample Error를 높게 만듭니다. 기계학습에서 가장 많이 행하게 되는 실수라고 하니 반드시 염두해 두도록 합시다.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 이번 장의 핵심 주제인 Logistic Regression을 공부해봅시다. 모델, 오류 측정 방법, 학습 알고리즘 순서로 배우게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear Model은 기본적으로 입력을 받고 출력값을 계산한다는 의미입니다. 이전에 배운 Linear Classification과 Linear Regression 모두 입력 $\mathbf{x}$와 가중치 $\mathbf{w}$를 받아 출력값을 계산하였습니다. 단순히 출력값을 그대로 내보내는가(Regression), +1/-1인지만 결정하는가(Classification)의 차이만 있었습니다. Logistic Regression도 이와 마찬가지로 입력 $\mathbf{x}$와 가중치 $\mathbf{w}$를 받아 출력값을 계산하는데, 그 출력값이 $\theta$ 함수를 거쳐 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;$\theta$ 함수는 다음 슬라이드에서 설명하도록 하고, 함수의 모양을 먼저 보면 S자를 눕힌 듯한 그래프를 가짐을 알 수 있습니다. 정확하게는, 최댓값은 1, 최소값은 0이 되고 그 사이를 부드럽게 올라가는 곡선입니다. Linear Classification은 Threshold를 기준으로 +1/-1로만 출력값이 나오지만, Logistic Regression은 0과 1 사이의 모든 실수가 출력값이 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로지스틱 함수 $\theta$의 명확한 정의는 ${e^s}/{(1+e^s)}$입니다. $s$가 크면 클수록 1에 가까워지게 되고, 작으면 작을수록 0에 가까워지게 됩니다. 특수한 경우로, $s$가 0이면 정확하게 0.5가 됩니다. 이 함수는 부드럽게 0과 1 사이를 움직이기 때문에 Soft Threshold로 부르기도 하지만, 일반적으로는 &lt;span style=&quot;color:red&quot;&gt;Sigmoid Function&lt;/span&gt;으로 불립니다. Sigmoid는 “S자 모양의” 라는 뜻입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;0과 1사이의 값을 갖는다는 것에서 눈치채신 분들도 계실텐데, 로지스틱 회귀는 바로 확률에 관한 문제를 다룰 때 쓰이게 됩니다. 이해를 돕기 위해 구체적인 예시로 심장 마비를 예측하는 문제가 있다고 가정해봅시다. 심장 마비에 영향을 줄 수 있는 요소인 콜레스트롤 수치, 나이, 몸무게 등은 입력 데이터로 들어가게 됩니다. 만약 Linear Classification으로 이 문제를 접근한다면 심장마비가 발생하지 않는다/발생한다 라는 결과만 출력할 수 있습니다. 그런데 정말 좋지 않은 신체적 상황이라고 해도 심장마비가 무조건 발생하지는 않습니다. 하지만 심장마비의 발생은 입력 데이터에 영향이 분명히 있기 때문에 이러한 상황에서는 확률적으로 접근해야 한다는 것입니다. 따라서 이 문제를 로지스틱 회귀로 풀게 된다면, $\theta$ 함수는 입력 $s$에 대해 심장 마비가 발생할 확률을 의미하게 됩니다. 이 때 입력 $s$는 이전의 선형 분류나 회귀와 마찬가지로 선형이고, 높으면 높을수록 심장 마비를 발생할 확률을 높게 만들기 때문에 “Risk Score”로 부를 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로지스틱 회귀에서 특이한 점은 학습 데이터입니다. 선형 분류나 회귀와 마찬가지로 지도학습으로 이루어지는데, 선형 분류에서는 데이터의 Output $y$가 +1/-1로, 선형 회귀에서는 실수 $\mathbb{R}$로 주어졌습니다. 그렇기에 로지스틱 회귀에서는 Output $y$가 0과 1 사이의 값으로 주어질 것이라고 생각할 수 있는데 그렇지 않는 것이 문제입니다. 로지스틱 회귀의 학습 데이터는 선형 분류와 마찬가지로 +1/-1로 주어집니다. 물론, 노이즈가 반영되기 때문에 같은 입력값에 대해 다른 Output을 갖고 있는 데이터가 포함됩니다.&lt;/p&gt;

&lt;p&gt;이전 슬라이드의 심장 마비 예측 문제를 생각하면, 완전히 동일한 신체조건을 가지고 있는 서로 다른 10명의 환자 데이터가 있다고 했을 때, 이 중 3명이 심장 마비가 발생하고 7명이 발생하지 않았다면 해당 신체조건(=$s$)에서 심장 마비가 발생할 확률이 0.3이다라고 예측하는 방식입니다. 물론 학습 알고리즘에 따라 0.3보다 작을수도, 클수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 로지스틱 회귀에서 Error Measure를 어떻게 하는지 배워봅시다. 선형 분류는 Output이 다른 것의 갯수를, 선형 회귀는 Squared Error를 사용했는데 로지스틱 회귀에서 Error Measure는 직관적으로 어떻게 해야 할지 떠오르지 않습니다.&lt;/p&gt;

&lt;p&gt;로지스틱 회귀에서는 &lt;span style=&quot;color:red&quot;&gt;Likelihood (가능도)&lt;/span&gt;를 Error Measure에 사용합니다. 가능도는 우도라고도 불리는데, 통계학에서 확률 분포가 확률 변수의 특정 값에 얼마나 일관적인지 그 정도를 나타내는 값입니다. 즉, 만약 가설 $h$와 Target Function $f$와 같다면, 가설 $h$에 의해 입력 데이터 $x$가 주어졌을 때 Output으로 $y$를 얻을 확률이 어느 정도인가? 를 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;개념을 이해했으니 구체적인 수식을 통해 계산하는 법을 알아봅시다. 먼저 가설 $h$는 로지스틱 회귀이므로 $\theta$ 함수와 같습니다. $\theta$ 함수의 특이한 점은 입력 $s$의 부호를 반대로 넣으면 $1-\theta(s)$와 같습니다. 직접 계산할 필요 없이 오른쪽의 함수 그림을 보시면 점선인 1에서 파란색 선을 빼 보시면 쉽게 알 수 있습니다. 이 성질을 이용하여 가능도 함수를 $P(y \mid \mathbf{x})=\theta(y\mathbf{w}^{\sf T}\mathbf{x})$로 간단하게 표현할 수 있습니다. 이를 이용하여 $N$개의 데이터가 주어져 있다고 가정할 때, 각 데이터에 대해 가능도 함수 $P$를 계산하여 곱하면 됩니다. 가능도 함수는 방금 구한 $\theta$ 함수와 같으니 $\theta$ 함수로 바꿔도 무방합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가능도는 인과관계를 의미하기 때문에 기존의 다른 Error Measure와 다르게 높을 수록 좋은 수치입니다. 그렇기에 다른 방법과의 통일성을 위해, 이전 슬라이드에서 도출한 식의 부호를 반대로 하고, 그 값을 최소화하는 방향으로 최적화를 진행할 것입니다. 식을 간단하게 정리하기 위해 마이너스 기호를 로그 안에 넣고, 파이를 로그 밖으로 빼어 시그마로 바꿉니다. 수식을 $N$으로 나눈 이유는 모든 주어진 데이터에 대해 평균적인 Error를 구하기 위함입니다.&lt;/p&gt;

&lt;p&gt;최적화를 수행하기 전에, $N$, $y$, $\mathbf{x}$는 모두 고정된 값이기 때문에 임의로 바꿀 수 없음에 유의하시기 바랍니다. 우리는 이 식에서 오로지 $\mathbf{w}$만 수정해야 합니다.&lt;/p&gt;

&lt;p&gt;먼저 $\theta$ 함수를 풀어서 표현합니다. 이전에 나온 $\theta$ 식과 조금 달라보이는데, 분모와 분자를 모두 $e^s$로 나눈 것 외에는 동일합니다. 식을 마지막까지 정리하게 되면 깔끔한 식이 나오게 되는데, 이 중 로그 부분을 따로 떼어 $e(h(\mathbf{x}_n, y_n)$으로 정의합니다. 이런식의 Error Measure를 &lt;span style=&quot;color:red&quot;&gt;Cross-Entropy Error&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 순서로는 로지스틱 회귀에서 어떤 학습 알고리즘을 사용할 것인지 배우게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 In Sample Error를 계산하는 식을 구했으니, 이를 최소화 시키는 방법을 알아야 합니다. 이전에 배운 선형 회귀의 경우 Pseudo-Inverse를 이용하여 여러 단계를 거치지 않고 한번에 최적의 가중치 $\mathbf{w}$를 계산하였습니다. 그러나 로지스틱 회귀에서는 불행하게도 로그 함수의 존재로 인해 한번에 쉽게 구하는 방법이 없습니다. 따라서 반복적인(Iterative) 방법으로 계산해야만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반복적인 방법 중 대표적인 것이 바로 &lt;span style=&quot;color:red&quot;&gt;Gradient Descent (경사하강법)&lt;/span&gt;입니다. 여기서 Gradient는 미적분학에서 배우는 기울기입니다. 위 슬라이드의 오른쪽 그림을 보시면 최종 목적지는 In Sample Error가 가장 낮게 나오는 점입니다. 이 점을 한번에 구할 수 없기 때문에 초기에는 무작위로 $\mathbf{w}$를 정합니다. 이 곳이 최종 목적지라면 좋겠지만 일반적인 경우에는 그렇지 않으니 이 점에서 다른 점으로 움직여야 합니다. 그 움직이는 방향을 가리키는 것이 단위벡터 $\widehat{\mathbf{v}}$입니다. 단위벡터 앞에 붙은 상수 $\eta$(에타)는 얼마나 움직일 것인지 그 크기를 나타냅니다. 그렇게 움직인 결과가 바로 새로운 $\mathbf{w}$가 되는 것입니다.&lt;/p&gt;

&lt;p&gt;방법을 알았으니 이제 방향을 결정하는 단위벡터 $\widehat{\mathbf{v}}$만 구하는 방법을 알면 됩니다. 이름에서 눈치채신 분들도 있지만, 이걸 구할때 바로 기울기(Gradient)를 이용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저, 이전 슬라이드에서 구한 $\mathbf{w}(1)$과 초기값 $\mathbf{w}(0)$을 비교해봅시다. $\mathbf{w}(1)$을 넣어서 In Sample Error를 계산하고, $\mathbf{w}(0)$을 넣어서 In Sample Error를 계산한 후, $\mathbf{w}(1)$로 계산한 In Sample Error에서 $\mathbf{w}(0)$로 계산한 In Sample Error를 빼면 In Sample Error의 차이($\Delta E_{in}$)를 계산할 수 있습니다. 우리가 원하는 것은 반복적인 과정을 거칠 때마다 In Sample Error가 줄어드는 것을 원하므로 이 변화가 음수가 나와야 합니다.&lt;/p&gt;

&lt;p&gt;이 차이를 계산하기 위하여 테일러 급수를 사용해야 합니다. 테일러 급수에서 맨 앞 항만 사용하고 나머지 부분은 $O(\eta^2)$로 묶었습니다. 사실 중요한 것은 맨 앞부분에 있는 $\widehat{\mathbf{v}}$ 이기 때문에 식을 간략화한 후 정규화(Normalization)를 시키면, $\mathbf{w}(0)$에서의 기울기 단위 벡터를 얻을 수 있습니다. 왜 $\widehat{\mathbf{v}}$에 마이너스(-) 가 붙는 지 궁금하실 수도 있는데, 쉽게 이해하기 위해서는 이전 슬라이드의 그림을 떠올리시면 됩니다. 그림처럼 점이 최소값의 왼쪽에 있을 때는 오른쪽으로 점을 움직여야 하는데, 현재의 기울기가 음수이므로, 양의 방향으로 움직여야 하기 때문입니다. 즉, 움직여야 하는 방향은 현재 기울기의 반대 방향임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 다음으로 논의할 것은 $\eta$입니다. 처음에는 $\eta$를 고정된 값을 사용한다고 했는데, 만약에 이 $\eta$가 너무 작으면 최종 목적지에 도달하는데 너무 많은 단계를 거쳐야 하는 문제가 생길 수 있습니다. 그렇다고 이 값을 크게 만들다가 너무 커지게 되면, 최종 목적지를 지나칠 수도 있고, 그 주변에서 왔다갔다 하느라 목적지에 도달하지 못할 수도 있습니다. 따라서 이런 단점들을 해결하기 위해, $\eta$를 유동적인 값을 사용하는 것이 좋습니다. 초기에는 큰 값을 부여해 목적지에 빠르게 가깝게 다가가고, 나중에는 작은 값을 부여해 목적지를 지나치지 않고 쉽게 다가갈 수 있도록 만드는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 유동적인 $\eta$를 어떻게 구현해야 할지 구체적으로 계산해봅시다. 이 강의에서는 $\Delta\mathbf{w}$의 식을 응용하여 $\eta$를 정의하였습니다. 바로 기존의 $\eta$값을 $\lVert \triangledown E_{in}(\mathbf{w}(0)) \rVert$에 비례하게 만들어 $\widehat{\mathbf{v}}$의 분모를 약분할 수 있게 만든 다음, 새로운 고정된 값인 $\eta$로 정의하는 것입니다. $\eta$ 기준에서 보면 초기에는 기울기가 가파르기 때문에 높은 값을 가지지만 목적지에 도달할수록 기울기가 완만해지기 때문에 낮은 값을 갖게 됩니다. 굉장히 기발한 아이디어라고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 필요한 것을 모두 배웠으니 이를 정리하여 하나의 알고리즘으로 통합해 봅시다. 먼저 무작위로 $\mathbf{w}(0)$을 정하고, 기울기를 계산하여 다음 단계의 $\mathbf{w}$를 계산합니다. 이 과정을 최종 목적지(In Sample Error가 가장 낮은 지점)에 도달할 때까지 반복하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로, 지금까지 배운 선형 모델을 간단하게 정리해봅시다. 선형 모델에는 선형 분류, 선형 회귀, 로지스틱 회귀 3가지로 나뉩니다. 3장에서 배운 신용 카드 발급 문제를 예로 들면 선형 분류는 신용 카드를 발급할지/거부할지를 결정하는 모델이고, 선형 회귀는 신용 카드를 발급한다면 한도를 얼마로 정할 것인지를 결정하는 모델이며, 로지스틱 회귀는 이 사람이 파산할 확률(즉, 신용카드 대금을 갚지 못할 확률)을 계산하는 모델입니다. 세 모델의 목적이 모두 다르기 때문에 오류 측정 방법도 모두 다릅니다. 선형 분류는 얼마나 분류가 틀렸는지(Classification Error), 선형 회귀는 정답과 얼마나 멀리 떨어져 있는지(Squared Error), 마지막으로 로지스틱 회귀는 입력과 출력이 얼마나 관련이 있는지(Cross-Entropy Error)를 기준으로 삼게 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry></feed>