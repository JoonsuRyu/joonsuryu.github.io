<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-24T17:25:50+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">Regularization</title><link href="http://localhost:4000/studies/regularization/" rel="alternate" type="text/html" title="Regularization" /><published>2019-10-05T00:00:00+09:00</published><updated>2019-10-05T00:00:00+09:00</updated><id>http://localhost:4000/studies/regularization</id><content type="html" xml:base="http://localhost:4000/studies/regularization/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;12장은 지난 시간에 배웠던 문제점인 Overfitting을 해결하는 방법 중 Regularization (정규화)에 대해 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장에서는 먼저 직관적인 접근 방식으로 정규화가 무엇인지 알아보고, 그 후에 수학적인 방법으로 정규화가 정확하게 무엇인지 알아봅니다. 그 후 정규화에서 중요한 Weight Decay가 무엇인지 배운 다음, Regularizer를 선택하는 방법을 공부하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;regularization---informal&quot;&gt;Regularization - informal&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정규화에 접근하는 방식은 두 가지 방법이 있습니다. 먼저 수학적으로 문제점을 분석해 해결하는 방법을 유도하는 방법이 있고, 다른 방법으로는 단지 $E_{in}$을 최소화하는 것을 방해하는 요소가 무엇이었는지를 따져가며 해결하는 방법을 찾는 방법입니다. 여기서는 두 번째 방법을 먼저 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 8장에서 사용했던 예제입니다. Target Function이 $y=\sin(\pi x)$이고 가설 모델이 1차 함수인 상황에서 무작위로 2개의 데이터가 주어졌을 때 발생하는 가설을 그린 것입니다. 이 당시에는 Variance가 너무 컸기 때문에 상수 함수보다 나쁜 가설 모델임을 밝혔습니다. 그런데 오른쪽 그림처럼 이 가설 모델에 정규화를 도입하고 나니 가설들이 발생 분포가 조금 작아진 것처럼 보입니다. 여기서는 가설의 기울기에 제한을 두어 기울기가 너무 가파른 직선은 가설에서 제외하는 방법을 사용하였다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예상대로 정규화를 거친 경우 그렇지 않은 경우에 비해 성능이 좋아진 것을 볼 수 있습니다. 빨간색 선의 위치는 동일한 것처럼 보이지만, 실제로는 모델의 분포가 줄어들었기 때문에 살짝 틀어져 Bias는 약간 높게 나오지만, Target Function의 범위를 초과하는 영역은 모두 사라졌기 때문에 Variance는 급격하게 줄어들었습니다.&lt;/p&gt;

&lt;h2 id=&quot;regularization---formal&quot;&gt;Regularization - formal&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 이런 정규화를 수학적으로 유도해볼 차례입니다. 이번에도 예를 들어 설명할텐데, 조금 복잡한 방법으로 Legendre Polynomials (르장드르 다항식)을 사용한 Transform을 예제로 다루게 됩니다.&lt;/p&gt;

&lt;p&gt;Legendre Polynomial은 Legendre Differential Equation (르장드르 미분 방정식)의 해가 되는 함수들을 일컫는데, 여기서는 구체적으로 Legendre Polynomial이 무엇인가까지 알 필요는 없습니다.&lt;/p&gt;

&lt;p&gt;다만 Legendre Polynomial은 최고차항이 1, 2, 3, … 일 때 각각 고유한 함수를 갖는데, 그 고유한 함수를 각각 $L_1, L_2, L_3, …$로 정의합니다. 위의 슬라이드에서는 $L_1$부터 $L_5$까지의 르장드르 다항식이 어떤 함수인지를 나타내고 있습니다.&lt;/p&gt;

&lt;p&gt;가설 모델 $\mathcal{H}_Q$는 최고차항이 $Q$인 다항식이라 가정합니다. 비선형 변환 $\mathbf{z}$는 1(=$L_0$)부터 $L_Q$까지를 원소로 갖는 벡터입니다. 이것을 사용하여 구체적인 $\mathcal{H}_Q$를 유도하면 각 Legendre Polynomial에 Weight $w_q$를 곱한 후 더한 형태로 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 주어진 데이터 $(x_1, y_1), … ,(x_N, y_n)$을 Transform을 통해 $(\mathbf{z}_1, y_1),$ $…, (\mathbf{z}_N, y_n)$를 만들면 $\mathbf{z}$에 대한 새로운 In Sample Error $E_{in}$을 유도할 수 있습니다. 이후의 전개는 3장에서 배운 Pseudo Inverse (유사 역행렬)을 구하시면 됩니다.&lt;/p&gt;

&lt;p&gt;즉, 임의의 일반적인 데이터 $\mathbf{x}$가 주어진다면, 이를 Legendre Polynomial을 벡터로 갖는 $\mathbf{z}$로 Transform을 하고 Pseudo Inverse를 통해 가중치 벡터 $\mathbf{w}$를 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 가중치 벡터가 제한된 상황에서는 어떻게 변하는지 알아봅시다. 지난 시간에 가설 모델 중 2차 방정식 모델 $\mathcal{H}_2$와 10차 방정식 모델 $\mathcal{H}_{10}$을 비교하였습니다. 그런데 생각해보면, $\mathcal{H}_2$는 $\mathcal{H}_{10}$의 제한된 버전, 즉 3차 이상의 항의 계수를 0으로 한 모델임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 것과 비슷하게 각 가중치를 제곱한 합을 일정 값 $C$ 이하로 제한하는 방법을 생각해볼 수 있습니다. 제곱이 되는 이유는 In Sample Error를 계산하는 식에 가중치 벡터 $\mathbf{w}$의 제곱이 들어가기 때문입니다. 이 제한이 들어갔을 때 나오는 해답을 이전의 해답과 비교하기 위해 $\mathbf{w}_{reg}$라고 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;제한 조건이 없는 In Sample Error $E_{in}$을 상수값과 같다고 식을 놓고 2차원으로 표현하면 오른쪽과 같은 파란색 타원으로 표현할 수 있습니다. 원의 중심으로 갈수록 $E_{in}$의 값이 낮아지기 때문에 제한조건이 없을 때의 해답 $\mathbf{w}_{lin}$은 원의 중심이 됩니다. 마찬가지로 제한 조건만을 2차원으로 표현한다면 빨간색 원으로 그릴 수 있습니다. 제한조건을 만족하면서 $E_{in}$을 최소화하는 점은 파란색 타원과 빨간색 원이 겹치는 영역에 포함됨을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;$E_{in}$을 최소로 만드는 점을 찾기 전에, 영역에 포함되는 점 중 하나인 $\mathbf{w}$를 하나 가정해봅시다. 이 점에서의 기울기(Gradient)를 구하게 되면 파란색 원의 바깥쪽을 향하게 됩니다. (경사하강법에서 목적지에 도달하기 위해 기울기의 반대 방향으로 갔던 것을 떠올리면 됩니다) 그리고 빨간색 원을 기준으로 $\mathbf{w}$에서 법선 벡터(Normal Vector)를 구하게 되면 역시 원 바깥으로 향하는 벡터가 생성됨을 쉽게 알 수 있습니다. 그렇다면 여기서 $E_{in}$을 최소화하려면 $\mathbf{w}$를 어느 방향으로 움직여야 할까요? 당연히 빨간색 원을 따라 움직여서 $\mathbf{w}_{lin}$에 최대한 가까운 점으로 움직이면 될 것입니다. 그 방향을 $\mathbf{w}$를 기준으로 표현한다면, $E_{in}$ 기울기의 반대 방향과 제한조건의 법선벡터 반대 방향으로 움직여야 함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이를 수학적으로 표현하면 $E_{in}$의 변화량 $\nabla E_{in}(\mathbf{w}_{reg})$은 $-\mathbf{w}_{reg}$에 비례한다고 말할 수 있습니다. 정확한 값으로는 $-2 \times \frac{\lambda}{N}\mathbf{w}_{reg}$로 표현하는데, 왜 갑자기 이 식이 나왔는지는 강의를 들어도 잘 이해가 되지 않습니다. 알게 되면 글을 수정하겠습니다. 혹시 아시는 분은 댓글로 알려주시면 감사하겠습니다.&lt;/p&gt;

&lt;p&gt;어쨌든 $\nabla E_{in}(\mathbf{w}_{reg})$의 오른쪽 항을 이항하면 오른쪽 항에는 0벡터만 남게 됩니다. 그런데 이 식은 어떤 식을 미분한 식처럼 보입니다. 쉽게 말해, $E_{in}(\mathbf{w}) + \frac{\lambda}{N}\mathbf{w}^{\sf T}\mathbf{w}$을 최소화하기 위하여 이를 미분했을 때의 결과로 보입니다.&lt;/p&gt;

&lt;p&gt;여기서 문제는 제한이 들어갔을 때의 상수 $C$가 이 식에는 사라져 있다는 것입니다. 대신 다른 상수 $\lambda$가 존재합니다. 다행히도 이 두 상수간에는 관계가 존재하는데, $C$가 증가할수록 $\lambda$가 줄어든다는 것이고 그 반대도 성립한다는 겁니다. 즉, 우리가 $C$의 값을 조절하고 싶다면 그 대신 $\lambda$를 조절함으로써 동일한 결과를 얻을 수 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 Augmented Error가 무엇인지 살펴보겠습니다. 이전 슬라이드에서 유도한 $E_{in}(\mathbf{w}) + \frac{\lambda}{N}\mathbf{w}^{\sf T}\mathbf{w}$식을 간단하게 $E_{aug}(\mathbf{w})$로 정의합니다. 여기서 $E_{in}$을 우리가 아는 식으로 대입하고, 그 식을 정리하면 이전 슬라이드에서 처음 시작했던 식과 동일한 결과가 나옵니다. (전개 과정은 추후에 글을 수정함으로써 보충하도록 하겠습니다)&lt;/p&gt;

&lt;p&gt;이 식은 가설 집합을 명시적으로 제한하고 있기 때문에 7장에서 배웠던 VC Analysis에 적합하다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 정의했던 $E_{aug}(\mathbf{w})$을 최소화하는 계산을 시도해봅시다. 식을 정리하여 미분값이 0이 되는 점을 찾으려면 $Z^{\sf T}(Z\mathbf{w}-\mathbf{y})+\lambda\mathbf{w})=\mathbf{0}$을 계산하면 됩니다. 이를 정리하여 $\mathbf{w}$를 계산하면 $(Z^{\sf T}Z + \lambda\mathbf{I})^{-1}Z^{\sf T}\mathbf{y}$을 됩니다. 정규화를 적용하기 전에 구했던 $\mathbf{w}_{lin}$과는 $\lambda\mathbf{I}$ 만큼의 차이가 있다는 것이 쉽게 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이번에는 $\lambda$의 값을 변화시킴으로써 학습 결과가 어떻게 변하는 지 알아보겠습니다. $\lambda=0$일 때는 정규화를 사용하지 않는 것과 같으므로 Overfitting이 일어납니다. 그런데 $\lambda$를 0.0001로 아주 약간만 올리게 되면 Target Function과 거의 비슷한 결과가 나오게 됩니다. $\lambda$를 조금 더 증가시켜 0.01로 설정하면 오히려 이전보다 Target Function과 더 멀어지지만, 그래도 Overfitting이 일어났을 때보다는 결과가 좋습니다. $\lambda$를 1까지 증가시키게 되면 오히려 학습을 제대로 하지 못하는 Underfitting이 일어나버립니다. $\lambda$가 커질수록 학습 결과는 점점 더 평평해지고, 너무 크게 정한다면 Underfitting이라는 새로운 문제가 생기기 때문에 적절한 $\lambda$의 값을 찾는 것이 중요하지만, 안타깝게도 최선의 $\lambda$를 찾을 수 있는 방법이 존재하지 않기 때문에 아주 작은 값부터 값을 올려가며 $\lambda$을 찾는 경험적인 방법을 사용할 수밖에 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 설명했던 방법을 Weight Decay라고 부릅니다. 왜 그런지 이유를 알아보기 위해 인공신경망 모델에서 경사하강법을 사용하는 상황을 가정해봅시다. 원래의 경사하강법의 수식에서 정규화를 적용한다면 $-2\eta\frac{\lambda}{N}\mathbf{w}(t)$ 항이 추가됩니다. 이 항이 포함된 식을 정리한다면 $\mathbf{w}(t)$가 $(1-2\eta\frac{\lambda}{N})$ 로 묶이게 됩니다. 즉, $t$가 한 단계 증가할 때마다 $\mathbf{w}(t)$는 조금씩 작아지게 됩니다. 이것이 바로 Weight Decay라고 불리게 되는 이유입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay의 변형에는 어떤 것들이 있는지 알아보겠습니다. 가중치 벡터의 각 Element $w_q^2$를 제곱한 값에 $\gamma_q$를 곱해준 것을 Regularizer라고 가정해봅시다. 여기서 $\gamma_q$를 어떻게 정하느냐에 따라 가중치 벡터의 Element의 크기를 정해줄 수 있습니다. 시그마 수식의 전체 총합은 일정한 상수 $C$ 보다 작아야 하기 때문인데요, 만약에 $\gamma_q$가 크다면 제한 사항을 만족해야 하기 때문에 가중치 벡터의 Element들은 크기가 작을 수밖에 없고, 반대로 $\gamma_q$가 작다면 Element를 그보다 크게 정할 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;여기에서는 두 가지 반대 케이스를 보여주는데, $\gamma_q=2^q$인 경우에는 함수의 차수를 작게 맞추려고 합니다. 예를 들어 가설 모델이 고차 다항식인 경우에는 낮은 차수의 다항식과 맞추려는 역할을 하게 된다는 뜻입니다. 반대로 $\gamma_q=2^{-q}$인 경우라면 좀 더 고차인 다항식에 맞추려고 하기 때문에 Target Function이 복잡한 함수로 예상되는 경우에 좋은 퍼포먼스를 보일 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 각 레이어마다 다른 $\gamma$의 값을 정해준다고 합니다. 가장 유명한 식은 Tikhonov Regularizer라는 것인데, 처음 주어졌던 식과는 다르게 완전한 2차식으로 표현됩니다. 그냥 이런 것이 있구나 정도만 이해하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay를 배웠으니 그 반대의 경우도 떠올려볼 수 있습니다. 만약에 Weight를 반대로 크게 만든다면 어떻게 될까요?&lt;/p&gt;

&lt;p&gt;우선 Weight Decay의 효과를 알기 위해 오른쪽 그림처럼 $\lambda$의 크기를 증가시킬 때 예상되는 Out of Sampel Error $E_{out}$이 어떻게 변하는지 보면, 일정 지점까지는 $\lambda$를 증가시킬 때 $E_{out}$이 감소하지만, 그 이후부터는 오히려 증가함을 알 수 있습니다. 만약에 Weight growth를 시키는 상황에서 $\lambda$를 증가시킨다면? 그냥 $E_{out}$이 바로 수직 상승해 버립니다. 이렇게 되면 Weight Growth를 하게 되면 엄청나게 큰 문제가 생길 것 같지만, 실제로 계산을 해보면 Weight Growth에서는 $\lambda$를 0으로 유도하기 때문에 계산의 낭비만을 제외하고는 정규화를 사용하지 않는 것과 차이는 없다고 합니다.&lt;/p&gt;

&lt;p&gt;또한 여기서 실용적인 규칙을 하나 알려주는데, Stochastic Noise는 High-frequency이고 Deterministic Noise는 Non-smooth라고 합니다. (강의에서 정말 간단하게 알려주고 넘어가는데, 저는 강의만 보고서는 이게 무슨 의미인지는 알지 못하겠습니다. 혹시라도 아시는 분은 댓글로 부탁드립니다.) Regularize는 더 Smooth 한 가설을 선택하는 경향이 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight Decay의 Regularizer를 $\Omega$라고 정의해 봅시다. $E_{aug}$는 $\Omega$의 형태로 표현할 수 있습니다. 여기서 $h$는 $\mathbf{w}$에 영향을 주는 변수라고 생각하시면 됩니다. 그런데 이 식을 조금만 변형시키면, 7장에서 보았던 Generalization Bound와 유사함을 알 수 있습니다. 물론 이 때는 $E_{aug}$가 아니라 $E_{out}$이었습니다. 그다음 문장이 조금 이해하기 어려운데, $E_{aug}$는 $E_{out}$에 대한 대용물로써 $E_{in}$보다 낫다고 합니다. 그 이유를 인터넷 강의에서 자세하게 설명해주지 않아서 이 부분은 추후에 공부한 후 보강하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;choosing-a-regularizer&quot;&gt;Choosing a regularizer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 문제별로 어떻게 빠르게 Regularizer를 선택하는지 그 방법을 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완벽한 Regularizer가 무엇일까요? Target Function은 알 수 없지만 그 방향으로 갈 수 있게 만드는, 즉 Target Function과 최대한 가까워질 수 있게 만드는 Regularizer일 것입니다. Regularization은 기본적으로 Overfitting을 완화시키는 목적으로 사용되기에, Noise를 손상시키게 됩니다. Perfect Regularizer는 Closed Form을 구하는 것이 아닌, 경험적인 방법으로 유도하게 됩니다. 여기에서는 가이드라인으로 Smooth 하거나 간단한 방향으로 움직여야 한다고 말합니다. 그 이유는 Noise가 Smooth 하지 않기 때문입니다. 그 방향으로 가게 되면 Noise를 더 손상시킬 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;만약에 나쁜 $\Omega$를 선택한다면 어떻게 될까요? 다행히도 $\lambda$의 값을 잘 조절한다면 나쁜 $\Omega$를 선택한다고 해도 Overfitting을 해결할 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 특별한 예제로 인공신경망 모델에서의 Regularizer를 직관적으로 선택해보겠습니다. 인공신경망 모델에서 큰 가중치와 작은 가중치를 사용하는 경우를 각각 따져봅시다. 만약에 매우 작은 가중치를 선택한다면, 가중치는 원점 근처, 즉 Linear 함수와 다르지 않은 부분에 분포해 있을 것입니다. 결과적으로 매우 작은 가중치에서는 간단한 선형 함수를 구현하는 것입니다. 반대로 가중치가 큰 경우라면 선형 부분을 넘어 Logical 한 부분으로 가게 됩니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 Overfitting을 해결하는 방법 중 하나로 Weight Elimination이 있습니다. 이름 그대로 몇 개의 Weight를 아예 0으로 만들어 버리는 방법입니다. Weight의 수가 적어진다면 그만큼 VC Dimension이 줄어들기 때문입니다. 다만 여기서는 완전히 0으로 만들어버리기보다는 Soft 한 방법을 제시하고 있는데, $\gamma$를 $\frac{\beta^2}{(w_{ij}^{(l)})^2}$로 정의하는 것입니다. 이렇게 되면 $w$가 작은 경우에는 $\frac{1}{\beta^2}$에 가깝게 될 것이고 $w$가 큰 경우라면 1에 가깝게 될 것입니다. 결과적으로 덜 중요한 Weight들은 0에 가깝게 되지만 중요한 Weight들은 그 값이 그대로 유지되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지난 장에서 살짝 언급하였던 Early Stopping 또한 Regularizer의 한 형태입니다. 이 방법은 특이하게 목적 함수를 변화시키지 않습니다. 학습을 언제 멈춰야 할지를 선택하는 방식이기 때문입니다. 이런 해결 방법을 Validation이라고 부르는데, 이것은 다음 장에서 더 자세하게 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/12. Regularization/ML 12-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 최적의 $\lambda$를 찾는 방법을 알아봅시다. 왼쪽의 그래프는 Stocahstic Noise가 정규분포를 따른다고 가정했을 때 Variation 별 $\lambda$와 예상되는 Out of Sample Error를 나타낸 것입니다. 만약에 Variation이 0이라면(=Stochastic Noise가 없다면) 정규화 자체를 할 필요가 없음을 알 수 있습니다. Variation이 0.25일 때와 0.5일 때를 비교하면 Variation이 클수록 $\lambda$의 값 또한 커져야 함이 보입니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그래프는 Deterministic Noise인 Target Function의 차수에 따른 $\lambda$와 Out of Sample Error의 상관관계를 나타내고 있습니다. 이 역시 Stochastic Noise 때와 비슷하게, 차수가 높을수록 높은 $\lambda$값으로 정해야 함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Overfitting</title><link href="http://localhost:4000/studies/overfitting/" rel="alternate" type="text/html" title="Overfitting" /><published>2019-09-28T00:00:00+09:00</published><updated>2019-09-28T00:00:00+09:00</updated><id>http://localhost:4000/studies/overfitting</id><content type="html" xml:base="http://localhost:4000/studies/overfitting/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11장은 Overfitting에 대해 배우게 됩니다. 단어의 뜻만 봐도 이게 무엇을 의미하는지 대충 알 수 있습니다. 예를 들면 옷 가게에서 Fitting Room은 옷이 자기한테 잘 맞는지를 확인하는 곳입니다. Fit의 의미는 &lt;strong&gt;맞다&lt;/strong&gt;이고 Over는 무언가 과다한 상태의 형용사이기 때문에, Overfit은 &lt;strong&gt;과다하게 맞다&lt;/strong&gt;라고 해석이 가능합니다. 기계학습에서 Overfitting도 이와 비슷한 의미입니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 4개 부분으로 나뉘어 있습니다. 먼저 Overfitting이 무엇인지부터 배우고, 4장에서 배웠던 Noise가 무슨 역할을 하는지 알아본 뒤, Deterministic Noise를 배우고 마지막으로 Overfitting 문제를 해결하기 위한 방법을 알아봅니다.&lt;/p&gt;

&lt;h2 id=&quot;what-is-overfitting&quot;&gt;What is overfitting?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Overfitting이 일어나는 상황부터 한번 살펴보겠습니다. 간단한 예제로, 2차함수와 비슷하게 생긴 Target Function (파란색 선)이 있습니다. 데이터는 물론 이 Target Function으로부터 나오는데, 이 데이터가 완벽하게 Target Function 위에 있지 않고 어느정도의 Noise가 끼어 있는 상태로 주어집니다. 5개의 점이 주어졌으니, 이를 완벽하게 커버하기 위해서는 4차함수가 필요합니다. 그렇게 해서 4차함수를 사용해 In Sample Error가 0이 되도록 함수를 구한다면, 주어진 데이터는 완벽하게 커버할지 몰라도, Out of Sample Error는 엄청나게 커지는 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 “Overfitting”과 “Bad Generalization”의 차이가 무엇인지 알아봅시다. Noise가 있는 데이터를 인공신경망 모델로 학습했을 때의 In Sample Error와 Out of Sample Error를 구합니다. 가로축은 학습횟수를 나타내고 세로축은 에러를 나타냅니다.&lt;/p&gt;

&lt;p&gt;가로축의 0과 1000 사이를 보시면 In Sample Error와 Out of Sample Error 모두 에러가 매우 높게 나온 것이 보입니다. 이렇게 두 Error가 모두 높은 상황은 아직 모델이 데이터를 커버하지 못하는, 즉 일반화가 나쁜 상황이 됩니다.&lt;/p&gt;

&lt;p&gt;반대로 가로축이 6000을 넘긴 시점을 보시면, 학습을 하면 할수록 In Sample Error가 줄어드는데, Out of Sample Error는 반대로 증가하는 모습을 보입니다. 이 때는 모델이 Sample 데이터에 과도하게 맞추고 있다는 것을 보여주므로, 이런 상황을 &lt;span style=&quot;color:red&quot;&gt;Overfitting&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Overfitting이 일어나는 이유는 데이터를 보장된 만큼보다 더 맞추려고 하기 때문입니다. 이 말은, 데이터에 Noise가 있다면 그것까지 굳이 맞추려고 할 필요가 없는데도 불구하고 완벽하게 맞추려고 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;슬라이드 3을 예로 들면 데이터에 Noise가 있는 것을 알았을 시, In Sample Error가 어느정도 발생하는 것을 감수하고 2차함수로 모델링을 한다면 Overfitting이 일어나지 않지만, Noise까지 완벽하게 맞추려고(fit) 4차함수로 모델링을 하기 때문에 Overfitting이 발생하는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-role-of-noise&quot;&gt;The role of noise&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;조금 더 복잡한 상황에서 살펴봅시다. 왼쪽의 예제는 Target Function이 10차 방정식인데, 주어진 데이터에 Noise가 끼어있는 상황이고, 오른쪽의 예제는 Target Function이 50차 방정식인데, 주어진 데이터에 Noise는 없는 상황입니다. 데이터는 두 모델 각각 15개씩 주어졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 두 예제는 2가지의 모델로 해결해 봅시다. 하나는 2차 방정식 모델이고, 다른 하나는 10차 방정식 모델입니다. 각 예제의 그림에서 초록색 선이 2차 방정식 모델로 해결한 결과이고, 빨간색 선이 10차 방정식 모델로 해결한 결과입니다. 놀랍게도 2차 방정식 모델이 두 예제에서 모두 10차 방정식 모델보다 훨씬 좋은 결과가 나왔습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드의 결과를 보면 의문이 듭니다. 첫번째 예제의 Target Function은 10차 방정식이였기 때문에 학습 모델을 10차 방정식으로 잡는 것은 상식적으로 생각해봤을때 전혀 과도하게 잡은 모델이 아닙니다. 오히려 2차 방정식으로 모델을 잡은 것이 너무 낮은 차수로 잡은 것이 아닌가 하는 생각을 들게 만들죠. 그렇다면 왜 이러한 결과가 나온 것일까요?&lt;/p&gt;

&lt;p&gt;그 이유는 바로 데이터의 수가 적었기 때문입니다. 7장에서 배웠듯이, 올바른 학습을 위해서는 VC Dimension의 10배 만큼의 데이터가 필요하다고 하였습니다. 물론 2차 함수 모델의 VC Dimension은 3이므로 30개가 필요한데 비해 데이터는 그것의 절반밖에는 주어지지 않았지만, 10차 함수 모델의 필요한 데이터의 수인 110개보다는 가까웠기 때문에 더 좋은 성능이 나온 것입니다. 즉, 학습 모델을 잡을 때는 Target Function 보다 주어진 데이터의 수에 맞추는 것이 훨씬 더 좋은 성능이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리는 이미 이 사실을 이전에 배웠습니다. 데이터의 수가 충분하다면 물론 Target Function의 차수와 가까운 10차 방정식 모델 $\mathcal{H}_{10}$이 $\mathcal{H}_{2}$보다 성능이 좋게 나올 것입니다. 그러나 주어진 상황에서는 데이터가 매우 적은 상황이었기 때문에 회색 영역이 현재의 상황을 나타내는 부분이 되고, 데이터가 적은 상황에서도 In Sample Error와 Out of Sample Error의 간격이 적은 2차 방정식 모델이 그렇지 않은 10차 방정식 모델 보다 &lt;strong&gt;아직은&lt;/strong&gt; 좋은 퍼포먼스를 보이게 되는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Target Function이 50차 방정식이고 Noise가 없었던 두 번째 예제로 넘어가 봅시다. Noise가 없는 상황이기 때문에 Overfitting은 일어나지 않을 것이라 생각되어 복잡한 모델인 $\mathcal{H}_{10}$이 이길 수 있을 것처럼 보이지만, 실제로는 여전히 $\mathcal{H}_{2}$가 더 나은 성능을 보여주고 있습니다. 왜 그렇게 되는지 알아보기 전에, 과연 이런 상황을 Noise가 정말로 없다고 부를 수 있을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 질문에 답하기 위해 좀 더 구체적인 실험을 해봅시다. Input을 간단하게 1차원 $x$라 하고 Output을 $y$라고 한다면 Noise가 없는 Input과 Output의 관계는 $y=f(x)$가 됩니다. 여기에 Noise Function $\epsilon(x)$를 더해주면, Noise가 추가된 Output $y$의 식이 됩니다. Noise는 일반적으로 가우시안 분포를 따르기 때문에 Noise의 정도를 나타내기 위해 이를 $\sigma^2$으로 표현합니다.&lt;/p&gt;

&lt;p&gt;문제를 구체적으로 하기 위해 Target Function은 다항함수라고 가정하고, 최대 차수를 $Q_f$라 정의합니다. 그렇게 되면 $x$의 각 항에 계수가 붙는 다항함수 꼴을 시그마로 표현할 수 있습니다. 그리고 데이터의 수를 $N$으로 정의합니다.&lt;/p&gt;

&lt;p&gt;방금 정의한 기호를 사용하여 하단 왼쪽에 있는 예제를 예로 들면, $Q_f=10$인 Target Function에서 $N=15$개의 데이터가 $\sigma^2$의 Noise를 따르도록 나타낸 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 가지고 Overfit을 측정해봅시다. $N$개의 데이터 $(x_1, y_1), \ldots, (x_N, y_N)$를 사용하여 2개의 모델 $\mathcal{H}_{2}$과 $\mathcal{H}_{10}$로 학습한 결과를 비교해야 합니다. 비교는 각각의 모델에서 가설을 가지고 Out of Sample을 계산한 뒤, 그 차이를 구하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 슬라이드는 그 결과를 표현하고 있습니다. 이 그래프는 무지개색으로 그 값이 어느정도인지를 표현하고 있는데, 색이 빨간색에 가까울 수록 Overfit Measure가 커지고(즉, $E_{out}(g_{10}) \gg E_{out}(g_2)$) 파란색에 가까울수록 Overfit Measure가 작아지는(즉, $E_{out}(g_{10}) \ll E_{out}(g_2)$) 상황입니다. 초록색인 상황은 두 가설에서의 Out of Sample이 동일하다는 뜻(즉, $E_{out}(g_{10}) = E_{out}(g_2)$) 입니다. 여기서 알아야 할 것은 빨간색이 짙을 수록 Overfitting이 심하게 일어난다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;먼저 왼쪽의 Noise Level의 변화에 따른 그래프를 봅시다. Noise Level이 클수록, Overfitting을 해결하기 위한 데이터의 수가 더 많이 필요한 것을 알 수 있습니다. 예를 들어, Noise Level이 0일 때는 100개의 데이터가 주어졌을 때 Overfitting이 일어나지 않지만, Noise Level이 2일 때는 똑같이 100개의 데이터가 주어졌을 때 Overfitting이 일어나기 때문입니다.&lt;/p&gt;

&lt;p&gt;왼쪽의 그래프는 단순해보이지만, 오른쪽의 그래프는 조금 복잡해보입니다. 오른쪽의 그래프는 Target Function의 복잡도(즉, 다항함수의 차수)가 Overfitting에 미치는 영향을 나타냅니다. 먼저, $Q_f$가 커지면 커질수록 Overfitting이 심해지는 것은 간단하게 알 수 있습니다. 특이한 점은 $Q_f$가 일정 수치 이하일 때는 복잡도가 높아질수록 Overfitting이 일어나지 않는데, 뒤에 나올 슬라이드 17에서 그 이유가 나옵니다.&lt;/p&gt;

&lt;p&gt;여담으로 이 두 그래프는 교재의 표지 하단 왼쪽에 나와있는 그림과 같습니다. 또한 매 강의 슬라이드 첫장 하단 왼쪽에도 동일한 그림이 나와있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 실험으로 알게 된 것은 Noise Level과 Target Complexity 모두 Overfitting을 증가시킨다는 사실입니다. Noise Level $\sigma^2$은 어떤 확률 분포를 따르는 Noise이므로 이를 &lt;span style=&quot;color:red&quot;&gt;Stochastic Noise&lt;/span&gt;라고 부르고, Target Complexity는 확률적인 부분이 아니기 때문에 이를 &lt;span style=&quot;color:red&quot;&gt;Deterministic Noise&lt;/span&gt;라고 부릅니다. 이것을 Noise라고 부르는 이유는 실제 기계학습에서는 Target Complexity를 알 수 없기 때문에 마치 Noise처럼 보이기 때문입니다.&lt;/p&gt;

&lt;p&gt;요약하자면, 데이터의 수를 증가시킬 수록 Overfitting은 감소하고 Stochastic Noise나 Deterministic Noise를 증가시키면 Overfitting 또한 증가한다는 결론을 낼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;deterministic-noise&quot;&gt;Deterministic noise&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 Deterministic Noise에 대해 좀 더 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deterministic Noise는 Stochastic Noise와 마찬가지로 가설 집합 $\mathcal{H}$가 알 수 없는 요소입니다. 알 수 없다는 것은 학습을 하면서 “이것이 Noise다!” 라고 분별할 수 없음을 뜻합니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림은 Target Function $f$와 그 $f$를 최대한 가까운 근사 함수 $h^{*}$를 구한 모습인데, 보시다시피 어느정도의 차이가 있습니다. 그 이유는 그림에서 알 수 있다시피 $h^{*}$의 차수가 $f$보다 낮기 때문입니다.&lt;/p&gt;

&lt;p&gt;만약에 $\mathcal{H}$가 더 큰 차수의 모델이었다면, 그 오차가 줄어듬을 알 수 있습니다. 이를 통해, Deterministic Noise는 $\mathcal{H}$에 따라 달라진 다는 것을 알 수 있습니다. 또한 학습을 주어진 데이터로 하기 때문에, 데이터가 동일하다면 학습 결과와 $h^{*}$ 또한 동일하므로, Deterministic Noise 또한 변하지 않는다는 특징이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Deterministic Noise가 Overfitting에 끼치는 영향을 살펴보겠습니다. 오른쪽의 그림은 이전 슬라이드에서 본 그림인데, 그때 느낀 이상한 점은 $Q_f$가 일정 이하(여기서는 10)일 때는 Target Complexty가 커져도 Overfitting이 일어나지 않는 것이었습니다.&lt;/p&gt;

&lt;p&gt;저 일정 이하의 부분에서는 Target Function보다 낮은 차수이기 때문에 Target Complexity를 높일 수록 Overfitting을 피할 수 있는 것입니다. (물론 일정 이상의 데이터가 필요하다는 것도 그래프를 통해 알 수 있습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림을 통해 분석해 보았으니, 이제 수식을 통해 좀 더 정밀하게 분석해 보겠습니다. 8장에서 배웠던 Bias와 Variance를 기억하시나요? 이 분석 방법은 샘플에서 벗어난 오차를 Bias와 Variance로 각각 분류하였습니다. 이 당시 분석에서는 $f$에 Noise가 있다고 생각하지 않았습니다. 만약에 $f$에 Noise를 추가하게 된다면 식이 어떻게 바뀔 지 알아봅시다. Noise는 Gaussian Distribution (정규분포)을 따르기 때문에 그 평균은 0이라고 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$y = f(\mathbf{x}) + \epsilon (\mathbf{x})$로 놓고 나머지는 8장에서 했던 전개 과정을 그대로 수행하였습니다. 식은 다행히 그렇게 어렵지 않은데, 그 이유는 Noise의 평균이 0이기 때문에 Noise와 다른 항을 곱한 Cross Term의 평균 또한 모두 0이 되어버리기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 최종적으로는 기존의 Variance와 Bias에 $\epsilon(\mathbf{x})$를 제곱한 평균만 추가되는 꼴이 됩니다. 그 마지막 항이 바로 Stochastic Noise가 됩니다. 그런데 Bias는 최선의 가설 $\bar{g}$가 Target Function과의 차이를 뜻했는는데 이것이 바로 Deterministic Noise의 정의와 같기 때문에, 여기서 이를 같은 용어로 치환합니다.&lt;/p&gt;

&lt;h2 id=&quot;dealing-with-overfitting&quot;&gt;Dealing with overfitting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이제 Overfitting을 해결하는 방법을 알아보겠습니다. 불행하게도, Overfitting은 쉽게 해결되는 문제가 아니기 때문에, 이번 장에서는 어떤 방법들이 있는지만 소개하고, 그 방법들은 다음 2개의 장에서 하나씩 자세하게 설명합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째로 &lt;span style=&quot;color:red&quot;&gt;Regulatization (정규화)&lt;/span&gt;라는 방법이 있습니다. 이는 학습 중간에 제동을 거는 방법인데, 구현 방법은 목적 함수 자체를 약간 변형시키는 방법을 사용합니다.&lt;/p&gt;

&lt;p&gt;두 번째 방법은 &lt;span style=&quot;color:red&quot;&gt;Validation (검증)&lt;/span&gt;입니다. 위의 슬라이드 4를 보시면 Early Stopping 이라는 부분에서 멈춘다면 Overfitting이 일어나지 않음을 알 수 있습니다. 이 부분을 찾아서 멈추는 것이 바로 Validation이라는 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/11. Overfitting/ML 11-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 장에서 바로 Regulatization을 배우게 될 텐데, 대충 어떤 것인지 감을 잡기 위해 이번 장의 첫 예제를 다시 한번 보여드리겠습니다. 왼쪽 그림이 바로 Overfitting의 문제를 알게 된 그 그림인데, 이 때는 모델을 4차함수로 잡았기 때문에 Target Function과 큰 차이가 벌어졌었습니다.&lt;/p&gt;

&lt;p&gt;그런데 4차함수의 모양은 꼭 저런 모양만 있지 않습니다. 4차함수의 변수를 약간 변형시킨다면, 오른쪽 그림과 같이 2차 함수와 비슷한 모양으로 변해 4차함수로 모델을 잡더라도 Overfitting 문제를 해결할 수 있습니다. 바로 그 방법이 무엇인지 구체적으로 배우게 될 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Neural Networks</title><link href="http://localhost:4000/studies/neural-networks/" rel="alternate" type="text/html" title="Neural Networks" /><published>2019-09-21T00:00:00+09:00</published><updated>2019-09-21T00:00:00+09:00</updated><id>http://localhost:4000/studies/neural-networks</id><content type="html" xml:base="http://localhost:4000/studies/neural-networks/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;10장은 인공신경망(Neural Network)에 대해 배우게 됩니다. 현재 인공신경망 모델은 기계학습의 대세가 되어 많은 관심을 받고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 3개로 나뉘어 있습니다. 지난 장에서 배운 경사하강법(Gradient Descent)의 변형인 확률적 경사하강법(Stochastic Gradient Descent)를 배우고, 본격적인 인공신경망 모델에 대해서 배운 다음, 마지막으로 인공신경망 모델의 학습 알고리즘인 역전파(Backpropagation) 알고리즘을 배우게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic gradient descent&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 간단하게 지난 장에서 배운 경사하강법을 복습해보면, 처음에 무작위의 $w$를 정한 다음 기울기를 계산하여 어떤 방향으로 움직일 것인가를 정하여 $\mathbf{w}$를 단계적으로 값을 수정하여 In Sample Error가 최소가 되도록 만들어주는 방법이었습니다.&lt;/p&gt;

&lt;p&gt;분명 경사하강법은 최적의 $\mathbf{w}$를 정하기 위한 좋은 방법이지만, 문제는 한 단계를 거칠 때마다 모든 데이터를 이용하여 방향을 결정해야 한다는 것입니다. 즉, 데이터의 개수가 $N$개라면, 매번 $N$개의 데이터의 $e(h(\mathbf{x}_n, y_n))$를 계산해야 한다는 것입니다. 데이터의 개수가 적다면 크게 문제 될 사항은 아니지만, 일반적으로 기계학습에서는 수많은 데이터를 보유하여 그것을 기반으로 문제를 해결하기 때문에, 계산량이 많다는 것은 결코 반가운 사항은 아닙니다.&lt;/p&gt;

&lt;p&gt;이런 식으로 한번의 움직임을 위해 모든 데이터를 일괄적으로 처리하는 방식을 &lt;span style=&quot;color:red&quot;&gt;Batch&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 확률적으로 계산하기 위해 조금 다른 방법을 사용할 것입니다. 한 단계에서 오직 1개의 데이터 $(\mathbf{x}_{n}, y_{n})$ 만을 무작위로 추출하는 것입니다. 그리고 오직 그 1개의 데이터만을 사용해 경사하강법을 사용하는 것입니다.&lt;/p&gt;

&lt;p&gt;언뜻 보면 전체의 데이터를 기준으로 방향을 정하던 때와 달리 한 개의 데이터만을 기준으로 방향을 정하기 때문에 원하는 결과가 나오지 않을 것이라고 생각이 들지만, 사실 이 방법은 이미 이전에 PLA에서도 사용한 방법입니다. 이런 방식으로 방향을 정하는 것이 일반적인 경사하강법과 같다는 것을 보이기 위해 “평균적인” 방향을 계산해보면, 경사하강법의 방향 계산 식과 동일하다는 것을 알 수 있습니다. 물론 하나하나의 단계에서는 경사하강법의 방향과 차이가 있을 수 있지만, 많은 횟수를 반복하게 되면 결과적으로는 원래의 경사하강법과 동일한 방향이 된다는 것입니다. 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Stochastic Gradient Descent (확률적 경사하강법)&lt;/span&gt; 이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;확률적 경사하강법의 장점으로는 먼저 경사하강법에 비해 계산이 빠르다는 것입니다. 두 번째 장점으로는 무작위성으로 인해 이득을 볼 수 있다는 것으로, 직관적으로 이해는 쉽지 않기 때문에 오른쪽 그림을 참고해봅시다. 지난 장에서 경사하강법을 설명했을 때 봤던 U자 모양의 그래프인 경우는 사실 일반적으로 잘 발생하지 않고, 보통은 중간의 그림처럼 울퉁불퉁한 모양의 그래프가 더 많이 발생합니다. 이런 울퉁불퉁한 모양의 그래프의 가장 큰 문제는, &lt;strong&gt;전역 최솟값(Global Minimum)&lt;/strong&gt;을 찾다가 &lt;strong&gt;지역 최솟값(Local Minimum)&lt;/strong&gt;을 찾는 일이 발생한다는 것입니다. 그림상으로 봤을 때는 어느 부분이 지역 최솟값인지 쉽게 구분이 가능하지만, 실제 문제를 해결하는 과정에서는 그림이 아닌 수치로만 확인하기 때문에 지금 내가 찾은 답이 지역 최솟값인지, 전체 최솟값인지 구분이 되지 않는 문제가 있습니다. 만약에 방금처럼 무작위 하게 데이터를 선택하여 방향을 정한다면, (데이터가 고루 퍼져있다는 전제 하에) 지역 최솟값에 빠지더라도 그곳을 빠져나갈 수 있는 기회를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;세 번째 장점으로는 간단하다는 것인데, 확률적 경사하강법의 경우 Learning Rate $\eta$를 보통 간단하게 0.1로 놓는다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1장에서 나왔던 영화 추천 문제를 다시 가져왔습니다. 유저의 영화 선호도는 코미디, 액션, 등장하는 배우 등으로 이루어진 벡터로 이루어져 있고, 영화 또한 영화의 장르, 등장하는 배우 등으로 이루어진 벡터가 있습니다. 이것을 이용하여 유저가 어떤 영화를 좋아할지 추천하는 시스템을 만들어야 하는데, 넷플릭스에서 기존의 방법보다 10% 향상시키는 방법에 대해 100만 달러의 상금을 걸었다고 언급했습니다. 이 예제가 왜 갑자기 이 곳에 다시 언급되었나 궁금했는데, 온라인 강의에서 확률적 경사하강법을 사용한 방법이 실제로 10%의 성능 향상을 이루어내 100만 달러의 상금을 탔다고 합니다.&lt;/p&gt;

&lt;p&gt;100만 달러의 상금을 받은 방법을 간단하게 설명하자면, 유저와 영화의 각 요소를 곱한 값을 더해 영화에 매긴 평점과의 Squared Error를 계산한 것을 $\mathbf{e}_{ij}$로 놓고 확률적 경사하강법을 사용했다고 합니다. 방법 자체는 크게 어렵지 않아 보이지만, 마치 콜럼버스의 달걀처럼 보고 나면 쉬운데 막상 이걸 떠올리지는 쉽지 않았나 봅니다.&lt;/p&gt;

&lt;h2 id=&quot;neural-network-model&quot;&gt;Neural network model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째로 이번 장의 핵심인 인공신경망 모델에 대해 배워봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망은 이름에서 알 수 있듯이 신경망의 생물학적인 구조에서 영감을 얻어 만들어진 모델입니다. “배운다” 라는 생물학적인 기능을 구현하기 위해 생물학적인 구조를 모방한 것입니다. 물론 구조만 그렇게 만들고 끝나는 것이 아니라 생물학적으로 동작하게끔 유사한 시스템까지 구현해야 합니다.&lt;/p&gt;

&lt;p&gt;신경망은 &lt;strong&gt;Synapse&lt;/strong&gt;로 연결된 &lt;strong&gt;Neuron&lt;/strong&gt;으로 구성되어 있습니다. 각 뉴런들은 입력을 받은 자극으로부터 간단한 연산을 한 후 그 결과를 내보냅니다. 마치, 퍼셉트론(Perceptron)과 유사하다고 생각하시면 됩니다. 인공신경망은 다수의 뉴런으로 구성되어 있는 신경망과 비슷하게 다수의 퍼셉트론으로 이루어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1장에서 배운 퍼셉트론은 간단하고도 꽤 유용한 도구였지만, 지금까지 문제가 꾸준히 있었습니다. 특히 선형 분류가 되지 않은 문제를 해결하기 위해 여러 장에 걸쳐 꽤 많은 노력을 기울여왔습니다. 그러나 이런 노력에도 불구하고 퍼셉트론으로 아예 해결할 수 없는 문제도 있습니다. 위 슬라이드의 첫 번째 그림을 보면 +와 -가 서로 대각선 영역으로 나뉘어 있습니다. 이를 정확하게 나누기 위해서는 최소한 2개의 직선이 필요합니다. 그러나 퍼셉트론은 1개의 직선으로 이루어진 방법입니다. 따라서 기존의 퍼셉트론을 사용해서는 이 문제를 해결할 수 없는데, 만약에 이 문제를 두 번째와 세 번째 그림과 같이 $h_1, h_2$ 2개의 선으로 각각 나눈 다음 합칠 수는 없을까 라는 새로운 방법이 제시되었습니다.&lt;/p&gt;

&lt;p&gt;두 개의 서로 다른 도구를 합치기 위한 방법은 기본적으로 OR 논리회로와 AND 논리회로인데, 슬라이드 아래의 그림은 이것을 퍼셉트론으로 구현한 것입니다. OR은 2개의 Input 모두 -1일 경우에만 Output이 -1이 나오고 그 외에는 모두 +1이 나오는 논리회로이고, AND는 2개의 Input 모두 1일 경우에만 Output이 1이 나오고 그 외에는 모두 -1이 나오는 논리회로입니다. 왼쪽 그림을 보시면 기본적으로 1.5의 값이 들어오기 때문에 $x_1, x_2$ 모두 -1이 들어와야만 -1이 나오고, 오른쪽 그림에서는 기본적으로 -1.5의 값이 들어오기 때문에 $x_1, x_2$ 모두 1이 들어와야만 1이 나오는 구조가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 배운 내용을 토대로, 위 슬라이드의 왼쪽 그림은 이전 슬라이드의 $h_1, h_2$를 가지고 이전 슬라이드의 위쪽 첫 번째 그림과 같은 분류가 되도록 논리회로를 구성한 결과입니다. Input이 조금 복잡한 모양을 가지고 있지만, 눈썰미가 좋으신 분은 XOR 논리회로를 구현한 퍼셉트론인 것을 아실 수 있을 것입니다. 이 표현방법이 틀린 것은 아니지만, Input이 너무 복잡하게 나와있기 때문에 $h_1, h_2$로만 Input이 구성될 수 있게끔 전개한 것이 위 슬라이드의 오른쪽 그림입니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림에서 가장 왼쪽의 단계에서는 $h_{1}\bar{h}_{2}$와 $\bar{h}_{1}h_{2}$를 구현하였습니다. Threshold가 -1.5로 들어가고 있는 것을 보면 $h_1, h_2$의 입력을 각각 AND회로로 구성한 것을 알 수 있습니다. XOR를 구현하기 위해서는 $h_1$과 $h_2$가 서로 한번씩 NOT회로를 거쳐야 하는데, 오른쪽 그림에서는 그것을 구현하기 위해 가중치에 -1을 부여하는 것으로 해결하였습니다. 두번째 단계에서는 $h_{1}\bar{h}_2$와 $\bar{h}_{1}h_{2}$를 더하는 OR 논리회로를 구현함으로써 왼쪽의 그림과 동일한 Output이 나오도록 만들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 문제에 맞게 그림을 수정하긴 했지만, 원래의 문제는 Input으로 $x_1, x_2$가 들어가지 $h_1, h_2$가 들어가는 게 아니었습니다. 따라서 $x_1, x_2$를 이용하여 $h_1, h_2$를 구현하는 것 또한 수행해 주어야 합니다. 퍼셉트론 $h_1, h_2$를 구현하기 위한 가중치 $\mathbf{w}_1$과 $\mathbf{w}_2$은 이미 구했다고 가정하고, 여기서는 그 구조만 표현하도록 합시다.&lt;/p&gt;

&lt;p&gt;결과적으로 이 문제는 총 3단계를 거쳐 해결할 수 있게 되었습니다. 인공신경망에서 이 각각의 단계를 &lt;span style=&quot;color:red&quot;&gt;Layer&lt;/span&gt;라고 부릅니다. 또한 이 인공신경망의 구조는 Input에서 Output까지 다음 레이어로만 이동하고 이전 레이어로 돌아가지 않는데, 이러한 구조를 &lt;span style=&quot;color:red&quot;&gt;Feedforward&lt;/span&gt; 구조라고 합니다. 따라서 이 문제는 3개의 레이어로 구성된 피드포워드 인공신경망이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망 모델을 이용한다면 기존에 퍼셉트론으로는 해결할 수 없었던 많은 문제를 해결할 수 있습니다. 위 슬라이드의 첫 번째 그림은 퍼셉트론으로 풀 수 없는 예제입니다. 이전보다 조금 더 어려워 보이는 원(Circle)이 주어졌습니다. 직선으로 원을 표현하기는 힘드니 두 번째 그림처럼 8개의 퍼셉트론을 이용하여 8각형으로 처리하는 방법이 있습니다. 그러나 Target과 동일한 모양이 아니기 때문에 연두색 부분처럼 어느 정도의 오류가 발생하게 됩니다. 이 오류를 줄이기 위해 세 번째 그림처럼 8개의 퍼셉트론을 추가해 더 원에 가까운 16각형으로 처리할 수도 있습니다. 우리가 원한다면 더 많은 퍼셉트론을 이용해 최대한 원과 가까운 모양을 만들면서 오류를 줄여갈 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 이렇게 퍼셉트론을 많이 사용할수록 또 다른 문제가 생기게 됩니다. 많은 퍼셉트론을 사용할수록 필요한 가중치의 수와 자유도, VC Dimension이 늘어나므로 일반화가 어려워지게 됩니다. 이보다 더 큰 또 다른 문제는 모델이 복잡해질수록 최적의 가중치를 찾는 최적화까지 어려워진다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공신경망의 일반적인 구조는 Input $\mathbf{x}$으로 이루어진 &lt;span style=&quot;color:red&quot;&gt;Input Layer&lt;/span&gt;, 그리고 로지스틱 함수 $\theta$로 이루어진 &lt;span style=&quot;color:red&quot;&gt;Hidden Layer&lt;/span&gt;, 결괏값을 내보내는 &lt;span style=&quot;color:red&quot;&gt;Output Layer&lt;/span&gt;로 이루어져 있습니다. 이 중 핵심은 Hidden Layer인데, 사용자가 일반적으로 어떤 값인지 알 수 없기 때문에 Hidden이란 이름이 붙었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 인공신경망을 구현하기 위해서는 약간의 변형이 필요합니다. 이전 장에서 배운 로지스틱 함수는 ${e^s}/{(1+e^s)}$ 였지만, 이는 0과 1 사이의 값을 갖는 비선형 곡선이었습니다. 그러나 우리는 -1과 1 사이의 값을 갖는 것이 필요하므로, 함수를 조금 아래로 내려야 합니다. 로지스틱 함수는 입력이 0일 때 0.5의 값을 가졌으므로, 로지스틱 함수에 0.5를 빼주면 정확하게 원점을 지나게 됩니다.&lt;/p&gt;

\[\frac{e^s}{1+e^s}-\frac{1}{2}\]

&lt;p&gt;문제는 이렇게 되면 함수의 크기상 -0.5와 0.5 사이만을 지나게 되는 문제가 있습니다. 우리가 원하는 것은 -1과 1 사이의 값을 갖는 것이니, 함수에 2를 곱해주어야 합니다.&lt;/p&gt;

\[2 \times \left( \frac{e^s}{1+e^s}-\frac{1}{2} \right)\]

&lt;p&gt;이 식을 정리하면 우리가 많이 보던 쌍곡 탄젠트 함수($\tanh$)와 유사한 함수가 나오게 됩니다.&lt;/p&gt;

\[2 \times \left( \frac{e^s}{1+e^s}-\frac{1}{2} \right) = \frac{2e^s-e^s-1}{1+e^s} = \frac{e^s-1}{e^s+1}\]

&lt;p&gt;입력만 $s$에서 $2s$로 바꾸어주면 쌍곡 탄젠트 함수 $\tanh$가 되는데, 왜 여기에 2를 곱해주는지는 사실 잘 모르겠습니다. 혹시 이 부분을 아시는 분은 댓글로 알려주시기 바랍니다.&lt;/p&gt;

&lt;p&gt;인공신경망에서는 퍼셉트론에 비해 가중치의 수가 많이 늘어났기 때문에 이 가중치가 어디에 연결되어있는지 좀 더 정교한 표기가 필요합니다. 이제는 $w^{(l)}_{ij}$라는 방식으로 표기하는데, 이것은 레이어 $l$의 가중치 중 이전 레이어의 $i$번째 뉴런과 현재 레이어 $l$의 $j$번째 뉴런이 연결되어있다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;또한 입력에 대한 표기법도 정교해졌는데, 이전 레이어의 Output이 다음 레이어의 Input이 되기 때문입니다. 입력의 표기는 $x^{(l)}_j$로 하는데, 이것은 레이어 $l$의 $j$번째 뉴런의 Input이라는 뜻입니다.&lt;/p&gt;

&lt;h2 id=&quot;backpropagation-algorithm&quot;&gt;Backpropagation algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 인공신경망에서 각각의 가중치를 학습하기 위해 사용하는 &lt;span style=&quot;color:red&quot;&gt;Backpropagation Algorithm (역전파 알고리즘)&lt;/span&gt;을 배워봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적으로는 이번 장 앞부분에서 배운 확률적 경사하강법을 사용합니다. 모든 가중치 $\mathbf{w}$를 학습할 때 한번에 1개의 데이터 $(\mathbf{x}_n, y_n)$을 사용하는데, 이 때의 오류를 $\mathbf{e}(\mathbf{w})$로 정의합니다. 확률적 경사하강법을 구현하기 위해서는 $\mathbf{e}(\mathbf{w})$의 기울기를 구해야 하는데, 모든 가중치의 값을 변화시켜야 하므로 모든 레이어 $l$, 모든 $i$, $j$를 잇는 뉴런에 대해서 수행해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;문제를 간단하게 접근하기 위해 $\nabla\mathbf{e}(\mathbf{w})$를 차근차근 분석해봅시다. 이 강의에서는 효율적인 계산을 위해 연쇄 법칙(Chain Rule)을 사용하였습니다. 오류와 가중치 간의 직접적인 기울기를 계산하는 것이 어렵기 때문에 오류 $\mathbf{e}(\mathbf{w})$와 로지스틱 함수를 거치기 전의 값인 $s^{(l)}_j$, $s^{(l)}_j$와 가중치 $w^{(l)}_{ij}$의 기울기를 각각 계산하는 방법을 이용합니다.&lt;/p&gt;

&lt;p&gt;연쇄 법칙을 이용하게 되면 문제가 약간 간단해집니다. 일단, 입력 $x^{(l-1)}_i$와 가중치 $w^{(l)}_{ij}$를 곱한 것이 $s^{(l)}_j$이므로, $s^{(l)}_j$를 $w^{(l)}_{ij}$로 편미분하게 되면 $x^{(l-1)}_i$만 남기 때문입니다. 아쉽게도 오류 $\mathbf{e}(\mathbf{w})$를 $s^{(l)}_j$로 편미분한 것은 간단하게 정리하지 못하지만, 표기만이라도 간단히 하기 위해 이를 $\delta^{(l)}_j$로 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 경사하강법에서 가중치를 계산할 때는 출력을 기준으로 계산했기 때문에, 이번에도 최종 출력인 마지막 레이어에서 먼저 $\delta^{(l)}_j$를 계산해봅시다. 마지막 레이어 $L$은 출력을 위해 단 1개의 뉴런만 존재하므로 $j$는 1이 됩니다. 이곳에서 $\mathbf{e}(\mathbf{w})$는 $\mathbf{e}(x^{(L)}_1, y_n)$이고, 이를 풀어쓰면 $(x^{(L)}_1 - y_n)^2$가 됩니다. $x^{(L)}_1$은 $s^{(L)}_1$가 쌍곡 탄젠트 함수 $\theta$를 거친 값이므로 최종적으로 $\theta$의 미분값을 계산하게 되면 $1-\theta^2(s)$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 레이어에서의 $\delta$를 계산했으니, 이제는 그보다 이전 레이어의 $\delta$를 구하기 위해 두 $\delta$간의 관계를 찾아보도록 하겠습니다. 이번에도 계산에는 연쇄 법칙을 사용하는데, 이전보다는 조금 더 복잡하게 2번을 사용하게 됩니다. 두 번째 단계에서 시그마가 갑자기 튀어나오는 것이 이해가 안 가실 수도 있는데, 다변수함수에서 편미분을 하게 되면 아래처럼 시그마가 나오게 됩니다.&lt;/p&gt;

\[\frac{\partial}{\partial t}f(x_1, x_2, \ldots, x_N) = \sum_{i=1}^{N}\frac{\partial f}{\partial x_i} \times \frac{\partial x_i}{\partial t}\]

&lt;p&gt;이렇게 연쇄 법칙으로 3덩이로 식을 나누게 되면, 각각의 덩이를 간단하게 표현할 수 있으므로 문제가 간단해집니다. 최종적으로는 맨 아래의 식과 같이 이전 레이어의 $\delta^{(l-1)}_i$와 이후 레이어의 $\delta^{(l)}_j$ 사이의 관계를 찾았으므로, 첫 번째 레이어의 $\delta$부터 마지막 레이어의 $\delta$까지 모두 계산할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 배운 내용을 알고리즘으로 정리하였습니다. 하나 특이한 점은, 가중치의 갱신을 위해서는 마지막 레이어부터 맨 앞의 레이어까지 반대방향(Backward)으로 계산하지만, 마지막 레이어에서 오류를 계산하기 위해서는 주어진 가중치로 첫번째 레이어부터 마지막 레이어까지 정방향(Forward)으로 계산해야 한다는 것입니다. 즉, 매 단계에서 앞으로 한번 계산하고, 뒤로 한번 계산하는 과정을 거쳐야만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/10. Neural Network/ML 10-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 이전에 배운 비선형 변환(Nonlinear Transform)과 인공신경망 모델과는 어떤 차이가 있는지 알아봅시다. 비선형 변환과 인공신경망 모델 모두 선형 분리가 되지 않는 데이터를 제대로 분리하기 위한 방법입니다. 차이가 있다면 비선형 변환의 경우 직접 특정한 함수를 찾아 데이터를 다른 차원으로 변환하였지만, 인공신경망 모델에서는 가중치를 학습시킴으로써 데이터를 분류하는 것이므로, 학습된 비선형 변환으로 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 제대로 학습하기 위해 히든 레이어의 수나 뉴런의 수는 어떻게 조정해야 할까요? 그 문제는 다음 장에서 다루게 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Linear Model II</title><link href="http://localhost:4000/studies/linear-models-2/" rel="alternate" type="text/html" title="Linear Model II" /><published>2019-09-13T00:00:00+09:00</published><updated>2019-09-13T00:00:00+09:00</updated><id>http://localhost:4000/studies/linear-models-2</id><content type="html" xml:base="http://localhost:4000/studies/linear-models-2/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;9장은 3장에 이어서 선형 모델을 배우게 됩니다. 교재에서는 3장과 9장의 내용이 하나의 장으로 구성되어 있지만, 인터넷 강의에서는 이론적인 내용을 연속해서 다루기보다 중간에 구체적인 예시를 추가하기 위하여 두 장으로 나누었다고 합니다. 따라서 이번 장을 공부하기 전에 3장을 다시 한번 복습하시는 것을 추천합니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3장에서 다루었던 것과 이번 장에서 다룰 것을 정리해봅시다. 3장에서 선형 분류(Linear Classification), 선형 회귀(Linear Regression)가 무엇인지 배웠고, 어떤 알고리즘을 사용해 학습하는지까지 간단히 소개하였습니다. (물론 3장에서 배운 학습 알고리즘이 전부는 아닙니다.)&lt;/p&gt;

&lt;p&gt;또한 3장 마지막에는 비선형 문제를 해결하는 방법인 Transform을 소개하였습니다. 다만 그 당시에는 간단하게 넘어갔기 때문에 이번 장에서 Nonlinear Transform을 먼저 더 공부하고, 이전에 다루지 않았던 Logistic Regression에 대해 본격적으로 공부하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-transforms&quot;&gt;Nonlinear transforms&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3장에서 배운 비선형 문제를 푸는 방법을 다시 한번 복습하고 넘어갑시다. 주어진 Input Data $\mathbf{x}$는 $d$차원의 벡터입니다. ($x_0$은 Threshold 역할을 하기 때문에 $d$에 포함시키지 않았습니다.) 그러나 $\mathbf{x}$의 분포가 선형 분리가 되지 않는 경우, 이를 선형 분리가 가능한 데이터로 만들어주기 위해 새로운 함수 $\Phi$를 정의하여 $\mathbf{z}$로 바꾸어 주었습니다. 이 때, $\mathbf{x}$와 차원이 달라질 수 있기 때문에 $\mathbf{z}$는 $\tilde{d}$차원을 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;여기서 주의할 점은, 변환을 하기 전과 변환을 하고 난 데이터의 Output은 동일하기 때문에, 굳이 변환한 학습 결과를 원래 차원으로 되돌릴 필요가 없다는 것입니다. 즉, 최종 가설 $g$는 선형 분류 문제의 경우 $\text{sign}(\tilde{\mathbf{w}}\Phi(\mathbf{x}))$, 선형 회귀 문제의 경우에는 $\tilde{\mathbf{w}}\Phi(\mathbf{x})$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 비선형 문제를 Transform을 통해 문제를 해결하게 되면 새로운 문제가 생기게 됩니다. 3장에서 이것을 처음 배울 때는 단순히 $\Phi$함수를 찾기가 쉽지 않다는 단점만을 생각할 수 있었지만, VC Dimension을 배우고 나니 Transform을 하게 되면 VC Dimension 또한 바뀌게 된다는 사실도 알게 되었습니다. 일반적으로 선형 데이터로 만들기 위해 Transform을 하는 것은 차원이 늘어날 수밖에 없기 때문에 VC Dimension 또한 늘어나게 됩니다. VC Dimension이 늘어난다는 것은 그만큼 일반화하기 힘들다는 말과 같습니다.&lt;/p&gt;

&lt;p&gt;원래의 데이터 $\mathbf{x}$의 VC Dimension은 Threshold를 포함하여 $d+1$인데, $\mathbf{z}$의 VC Dimension은 최대 $\tilde{d}+1$이 됩니다. 왜 정확히 $\tilde{d}+1$이 아니라 최대 $\tilde{d}+1$이 되냐면 $\mathbf{z}$는 $\mathbf{x}$를 통해 만들게 되기 때문입니다. 이전 슬라이드의 Example 처럼 $\mathbf{z}$ 벡터의 각 원소는 $\mathbf{x}$의 원소를 사용해 만들기 때문에, 각 원소가 독립적이지 않는 경우가 존재하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 비선형 예제 2개를 살펴보겠습니다. 첫번째 예제는 왼쪽 그림처럼 거의 선형이지만 선형으로 나눌 경우 약간의 에러가 발생하는 경우이고, 두번째 예제는 오른쪽 그림처럼 데이터들이 처음부터 비선형이라 선형으로는 절때 나눌 수 없는 경우입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 예제는 두 가지 선택을 할 수 있습니다. 약간의 In Sample Error를 감수하고 선형으로 나눌 것인지, 그 Error조차 용납하지 못하고 높은 차원으로 Transform을 시켜 In Sample Error를 0으로 만들게 할 수도 있습니다. 말할 필요도 없이 어떤 방법이 더 좋은지 우리는 이미 알고 있습니다. 굳이 VC Dimension을 따져보지 않더라도 Transform을 시켜서 푸는 방법이 일반화가 더 어렵기 때문에 더 좋지 않은 해결방법이라는 직관적인 판단을 하셨다면, 훌륭합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 예제는 Transform을 해야만 문제를 풀 수 있기 때문에 Transform을 한다고 가정합시다. 원래의 데이터는 $x_1, x_2$로 이루어진 2+1차원의 데이터였으나, 차수를 늘려 5+1차원의 데이터로 바뀌게 되었습니다. 그렇게 Transform을 하고 학습을 성공적으로 마쳤을 때의 상황이 바로 오른쪽 그림입니다. 가만히 생각해보면, 나눈 결과를 보아하니 원 모양인데, 원의 방정식에 필요한 2차항만 남겨서 차수를 줄일 수 있지 않을까하는 느낌이 들 수도 있습니다. $x_1^2, x_2^2$만 남기고 2+1차원을 데이터로 바꿀 수도 있고, 혹은 그보다 줄이는 방법도 생각해 볼 수 있습니다. 물론 초기 상태인 5+1차원과 동일한 결과가 나옵니다. 하지만 이 방법은 뭔가 이상하다는 직관적인 느낌이 듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 전의 두 번째 예제에서 차원을 줄이는 과정을 다시 한번 복기해봅시다. 먼저, 데이터를 Transform 시키고 학습이 끝난 다음 결과를 확인하였습니다. 그런데, 학습 결과를 보고 원과 비슷한 모양인 것을 확인한 후, &lt;strong&gt;의도적으로&lt;/strong&gt; Transform 시킨 데이터의 차원을 줄였습니다. 이 부분에서 잘못된 판단을 한 것입니다. 데이터를 눈으로 먼저 보고 모델을 고르는 것을 &lt;span style=&quot;color:red&quot;&gt;Data Snooping&lt;/span&gt;이라고 하는데, 이렇게 모델을 정할 경우 학습 결과가 일반화와는 거리가 멀어지기 때문에 Out of Sample Error에 큰 악영향을 주게 됩니다. 방금 전에 “원”의 모양을 눈으로 보고 그 모양이 나오도록 학습 결과를 의도적으로 수정했기 때문에 In Sample Error에는 영향이 없을 수도 있지만, 실제 Out of Sample Data가 어떻게 분포되어 있을지 모르는 상황이기 때문에 대부분의 경우에는 Out of Sample Error를 높게 만듭니다. 기계학습에서 가장 많이 행하게 되는 실수라고 하니 반드시 염두해 두도록 합시다.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 이번 장의 핵심 주제인 Logistic Regression을 공부해봅시다. 모델, 오류 측정 방법, 학습 알고리즘 순서로 배우게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear Model은 기본적으로 입력을 받고 출력값을 계산한다는 의미입니다. 이전에 배운 Linear Classification과 Linear Regression 모두 입력 $\mathbf{x}$와 가중치 $\mathbf{w}$를 받아 출력값을 계산하였습니다. 단순히 출력값을 그대로 내보내는가(Regression), +1/-1인지만 결정하는가(Classification)의 차이만 있었습니다. Logistic Regression도 이와 마찬가지로 입력 $\mathbf{x}$와 가중치 $\mathbf{w}$를 받아 출력값을 계산하는데, 그 출력값이 $\theta$ 함수를 거쳐 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;$\theta$ 함수는 다음 슬라이드에서 설명하도록 하고, 함수의 모양을 먼저 보면 S자를 눕힌 듯한 그래프를 가짐을 알 수 있습니다. 정확하게는, 최댓값은 1, 최소값은 0이 되고 그 사이를 부드럽게 올라가는 곡선입니다. Linear Classification은 Threshold를 기준으로 +1/-1로만 출력값이 나오지만, Logistic Regression은 0과 1 사이의 모든 실수가 출력값이 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로지스틱 함수 $\theta$의 명확한 정의는 ${e^s}/{(1+e^s)}$입니다. $s$가 크면 클수록 1에 가까워지게 되고, 작으면 작을수록 0에 가까워지게 됩니다. 특수한 경우로, $s$가 0이면 정확하게 0.5가 됩니다. 이 함수는 부드럽게 0과 1 사이를 움직이기 때문에 Soft Threshold로 부르기도 하지만, 일반적으로는 &lt;span style=&quot;color:red&quot;&gt;Sigmoid Function&lt;/span&gt;으로 불립니다. Sigmoid는 “S자 모양의” 라는 뜻입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;0과 1사이의 값을 갖는다는 것에서 눈치채신 분들도 계실텐데, 로지스틱 회귀는 바로 확률에 관한 문제를 다룰 때 쓰이게 됩니다. 이해를 돕기 위해 구체적인 예시로 심장 마비를 예측하는 문제가 있다고 가정해봅시다. 심장 마비에 영향을 줄 수 있는 요소인 콜레스트롤 수치, 나이, 몸무게 등은 입력 데이터로 들어가게 됩니다. 만약 Linear Classification으로 이 문제를 접근한다면 심장마비가 발생하지 않는다/발생한다 라는 결과만 출력할 수 있습니다. 그런데 정말 좋지 않은 신체적 상황이라고 해도 심장마비가 무조건 발생하지는 않습니다. 하지만 심장마비의 발생은 입력 데이터에 영향이 분명히 있기 때문에 이러한 상황에서는 확률적으로 접근해야 한다는 것입니다. 따라서 이 문제를 로지스틱 회귀로 풀게 된다면, $\theta$ 함수는 입력 $s$에 대해 심장 마비가 발생할 확률을 의미하게 됩니다. 이 때 입력 $s$는 이전의 선형 분류나 회귀와 마찬가지로 선형이고, 높으면 높을수록 심장 마비를 발생할 확률을 높게 만들기 때문에 “Risk Score”로 부를 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로지스틱 회귀에서 특이한 점은 학습 데이터입니다. 선형 분류나 회귀와 마찬가지로 지도학습으로 이루어지는데, 선형 분류에서는 데이터의 Output $y$가 +1/-1로, 선형 회귀에서는 실수 $\mathbb{R}$로 주어졌습니다. 그렇기에 로지스틱 회귀에서는 Output $y$가 0과 1 사이의 값으로 주어질 것이라고 생각할 수 있는데 그렇지 않는 것이 문제입니다. 로지스틱 회귀의 학습 데이터는 선형 분류와 마찬가지로 +1/-1로 주어집니다. 물론, 노이즈가 반영되기 때문에 같은 입력값에 대해 다른 Output을 갖고 있는 데이터가 포함됩니다.&lt;/p&gt;

&lt;p&gt;이전 슬라이드의 심장 마비 예측 문제를 생각하면, 완전히 동일한 신체조건을 가지고 있는 서로 다른 10명의 환자 데이터가 있다고 했을 때, 이 중 3명이 심장 마비가 발생하고 7명이 발생하지 않았다면 해당 신체조건(=$s$)에서 심장 마비가 발생할 확률이 0.3이다라고 예측하는 방식입니다. 물론 학습 알고리즘에 따라 0.3보다 작을수도, 클수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 로지스틱 회귀에서 Error Measure를 어떻게 하는지 배워봅시다. 선형 분류는 Output이 다른 것의 갯수를, 선형 회귀는 Squared Error를 사용했는데 로지스틱 회귀에서 Error Measure는 직관적으로 어떻게 해야 할지 떠오르지 않습니다.&lt;/p&gt;

&lt;p&gt;로지스틱 회귀에서는 &lt;span style=&quot;color:red&quot;&gt;Likelihood (가능도)&lt;/span&gt;를 Error Measure에 사용합니다. 가능도는 우도라고도 불리는데, 통계학에서 확률 분포가 확률 변수의 특정 값에 얼마나 일관적인지 그 정도를 나타내는 값입니다. 즉, 만약 가설 $h$와 Target Function $f$와 같다면, 가설 $h$에 의해 입력 데이터 $x$가 주어졌을 때 Output으로 $y$를 얻을 확률이 어느 정도인가? 를 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;개념을 이해했으니 구체적인 수식을 통해 계산하는 법을 알아봅시다. 먼저 가설 $h$는 로지스틱 회귀이므로 $\theta$ 함수와 같습니다. $\theta$ 함수의 특이한 점은 입력 $s$의 부호를 반대로 넣으면 $1-\theta(s)$와 같습니다. 직접 계산할 필요 없이 오른쪽의 함수 그림을 보시면 점선인 1에서 파란색 선을 빼 보시면 쉽게 알 수 있습니다. 이 성질을 이용하여 가능도 함수를 $P(y \mid \mathbf{x})=\theta(y\mathbf{w}^{\sf T}\mathbf{x})$로 간단하게 표현할 수 있습니다. 이를 이용하여 $N$개의 데이터가 주어져 있다고 가정할 때, 각 데이터에 대해 가능도 함수 $P$를 계산하여 곱하면 됩니다. 가능도 함수는 방금 구한 $\theta$ 함수와 같으니 $\theta$ 함수로 바꿔도 무방합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가능도는 인과관계를 의미하기 때문에 기존의 다른 Error Measure와 다르게 높을 수록 좋은 수치입니다. 그렇기에 다른 방법과의 통일성을 위해, 이전 슬라이드에서 도출한 식의 부호를 반대로 하고, 그 값을 최소화하는 방향으로 최적화를 진행할 것입니다. 식을 간단하게 정리하기 위해 마이너스 기호를 로그 안에 넣고, 파이를 로그 밖으로 빼어 시그마로 바꿉니다. 수식을 $N$으로 나눈 이유는 모든 주어진 데이터에 대해 평균적인 Error를 구하기 위함입니다.&lt;/p&gt;

&lt;p&gt;최적화를 수행하기 전에, $N$, $y$, $\mathbf{x}$는 모두 고정된 값이기 때문에 임의로 바꿀 수 없음에 유의하시기 바랍니다. 우리는 이 식에서 오로지 $\mathbf{w}$만 수정해야 합니다.&lt;/p&gt;

&lt;p&gt;먼저 $\theta$ 함수를 풀어서 표현합니다. 이전에 나온 $\theta$ 식과 조금 달라보이는데, 분모와 분자를 모두 $e^s$로 나눈 것 외에는 동일합니다. 식을 마지막까지 정리하게 되면 깔끔한 식이 나오게 되는데, 이 중 로그 부분을 따로 떼어 $e(h(\mathbf{x}_n, y_n)$으로 정의합니다. 이런식의 Error Measure를 &lt;span style=&quot;color:red&quot;&gt;Cross-Entropy Error&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 순서로는 로지스틱 회귀에서 어떤 학습 알고리즘을 사용할 것인지 배우게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 In Sample Error를 계산하는 식을 구했으니, 이를 최소화 시키는 방법을 알아야 합니다. 이전에 배운 선형 회귀의 경우 Pseudo-Inverse를 이용하여 여러 단계를 거치지 않고 한번에 최적의 가중치 $\mathbf{w}$를 계산하였습니다. 그러나 로지스틱 회귀에서는 불행하게도 로그 함수의 존재로 인해 한번에 쉽게 구하는 방법이 없습니다. 따라서 반복적인(Iterative) 방법으로 계산해야만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반복적인 방법 중 대표적인 것이 바로 &lt;span style=&quot;color:red&quot;&gt;Gradient Descent (경사하강법)&lt;/span&gt;입니다. 여기서 Gradient는 미적분학에서 배우는 기울기입니다. 위 슬라이드의 오른쪽 그림을 보시면 최종 목적지는 In Sample Error가 가장 낮게 나오는 점입니다. 이 점을 한번에 구할 수 없기 때문에 초기에는 무작위로 $\mathbf{w}$를 정합니다. 이 곳이 최종 목적지라면 좋겠지만 일반적인 경우에는 그렇지 않으니 이 점에서 다른 점으로 움직여야 합니다. 그 움직이는 방향을 가리키는 것이 단위벡터 $\widehat{\mathbf{v}}$입니다. 단위벡터 앞에 붙은 상수 $\eta$(에타)는 얼마나 움직일 것인지 그 크기를 나타냅니다. 그렇게 움직인 결과가 바로 새로운 $\mathbf{w}$가 되는 것입니다.&lt;/p&gt;

&lt;p&gt;방법을 알았으니 이제 방향을 결정하는 단위벡터 $\widehat{\mathbf{v}}$만 구하는 방법을 알면 됩니다. 이름에서 눈치채신 분들도 있지만, 이걸 구할때 바로 기울기(Gradient)를 이용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저, 이전 슬라이드에서 구한 $\mathbf{w}(1)$과 초기값 $\mathbf{w}(0)$을 비교해봅시다. $\mathbf{w}(1)$을 넣어서 In Sample Error를 계산하고, $\mathbf{w}(0)$을 넣어서 In Sample Error를 계산한 후, $\mathbf{w}(1)$로 계산한 In Sample Error에서 $\mathbf{w}(0)$로 계산한 In Sample Error를 빼면 In Sample Error의 차이($\Delta E_{in}$)를 계산할 수 있습니다. 우리가 원하는 것은 반복적인 과정을 거칠 때마다 In Sample Error가 줄어드는 것을 원하므로 이 변화가 음수가 나와야 합니다.&lt;/p&gt;

&lt;p&gt;이 차이를 계산하기 위하여 테일러 급수를 사용해야 합니다. 테일러 급수에서 맨 앞 항만 사용하고 나머지 부분은 $O(\eta^2)$로 묶었습니다. 사실 중요한 것은 맨 앞부분에 있는 $\widehat{\mathbf{v}}$ 이기 때문에 식을 간략화한 후 정규화(Normalization)를 시키면, $\mathbf{w}(0)$에서의 기울기 단위 벡터를 얻을 수 있습니다. 왜 $\widehat{\mathbf{v}}$에 마이너스(-) 가 붙는 지 궁금하실 수도 있는데, 쉽게 이해하기 위해서는 이전 슬라이드의 그림을 떠올리시면 됩니다. 그림처럼 점이 최소값의 왼쪽에 있을 때는 오른쪽으로 점을 움직여야 하는데, 현재의 기울기가 음수이므로, 양의 방향으로 움직여야 하기 때문입니다. 즉, 움직여야 하는 방향은 현재 기울기의 반대 방향임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 다음으로 논의할 것은 $\eta$입니다. 처음에는 $\eta$를 고정된 값을 사용한다고 했는데, 만약에 이 $\eta$가 너무 작으면 최종 목적지에 도달하는데 너무 많은 단계를 거쳐야 하는 문제가 생길 수 있습니다. 그렇다고 이 값을 크게 만들다가 너무 커지게 되면, 최종 목적지를 지나칠 수도 있고, 그 주변에서 왔다갔다 하느라 목적지에 도달하지 못할 수도 있습니다. 따라서 이런 단점들을 해결하기 위해, $\eta$를 유동적인 값을 사용하는 것이 좋습니다. 초기에는 큰 값을 부여해 목적지에 빠르게 가깝게 다가가고, 나중에는 작은 값을 부여해 목적지를 지나치지 않고 쉽게 다가갈 수 있도록 만드는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 유동적인 $\eta$를 어떻게 구현해야 할지 구체적으로 계산해봅시다. 이 강의에서는 $\Delta\mathbf{w}$의 식을 응용하여 $\eta$를 정의하였습니다. 바로 기존의 $\eta$값을 $\lVert \triangledown E_{in}(\mathbf{w}(0)) \rVert$에 비례하게 만들어 $\widehat{\mathbf{v}}$의 분모를 약분할 수 있게 만든 다음, 새로운 고정된 값인 $\eta$로 정의하는 것입니다. $\eta$ 기준에서 보면 초기에는 기울기가 가파르기 때문에 높은 값을 가지지만 목적지에 도달할수록 기울기가 완만해지기 때문에 낮은 값을 갖게 됩니다. 굉장히 기발한 아이디어라고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 필요한 것을 모두 배웠으니 이를 정리하여 하나의 알고리즘으로 통합해 봅시다. 먼저 무작위로 $\mathbf{w}(0)$을 정하고, 기울기를 계산하여 다음 단계의 $\mathbf{w}$를 계산합니다. 이 과정을 최종 목적지(In Sample Error가 가장 낮은 지점)에 도달할 때까지 반복하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/9. Linear Model II/ML 09-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로, 지금까지 배운 선형 모델을 간단하게 정리해봅시다. 선형 모델에는 선형 분류, 선형 회귀, 로지스틱 회귀 3가지로 나뉩니다. 3장에서 배운 신용 카드 발급 문제를 예로 들면 선형 분류는 신용 카드를 발급할지/거부할지를 결정하는 모델이고, 선형 회귀는 신용 카드를 발급한다면 한도를 얼마로 정할 것인지를 결정하는 모델이며, 로지스틱 회귀는 이 사람이 파산할 확률(즉, 신용카드 대금을 갚지 못할 확률)을 계산하는 모델입니다. 세 모델의 목적이 모두 다르기 때문에 오류 측정 방법도 모두 다릅니다. 선형 분류는 얼마나 분류가 틀렸는지(Classification Error), 선형 회귀는 정답과 얼마나 멀리 떨어져 있는지(Squared Error), 마지막으로 로지스틱 회귀는 입력과 출력이 얼마나 관련이 있는지(Cross-Entropy Error)를 기준으로 삼게 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Bias-Variance Tradeoff</title><link href="http://localhost:4000/studies/bias-variance-tradeoff/" rel="alternate" type="text/html" title="Bias-Variance Tradeoff" /><published>2019-09-05T00:00:00+09:00</published><updated>2019-09-05T00:00:00+09:00</updated><id>http://localhost:4000/studies/bias-variance-tradeoff</id><content type="html" xml:base="http://localhost:4000/studies/bias-variance-tradeoff/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;8장에서는 Bias와 Variance에 대해 배웁니다. Bias-Variance는 지난 장에서 배운 VC 처럼 Error에 대해 분석하는 방법이지만, 직접적인 관련은 없으므로 VC를 제대로 이해하지 못하셨더라도 이번 장을 배우는 것에는 큰 무리가 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 크게 2개의 소주제로 구성되어 있습니다. 먼저 Bias와 Variance가 무엇인지 배우고, 이것이 어떤 의지를 가지는지를 예제를 통해 설명합니다. 아무래도 새로운 개념과 예제까지 다루다보니 대부분의 슬라이드는 여기에 할당되어 있고, 두번째 소주제는 VC를 사용한 분석과 어떤 차이가 있는지 Linear Regression의 예를 통해 살펴보게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지의 학습의 목적은 Out of Sample Error를 줄이는 것이었습니다. 이 말은 즉슨, Target Function $f$와 유사한 가설을 찾는 것입니다.&lt;/p&gt;

&lt;p&gt;가설을 좀 더 복잡하게 설정할수록(ex. 다항함수에서 차수를 늘림) $f$에 근사하도록 만들기 쉬워지지만, 가설을 간단하게 설정할수록(ex. 다항함수의 차수를 줄임) Out of Sample을 일반화하기 쉬워집니다. 물론, $f$ 자체를 가설로 설정할 수 있다면 더할 나위 없게 됩니다. (실제로는 불가능하지만요)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지난 시간에 배운 VC를 통해 Out of Sample Error는 In Sample Error에 $\Omega$를 더한 값 이하인 것을 알고 있습니다. ($\Omega$는 7장의 마지막 부분에서 VC Inequality 식으로부터 유도된 값입니다)&lt;/p&gt;

&lt;p&gt;Bias-Variance 분석은 이와 다르게 Out of Sample Error를 2가지로 나누어 분석합니다. &lt;strong&gt;가설 집합 $\mathcal{H}$가 얼마나 Target Function $f$에 근사하였는가&lt;/strong&gt;와 &lt;strong&gt;가설 집합 $\mathcal{H}$가 집합 내에서 좋은 가설 $h$를 뽑을 수 있는가&lt;/strong&gt;를 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;이 방법은 Regression과 같이 실수의 값을 가지는 Target에 적용할 수 있으며 Error Measure로 Squared Error를 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기계학습은 기본적으로 주어진 데이터를 가지로 수행하게 되므로, Out of Sample Error를 구하는 식을 살짝 변형시켜 보겠습니다. 무작위로 주어진 Dataset을 $\mathcal{D}$라 하면 $g^{(\mathcal{D})}$는 Dataset $\mathcal{D}$로 얻어진 Final Hypothesis $g$로 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;$g^{(\mathcal{D})}$를 사용해 Squared Error를 구하려면, $g^{(\mathcal{D})}$와 Target Function $f$의 차이를 제곱함으로써 얻을 수 있습니다. 다만 $\mathcal{D}$는 전체 Dataset $\mathbf{x}$에서 임의로 뽑은 Dataset이므로 어떻게 뽑는지에 따라 달라지기 때문에 Squared Error의 평균값을 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;그 후 식을 간단하게 정리하기 위해 양변에 $\mathcal{D}$에 대한 평균을 취합니다. 우변의 경우 $(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2$이 항상 0보다 크기 때문에 $\mathbb{E}_{\mathcal{D}}$와 $\mathbb{E}_{\mathbf{x}}$의 순서를 바꿀 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제는 $\mathbb{E}_{\mathcal{D}}[(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2]$에 집중해봅시다.&lt;/p&gt;

&lt;p&gt;먼저 가설 $g$의 평균으로 $\bar{g}$라는 것을 새롭게 정의합니다. $\bar{g}(\mathbf{x})$는 $g^{(\mathcal{D})}(\mathbf{x})$를 확률변수 $\mathcal{D}$로 평균낸 값입니다. $\mathcal{D}$는 원래 정의대로 전체 Dataset에서 무작위로 추출한 Dataset입니다.&lt;/p&gt;

&lt;p&gt;이런 추출 작업을 $K$번 반복했다고 하면 총 $K$개의 Dataset이 생기는데, $\bar{g}(\mathbf{x})$는 가설들의 평균이므로 $g^{(\mathcal{D}_k)}(\mathbf{x})$를 모두 더한 다음 $K$로 나눈 값이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정의는 잠시 제쳐 놓고, 구하려고 하던 $\mathbb{E}_{\mathcal{D}}[(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2]$를 전개해보도록 합시다. 오른쪽 항 $\mathbb{E}_{\mathcal{D}}$의 내부에 $\bar{g}$를 빼고 다시 더해줍니다. 똑같은 함수를 빼고 더했으니 원래의 식과 동일합니다. 그런 다음, 조금 번거롭지만 제곱식을 풀어준 다음 정리하도록 합시다.&lt;/p&gt;

\[\mathbb{E}_{\mathcal{D}}[(g^{(\mathcal{D})}(\mathbf{x})-\bar{g}(\mathbf{x})+\bar{g}(\mathbf{x})-f(\mathbf{x}))^2]\]

\[=\mathbb{E}_{\mathcal{D}}[g^{(\mathcal{D})}(\mathbf{x})^2-g^{(\mathcal{D})}(\mathbf{x})\bar{g}(\mathbf{x})+g^{(\mathcal{D})}(\mathbf{x})\bar{g}(\mathbf{x})-g^{(\mathcal{D})}(\mathbf{x})f(\mathbf{x})-g^{(\mathcal{D})}(\mathbf{x})\bar{g}(\mathbf{x})+\bar{g}(\mathbf{x})^2-\bar{g}(\mathbf{x})^2+\bar{g}(\mathbf{x})f(\mathbf{x})+g^{(\mathcal{D})}(\mathbf{x})\bar{g}(\mathbf{x})-\bar{g}(\mathbf{x})^2+\bar{g}(\mathbf{x})^2-\bar{g}(\mathbf{x})f(\mathbf{x})-g^{(\mathcal{D})}(\mathbf{x})f(\mathbf{x})+\bar{g}(\mathbf{x})f(\mathbf{x})-\bar{g}(\mathbf{x})f(\mathbf{x})+f(\mathbf{x})^2]\]

&lt;p&gt;(전개식 추가)&lt;/p&gt;

&lt;h2 id=&quot;bias-and-variance&quot;&gt;Bias and variance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 정리한 식에 각각 이름을 붙여봅시다.&lt;/p&gt;

&lt;p&gt;먼저 오른쪽 항의 두번째 부분은 가설 $g$의 평균 $\bar{g}$와 Target Function $f$의 Squared Error입니다. 이 부분은 정답과 가설들의 평균이 얼마나 떨어져 있는지, 즉, 정답에 대해 얼마나 편향적인지를 나타내는 지표로 해석할 수 있습니다. 따라서 이 부분을 Bias라고 부릅니다.&lt;/p&gt;

&lt;p&gt;첫번째 부분은 가설 집합 내의 특정 가설이 가설의 평균과 얼마나 떨어져 있는지를 측정하여 그것을 평균낸 값이 됩니다. 즉, 이 부분은 가설들이 얼마나 흩어져 있는지를 나타내는 지표로 해석할 수 있습니다. 그러므로 이 부분을 Variance로 부릅니다.&lt;/p&gt;

&lt;p&gt;결론적으로, 임의의 Dataset $\mathcal{D}$에 대해 가설 $g$의 Out of Sample Error는 Bias와 Variance의 합으로 나타낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 그림으로 표현한다면 더 직관적으로 표현이 가능합니다. 왼쪽의 그림은 전체 가설의 집합인 $\mathcal{H}$내에 최종 가설 $g^{(\mathcal{D})}$이 단 한개만 존재하여 그 자체가 평균 $\bar{g}$가 되는 극단적인 경우를 나타내고 있습니다. 이 때 그 점인 $\bar{g}$와 Target Function $f$의 거리가 Bias가 됩니다. 점이 단 하나밖에 없기 때문에, Variance는 0이 됩니다.&lt;/p&gt;

&lt;p&gt;오른쪽 그림은 전체 가설 집합 $\mathcal{H}$이 매우 큰 경우입니다. 포함된 점들도 매우 많고 Target Function $f$도 $\mathcal{H}$ 내에 존재합니다. 이 때는 최종 가설의 평균 $\bar{g}$과 $f$의 거리가 매우 가깝기 때문에 거리가 0에 수렴하므로, Bias는 0이라고 볼 수 있습니다. 반대로, $\bar{g}$와 나머지 점들의 거리는 왼쪽 그림에 비해 상대적으로 큰편이고, 특히 $\bar{g}$와 반대쪽에 있는 점들과의 거리는 매우 멀기 때문에 Variance는 매우 크게 나올 것임을 알 수 있습니다&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;$\mathcal{H}$를 크게 만들수록 Bias는 작아지고, Variance는 커진다&lt;/strong&gt;는 결론을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구체적인 예제를 보면서 Bias와 Variance를 계산하여 어떤 가설이 더 좋은가를 판단하는 법을 알아보겠습니다. Target Function $f$를 사인함수로 가정하고, 정의역의 범위는 -1부터 1까지로 잡겠습니다. 그리고 상수함수 모델을 가설로 한 $\mathcal{H}_0$, 일차함수 모델을 가설로 한 $\mathcal{H}_1$ 단 2개의 가설만 존재하는 상황을 가정하겠습니다. 과연 이 두 가설 중 “더 나은 것”은 무엇일까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“&lt;strong&gt;더 나은 것&lt;/strong&gt;“을 찾기 위해서는 어떤 것더 나은지에 대한 기준을 먼저 잡아야합니다. 지금까지 “더 낫다” 라는 것은 일반적으로 “&lt;strong&gt;Out of Sample Error가 낮다&lt;/strong&gt;“로 이해했기 때문에 Out of Sample Error를 계산해봅시다.&lt;/p&gt;

&lt;p&gt;Error Measure는 Square Error로 할 경우, 각 가설에서 Out of Sample Error가 가장 낮게 나오는 함수는 $\mathcal{H}_0$에서의 초록색 선, $\mathcal{H}_1$에서 빨간색 선으로 나타난 함수가 됩니다. Square Error 계산은 $f(x)$에서 방금 구한 함수를 뺀 제곱을 -1부터 1까지 적분하면 됩니다. 그렇게 하면 $\mathcal{H}_0$의 Out of Sample Error는 0.5, $\mathcal{H}_1$의 Out of Sample Error는 0.2가 되기 때문에 $\mathcal{H}_1$이 “더 낫다”라고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 전의 계산은 Target Function $f(x)$ 자체를 안다는 기준으로 계산했지만, 이번에는 모른다고 가정해보겠습니다. 대신 $f(x)$ 위에 있는 점 2개를 무작위로 추출하여 그 데이터를 가장 잘 표현하는 각 가설의 함수를 찾도록 합니다.&lt;/p&gt;

&lt;p&gt;$\mathcal{H}_0$은 어차피 상수함수이기 때문에 두 점을 받았을 때 그 중간을 지나는 직선이 가설임을 쉽게 알 수 있습니다. 가설 $\mathcal{H}_1$도 마찬가지로 일차함수이기 때문에 두 점을 지나는 직선을 긋게 되면 바로 가설을 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런 방법으로 두 점을 뽑는 과정을 여러번 반복합니다. 위 슬라이드의 왼쪽 그림은 임의의 두 점으로 만들 수 있는 가설 $\mathcal{H}_0$을 모두 그린 것이고, 오른쪽 그림은 그것들을 통해 가설 $\mathcal{H}_0$의 평균인 $\bar{g}$를 구하고, 가설들이 평균치 주변에서 주로 발생한다는 것을 나타낸 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가설 $\mathcal{H}_1$에서도 똑같은 작업을 수행한 모습입니다. $\mathcal{H}_0$와 비교해보면 가설이 발생하는 영역이 더 큰 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 결과를 가지고 Bias와 Variance를 구해봅시다. Bias는 Target Function $f$와 가설의 평균인 $\bar{g}$의 차이를 계산하면 되니 Out of Sample Error를 계산할 때처럼 두 함수의 차이를 적분해주면 됩니다. $\mathcal{H}_0$의 경우 이전과 같은 0.5이고, $\mathcal{H}_1$도 약간의 차이는 있지만 Out of Sample Error를 구할 때와 큰 차이가 없습니다.&lt;/p&gt;

&lt;p&gt;문제는 Variance인데, $\mathcal{H}_0$에서는 가설이 발생하는 영역이 오밀조밀하여 Variance가 크지 않지만, $\mathcal{H}_0$에서는 가설이 발생하는 영역이 이보다 훨씬 넓기 때문에 $\mathcal{H}_0$에 비해 Variance가 매우 크게 나옴을 알 수 있습니다. Bias와 Variance를 더하게 되면 0.75 &amp;lt; 1.9이므로 $\mathcal{H}_0$이 $\mathcal{H}_1$보다 더 좋은 가설임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왜 이런 결과가 나왔을까요? 그 이유는 모델의 복잡도는 &lt;span style=&quot;color:red&quot;&gt;Target Function이 얼마나 복잡한가&lt;/span&gt;가 아니라 &lt;strong&gt;데이터&lt;/strong&gt;를 기준으로 잡아야하기 때문입니다.&lt;/p&gt;

&lt;p&gt;방금의 예제에서는 데이터가 단 2개의 점으로만 주어졌습니다. 점 2개를 가지로 구할 수 있는 함수는 매우 한정적이기 때문에 모델의 복잡도가 낮은 $\mathcal{H}_0$이 더 뛰어난 성능을 보인 것입니다.&lt;/p&gt;

&lt;p&gt;만약 똑같은 Target Function을 가지고 점이 더 많이 주어진다면, $\mathcal{H}_1$이 더 뛰어난 성능을 보일 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;learning-curves&quot;&gt;Learning curves&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 데이터 집합의 크기에 따라 In Sample Error와 Out of Sample Error가 어떤 곡선을 그리는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터 집합 $\mathcal{D}$의 크기를 $N$이라 정의합니다. 이 때, Out of Sample Error의 평균은 $\mathbb{E}_{\mathcal{D}}[E_{out}(g^{\mathcal{D}})]$가 되고 In Sample Error의 평균은 $\mathbb{E}_{\mathcal{D}}[E_{in}(g^{\mathcal{D}})]$가 됩니다. 이 두 평균이 $N$에 따라 어떻게 변하는지를 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서는 간단한 모델과 복잡한 모델로 나누어 실험하였습니다. 우선 공통적인 부분은 두 모델 모두 $N$이 충분히 커진다면 In Sample Error와 Out of Sample Error가 비슷해짐을 알 수 있습니다. 다만 간단한 모델은 더 빠르게 두 곡선이 만나게 됩니다.&lt;/p&gt;

&lt;p&gt;간단한 모델의 경우 머릿속으로 생각해왔던 당연한 그림의 결과이기 때문에 논할 것이 없지만, 복잡한 모델의 곡선은 조금 특이합니다. $N$이 작을때 Out of Sample Error가 매우 커지는 것은 당연해보이지만, In Sample Error는 0이 되어 버립니다. 이렇게 되는 이유는 복잡한 모델은 주어진 데이터가 적을 때 그 데이터를 완전히 커버하는 것이 가능하기 때문입니다. 이전에 했던 사인 함수 예제에서 점이 2개 주어진 경우를 생각해보시면, 상대적으로 복잡한 모델인 일차함수의 경우 주어진 2개의 점을 지나는 함수를 구할 수 있기 때문에 In Sample Error가 0이었던 것으로 이해하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 똑같은 곡선을 지난 장에서 배운 VC Analysis와 이번 장에서 배운 Bias-Variance Analysis의 관점으로 각각 살펴보겠습니다. VC Analysis에서는 Out of Sample Error를 In Sample Error와 Generalization Error(=$\Omega$)를 더한 값 이하로 표현하였습니다. 반대로 Bias-Variance Analysis에서는 Out of Sample Error를 Bias와 Variance의 합으로 표현하였습니다.&lt;/p&gt;

&lt;p&gt;그림을 통해 알 수 있는 유사점은 $N$이 커질수록 In Sample Error와 Out of Sample Error가 비슷해지기 때문에 Generalization Error와 Variance가 0에 수렴한다는 것이지만, VC Analysis는 $N$이 작을때 In Sample Error가 0에 가깝기 때문에 제 기능을 할 수 없는 단점이 있다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이해하기 쉽게 특정한 케이스를 예로 들어봅시다.&lt;/p&gt;

&lt;p&gt;이전에 배운 Linear Regression 문제에서 Target에 Noise가 낀 상황입니다. 데이터 집합의 크기는 $N$이고, 3장에서 배운 Pseudo-Inverse를 사용하여 최적의 Weight Vector $\mathbf{w}^{*}$를 찾도록 합시다.&lt;/p&gt;

&lt;p&gt;Input Vector를 $X$라고 하면 $X\mathbf{w}$와 Output Vector $y$의 차이가 In Sample Error Vector가 됩니다. 이 문제에서, Target에는 Noise가 추가되었기 때문에 주어진 데이터 집합 바깥에는 주어진 데이터 집합과 동일한 Input을 가지고 다른 Output를 가지는 데이터가 존재합니다.&lt;/p&gt;

&lt;p&gt;따라서 Out of Sample Error는 주어진 데이터 집합과 동일한 Input Vector $X$와 주어진 데이터 집합과 다른 Output Vector $y’$를 사용하여 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/8. Bias-Variance Tradeoff/ML 08-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제를 가지고 슬라이드 20에 나온 곡선을 분석해봅시다.&lt;/p&gt;

&lt;p&gt;먼저, Noise의 분산을 $\sigma^2$이라 하면, $N$이 커질수록 In Sample/Out of Sample Error는 Noise의 분산에 수렴함을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;Pseudo-Inverse에서는 $d+1$의 크기를 가지는 행렬로 계산했기 때문에 $N$이 $d+1$와 같은 상황에서는 주어진 데이터에 오차없이 가설을 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;$N$이 $d+1$보다 커지는 상황에서는, 주어진 점에 완전히 일치하는 가설을 찾을 수 없기 때문에 In Sample Error가 점점 늘어납니다. 이를 수식화하게 되면, $\sigma^2$을 기준으로 $N$이 $d+1$와 같을 때 0이되고, “Linear” Regression 문제이므로 $\sigma^{2}(1-\frac{d+1}{N})$이라는 식을 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Out of Sample Error도 같은 방법으로 구할 수 있습니다. Generalization Error는 슬라이드 20에서 Out of Sample Error와 In Sample Error의 차이였기 때문에 간단하게 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">VC Dimension</title><link href="http://localhost:4000/studies/VC-demension/" rel="alternate" type="text/html" title="VC Dimension" /><published>2019-08-29T00:00:00+09:00</published><updated>2019-08-29T00:00:00+09:00</updated><id>http://localhost:4000/studies/VC-demension</id><content type="html" xml:base="http://localhost:4000/studies/VC-demension/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;7장에서는 지난 장 마지막에 배운 Vapnic-Chervonenkis (VC) Dimension에 대해 자세히 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장은 4개의 소주제로 나뉘어 있습니다. 먼저 VC Dimension의 정의를 배우고, Perceptron에서의 VC Dimension이 어떻게 되는지 예제를 통해 알아봅니다. 세 번째로는 VC Dimension이 수학적으로/기계학습에서 어떤 의미를 가지는지 알아보고 마지막으로는 Hoeffding’s Inequality를 변형한 VC Inequality에서 각종 변수의 상한/하한점을 찾게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;the-definition&quot;&gt;The definition&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5장에서 배웠던 Break Point의 개념을 기억하고 계신다면 VC Dimension의 정의를 배우는 것은 어렵지 않습니다. Break Point $k$의 정의는 데이터를 흩뿌릴 수 없는, 다시말해 어떻게 데이터를 배치해도 Growth Function이 $2^k$보다 작은 $k$를 의미했습니다. VC Dimension은 이와 유사하게, 최대로 데이터를 흩뿌릴 수 있는 $k$를 의미합니다. 즉, &lt;span style=&quot;color:red&quot;&gt;Break Point - 1 = VC Dimension&lt;/span&gt; 입니다. VC Dimension은 $d_{VC}$로 표기합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;비교를 위해, 지난 시간에 Growth Function이 다항함수임을 증명했을 때 배운 식으로 Break Point와 VC Dimension의 표기 차이를 비교해보면 단순히 $k-1$이 $d_{VC}$로 치환되었음을 알 수 있습니다. 즉, 최고차 항의 지수는 $d_{VC}$까지 나올 수 있습니다. (최대 지수를 의미하는 것이지, 항상 $d_{VC}$가 최대임을 의미하는 것이 아닙니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간단한 예제를 통해 VC Dimension을 구해봅시다. 첫째로 Positive Ray는 특정한 점을 기준으로 한쪽은 모두 -1, 다른 쪽은 모두 +1로 분류하였습니다. 이 때의 Growth Function이 $N+1$이었으므로 Break Point는 $k=2$가 됩니다. VC Dimension은 Break Point보다 하나 작은 값이기 때문에 $d_{VC}=1$임을 쉽게 알 수 있습니다. 두 번째 2D Perceptron도 마찬가지로 Break Point $k=4$이었기 때문에 $d_{VC}=2$가 되고, 마지막으로 Convex Set의 경우 Break Point가 무한대(Infinite)였기 때문에 VC Dimension 역시 무한대가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 VC Dimension이 Learning에 어떤 관계가 있는지 알아봅시다. 가장 먼저, VC Dimension이 유한하다는 뜻은 곧 최종 가설 $g$이 일반화된다는 뜻입니다. (오른쪽 그림에서의 초록색 부분) 또한 Learning Algorithm, Input Distribution, Target Function에 독립적이기 때문에 이들을 고려할 필요가 없습니다. 다만 Hypothesis Set은 VC Dimension을 정의하기 위해 필요하기 때문에 독립적이지 않습니다. Tranining Example 또한 일반화에 필요하므로 무시할 수 없습니다. 이들은 추후 VC Inequality에 다시 등장하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;vc-dimension-of-perceptrons&quot;&gt;VC dimension of perceptrons&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞의 예제들을 통해 특수한 경우의 VC Dimension 값을 구했지만, 제대로 사용하기 위해서는 Growth Function을 배웠을 때처럼 일반항이 필요합니다. 일반적으로 VC Dimension의 값은 데이터의 차원 + 1과 같은데, 그 증명이 그렇게 어려운 편이 아니기 때문에 강의에서도 짚고 넘어갑니다. 증명을 스킵하실 분들은 슬라이드 16으로 바로 건너뛰셔도 됩니다.&lt;/p&gt;

&lt;p&gt;증명의 방법은 두 집합이 같은 것을 증명하는 방법과 유사합니다. 일반적으로 A, B 두 집합이 같음을 증명할 때는 A 집합이 B 집합에 포함되는 것을 증명하고, B 집합이 A 집합에 포함된다는 것을 보이는 방법으로 증명하는데, 여기에서도 먼저 $d_{VC}$가 데이터의 차원 + 1보다 이상이라는 것과 이하라는 것을 각각 보여 둘을 합치는 방식으로 같음을 증명합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 $d$ 차원을 갖고 있는 $d+1$개의 점을 준비합니다. $d$ 차원이지만 Perceptron에서 항상 0번째 원소는 Threshold로 배정을 해 두었기 때문에 위의 슬라이드와 같이 Transpose를 사용해 $d+1$개의 점을 하나의 행렬 $X$로 만든다면 정사각 행렬이 됩니다. (첫 번째 열이 모두 1인 이유는 Threshold에 사용한 0번째 원소가 Transpose로 들어갔기 때문입니다.) 나머지 점들은 선형 독립이 되도록 배치하게 된다면 행렬 $X$는 역행렬이 존재하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 데이터 셋에서 Output $y$를 임의로 만들어준 후에, Input $X$와 행렬곱을 했을 시 결과물의 부호가 Output $y$와 같은 가중치 벡터 $w$를 찾아봅시다. 어렵게 찾을 필요 없이, 그냥 $Xw=y$가 성립하는 $w$를 만들면 됩니다. 이전 슬라이드에서 행렬 $X$는 역행렬이 존재한다고 했으니, $w=X^{-1}y$임을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 결과로 알 수 있는 것은 무엇일까요? 지금까지 전개한 내용은 $d$ 차원의 $d+1$개의 점을 배치했을 때 임의의 Output $y$에 대해서 $w$를 항상 구할 수 있다는 것이었습니다. 즉, 이 말은 $d+1$개의 점을 흩뿌릴 수 있다는 말과 동일하게 되니, $d_{VC}$는 최소한 $d+1$개 이상이다라는 말과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반대로 이제는 $d_{VC}$가 $d+1$ 이하라는 것을 보일 차례입니다. $d$는 차원을 의미하기 때문에 당연히 정수라는 것을 생각해본다면, 모든 $d+2$개의 점을 흩뿌릴 수 없다는 것을 보인다면 자연스레 $d_{VC}$가 $d+1$ 이하라는 것을 보일 수 있음을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 이번에는 $d$ 차원인 $d+2$개의 임의의 점을 만들어봅시다. 여기서 알 수 있는 것은 차원의 수가 점의 수보다 적다는 겁니다. 따라서, 모든 점이 선형 독립인 것은 불가능합니다. 간단하게 생각해서 3차원의 점을 생각해본다면 선형 독립인 기저점은 $(1, 0, 0), (0, 1, 0), (0, 0, 1)$ 3개입니다. 만약에 여기서 그 어떤 점이 추가된다고 하더라도, 3개의 기저점을 적당히 조합한다면 만드는 것이 가능함이 자명합니다. 즉, $a \times (1, 0, 0) + b \times (0, 1, 0) + c \times (0, 0, 1) = $ 임의의 점이 성립한다는 것이고, 최소한 $a, b, c$ 중 하나는 0이 아니라는 겁니다. 마찬가지로 슬라이드에서도 어떤 점 $x_j$을 다른 점들의 실수 배 $a_i$를 곱한 벡터를 더한다면 표현이 가능하다는 것이고, 최소한 그 $a_i$ 중 몇몇은 0이 아님을 보장할 수 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것이 의미하는 바는 상당히 큽니다. 만약 $x_j$ 중 0이 아닌 $a_i$를 생각해봅시다. $a_i$가 0이 아닌 점 $x_i$는 Output이 $y_i = sign(a_i)$와 같기 때문에 $x_j$의 Output $y_i$는 무조건 -1이 나올 수밖에 없습니다. 이 말은 +1/-1이 모두 가능하지 않다는 뜻이므로 $d+2$개의 점들을 흩뿌릴 수 없다는 이야기와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왜 그렇게 되는지를 자세히 한번 알아보겠습니다. 위 슬라이드 맨 위에 나오는 식은 이전 슬라이드에 나왔던 식과 같습니다. 여기에 가중치 벡터 $\mathbf{w}$를 Transpose 시킨 다음 양 변에 똑같이 곱합니다. Output $y_{i}$의 정의에 의해, $sign(\mathbf{w}^{\sf T} \mathbf{x})$와 같습니다. 이전 페이지에서 $a_{i}$의 부호가 $y_{i}$의 부호와 같다고 했으니, $a_{i} \mathbf{w}^{\sf T} \mathbf{x}$는 무조건 0보다 클 수밖에 없습니다.&lt;/p&gt;

&lt;p&gt;즉, $y_{j}$는 무조건 +1로 분류가 될 수밖에 없게 됩니다. 결론적으로, $d+2$개의 점은 흩뿌릴 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지의 과정을 통해 $d_{VC}$가 $d+1$ 이하라는 것과 $d_{VC}$가 $d+1$ 이상이라는 것을 모두 보였으므로 $d_{VC}$와 $d+1$이 같다는 것을 알 수 있습니다. $d+1$인 이유는 각각의 점이 $d$ 차원으로 이루어져 있는 것 + Threshold를 위한 0번째 원소의 존재라고 생각하시면 됩니다. 다시 말해, Input $X$를 통해 임의의 $y$ 값을 구할 수 있기 위해서는 Weight Vector $w$가 $d+1$ 차원이면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;interpreting-the-vc-dimension&quot;&gt;Interpreting the VC dimension&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 VC Dimension이 의미하는 바가 무엇인지를 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫째로, VC Dimension으로 모델의 자유도를 구할 수 있습니다. 물론, 매개변수의 개수도 자유도에 영향이 있습니다. 다만 자유도에 영향을 주는 방향이 서로 다릅니다. 쉬운 예시로 위 슬라이드 오른쪽 그림과 같이 여러 개의 노브가 달려있는 오디오 시스템을 생각해봅시다. 최적의 음악을 듣기 위해서는 각각의 노브를 이리저리 돌려봐야 합니다. 매개변수는 각각의 매개변수에 대해 실수 Weight를 부여해야 합니다. 노브를 기준으로 생각해본다면, 어떤 노브를 얼만큼 돌려야 최적의 소리가 나오는지 찾는다고 생각해보시면 됩니다. VC Dimension을 노브에 빗대어 설명한다면, 이 노브를 돌릴 것인지 말 것인지를 결정한다고 보시면 됩니다. 똑같이 자유도를 판단하는 기준이지만, 이렇게 약간의 차이가 있음을 알게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 여러번 보았던 예제들로 자유도를 구해봅시다. Positive Ray 예제는 $a$를 어디에 놓는가에 따라 데이터의 +1/-1 여부가 결정됩니다. 즉, 이 예제에서는 매개변수가 1개, VC Dimension도 1, 자유도도 1이 됩니다.&lt;/p&gt;

&lt;p&gt;Positive Interval 예제도 마찬가지로, Interval을 확정하기 위해서는 두 개의 점을 정하는 것이 필요합니다. 즉, 매개변수는 2개가 되며 VC Dimension, 자유도도 마찬가지로 2 임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 보면 자유도는 매개변수의 개수만 알면 구할 수 있지 않은가 하고 의문을 가지실 수도 있습니다. 매개변수의 갯수는 자유도에 영향을 주기는 하지만 그렇지 않은 경우도 있습니다. 가령 위의 슬라이드에 나온 것과 같이 1차원 Perceptron을 4개 연달아 배치한 예제를 살펴보겠습니다. 1차원 Perceptron이기 때문에 하나의 Perceptron에 사용되는 매개변수는 $w_0, w_1$로 2개입니다. 즉, 총 매개변수의 개수는 2+2+2+2=8개가 됩니다.&lt;/p&gt;

&lt;p&gt;얼핏 보면 8개의 매개변수를 갖고 있기 때문에 8의 자유도를 가지고 있다고 착각할 수가 있습니다. 하지만 자세히 보시면 첫 번째 Perceptron을 거치는 순간 Output은 +1/-1 단 두가지 경우만 가능하게 됩니다. 즉, 두 번째 Perceptron의 Input도 +1/-1 단 두 가지 경우밖에 없다는 것입니다. 세 번째, 네 번째 Perceptron도 마찬가지입니다. 따라서 이 예제의 자유도는 2가 됩니다.&lt;/p&gt;

&lt;p&gt;만약에 VC Dimension을 사용해 자유도를 찾게 된다면 매개변수의 갯수를 사용해 찾는 것보다 훨씬 간단하게 됩니다. 이 경우에는 1차원 Perceptron이므로 $d=1$입니다. VC Dimension은 차원 + 1이기 때문에 간단하게 $d_{VC}=2$임을 알 수 있습니다. VC Dimension과 자유도는 같으므로 자유도도 2 임을 바로 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;VC Dimension의 두 번째 의미는 VC Inequality를 만족하기 위해 필요한 데이터 수를 대략적으로 제공한다는 것입니다. VC Inequality도 Hoeffding’s Inequality와 마찬가지로, 의미있는 식이 되기 위해서는 오른쪽 항이 1보다 작아야 할 필요가 있습니다. 오른쪽 항을 작게 만들기 위해서 필요한 것은 충분한 크기의 $N$인데, 어느 정도의 값이 되어야 충분한지 한번 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 오른쪽 항이 조금 복잡한 형태이므로 간단하게 바꾸어 보겠습니다. 지난 장에서 Growth Function이 Dimension에 대해 Polynomial 하다는 것을 증명하였고, $e$ 지수에 달려있는 복잡한 식을 $N$만 남기고 모두 생략한다면, $N^{d}e^{-N}$ 으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 슬라이드의 오른쪽 그림은 $d$의 크기를 5부터 30까지 5씩 늘려가며 그린 그래프입니다. 세로축의 단위 간격이 매우 커서 그래프가 살짝 이상하게 보이지만, 실제 그래프의 모양은 아래 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-22-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그래프에서 파란색 선이 $d=5$일 때의 그래프 모양입니다. $x$축 위에 있는 가로줄은 $y=1$을 의미합니다. 즉, 파란색 그래프가 $y=1$보다 작은 구역에 있어야만 VC Inequality에 의미가 생기게 됩니다. 그림상으로는 대략 13 정도로 보입니다.&lt;/p&gt;

&lt;p&gt;이와 같은 방법으로 모든 $d$에 대해 적당한 $N$을 구해야 하는데, 일반적으로 정확하게 구하지 않고 대략적인 값(Rule of Thumb)을 구한다고 합니다. 이유는 데이터는 어차피 필요한 최소의 개수만 구하고 그만두지 않고 많으면 많을수록 좋기 때문이라고 생각합니다. 대략적으로 필요한 양은 VC Dimension의 10배 이상이라고 합니다.&lt;/p&gt;

&lt;p&gt;여담으로 Rule of Thumb이란 이름이 붙은 이유는 측량 도구가 없던 옛날 시절, 길이를 재기 위해 엄지손가락을 많이 사용해서라고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;generalization-bounds&quot;&gt;Generalization bounds&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 VC Dimension을 이용해 VC Inequality에 사용되는 변수들의 상한/하한점을 간단하게 구해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;슬라이드 20에서도 다루었지만, 식을 조금 변형시키기 위해 VC Inequality의 오른쪽 항 전체를 $\delta$로 치환해 보겠습니다. 식을 $\epsilon$에 대해 정리하기 위해 $\epsilon$를 제외한 항들을 모두 $\delta$가 있는 항으로 이항시키면 위 슬라이드의 두 번째 식이 나옵니다. $\epsilon$은 In Sample Error와 Out of Sample의 차이를 의미하기 때문에 이 값이 줄어들수록 좋은데, 그러기 위해서는 역시 $N$이 커야함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;$\epsilon$의 오른쪽 항을 $\Omega$ 함수로 정의한다면, 이 $\Omega$ 함수는 데이터의 수 $N$, 가설 $\mathcal{H}$, VC Inequality의 오른쪽 항 $\delta$에 영향을 받는 함수로 표현이 가능합니다.&lt;/p&gt;

&lt;p&gt;추가로 VC Inequality 왼쪽 항의 확률은 “Bad Event”가 일어날 확률을 의미한다고 배웠습니다. 이를 “Good Event”로 바꾸게 되면 $1-\delta$의 확률로 In Sample Error와 Out of Sample의 차이가 $\Omega$ 이하라고 해석이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/7. VC Dimension/ML 07-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 전 슬라이드 마지막에 유도한 식을 조금만 바꾸어 보겠습니다. 절댓값 부분을 없애고 싶은데, 일반적으로 Out of Sample Error가 In Sample Error보다 크므로 절댓값 부분을 없애게 되면 순서가 바뀌게 됩니다. 여기서 In Sample Error $E_{in}$을 오른쪽 항으로 이항 하게 되면 최종적으로 Out of Sample Error는 In Sample Error에 $\Omega$를 더한 값 이하라는 결론을 내릴 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Theory of Generalization</title><link href="http://localhost:4000/studies/theory-of-generalization/" rel="alternate" type="text/html" title="Theory of Generalization" /><published>2019-08-23T00:00:00+09:00</published><updated>2019-08-23T00:00:00+09:00</updated><id>http://localhost:4000/studies/theory-of-generalization</id><content type="html" xml:base="http://localhost:4000/studies/theory-of-generalization/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6장에서는 지난 장에서 다루었던 $m_{\mathcal{H}}$에 대한 증명들을 배우게 됩니다.&lt;/p&gt;

&lt;p&gt;증명을 다루는 만큼, 이번 장은 가장 이론적인 내용을 담고 있습니다. 수학식이 많이 나오기 때문에 다소 지루할 수 있다는 것을 미리 말씀드립니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 증명은 크게 두 가지로 나뉘어 있습니다.&lt;/p&gt;

&lt;p&gt;첫째는 $m_{\mathcal{H}}$가 다항 함수인 것을 증명하는 것이고, 두 번째는 $m_{\mathcal{H}}$가 Hoeffding’s Inequality에서 $M$을 대체하기 위한 증명입니다. 이전 장까지 계속 Hoeffding’s Inequality에서 오른쪽 항의 값이 1보다 큰 것이 문제였기 때문에 이것을 해결하기 위한 과정이라고 생각하시면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;proof-that-m_mathcalh-is-polynomial&quot;&gt;Proof that $m_{\mathcal{H}}$ is polynomial&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$m_{\mathcal{H}}(N)$이 $N$에 대해서 다항함수임을 증명하려면, 부등식을 통해 $m_{\mathcal{H}}(N)$이 $N$으로 이루어진 다항식보다 작거나 같다는 것을 증명하면 됩니다. 다항식의 차수를 걱정하실 수도 있지만, 다항식의 차수가 어떻든 $N$이 충분히 커지게 되면 지수함수 보다 작아지기 때문에 차수 자체는 큰 문제가 아닙니다.&lt;/p&gt;

&lt;p&gt;본 증명을 하기 앞서 새로운 함수인 $B(N, k)$를 정의합니다. 이것은 $N$개의 점이 있고 Break Point가 $k$일 때 가능한 Dichotomy의 &lt;strong&gt;최대 개수&lt;/strong&gt;를 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 정의한 $B(N, k)$ 함수를 구체적인 변수들로 표현하기 위해 위 슬라이드의 오른쪽 표를 참고하여 따라가보겠습니다. 오른쪽 표에서 $\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_N$은 각각 $N$개의 점을 의미합니다. 이 값은 분류된 결과를 의미하므로 +1이나 -1의 값을 가질 수 있습니다.&lt;/p&gt;

&lt;p&gt;가능한 Dichotomy 들의 조합 중에서, $\mathbf{x}_1, …, \mathbf{x}_{N-1}$ 까지의 값은 같고, $\mathbf{x}_N$의 값만 정 반대인 집합을 $S_2$라고 정의하고, 그렇지 않은 것들을 모아둔 집합을 $S_1$이라고 정의하겠습니다. 그럼 모든 Dichotomy 들의 조합들은 $S_1$이나 $S_2$ 둘 중 하나에 포함될 것이라는 것을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;그 중에서 $S_2$를 좀 더 세분화하기 위해, $\mathbf{x}_N$의 값이 +1인 것들의 집합을 $S^+_2$이라 하고, $\mathbf{x}_N$의 값이 -1인 것들의 집합을 $S^-_2$라고 하겠습니다. 당연히 $S^+_2$와 $S^-_2$의 갯수는 같을 수밖에 없습니다.&lt;/p&gt;

&lt;p&gt;$S_{1}$의 원소의 개수를 $\alpha$라고 하고, $S^+_2$, $S^-_2$의 원소의 개수를 각각 $\beta$라고 하면 $B(N, k)$의 값은 $\alpha + 2\beta$로 표현이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 점 $\mathbf{x}_{N}$을 삭제해보겠습니다. 다시 말해 $N-1$개의 점에서 Break Point가 $k$일 때 $B(N-1, k)$를 구하자는 것입니다. 직관적인 생각으로는 $\alpha + \beta = B(N-1, k)$가 되는 것일텐데, 왜 $\alpha + \beta \leq B(N-1, k)$인지 의문이 들 수도 있습니다.&lt;/p&gt;

&lt;p&gt;그 이유는 $B(N-1, k)$는 가능한 Dichotomy의 &lt;strong&gt;최대 개수&lt;/strong&gt;라고 정의되었기 때문입니다. 방금처럼 점이 $N$개였던 표에서 $\mathbf{x}_{N}$만 지운 표가 점이 $N-1$일 때의 Dichotomy의 최대 개수인지 아니면 일부분인지 모르기 때문에 등호가 아닌 부등호가 들어간 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마찬가지로 이번에는 $B(N-1, k-1)$을 구해볼 것인데, 역시 점이 $N-1$개이므로 표에서 점 $\mathbf{x}_{N}$을 삭제해보겠습니다. 이전 슬라이드와 마찬가지로 $B(N-1, k-1)$가 $\beta$ 이상이라고 부등호가 되어있는데, 왜 이것이 $k-1$의 Break Point를 가지는지 직관적으로 이해하기 쉽지 않습니다.&lt;/p&gt;

&lt;p&gt;만약에 $\beta$가 $k$개의 Break Point를 가진다고 가정한다면, Break Point의 정의에 의해서 $2^{k-1}$개의 Dichotomy를 표현 가능해야 합니다. 그러나 처음 가정으로 $B(N, k)$이 $k$개의 Break Point를 가지고 있기 때문에 그것의 일부분인 $\beta$는 $k$개의 Break Point를 가질 수 없습니다. (즉, $\mathbf{x}_{N}$을 추가했을 때 $2^{k}$개의 Dichotomy를 표현한다는 것에 모순입니다.)&lt;/p&gt;

&lt;p&gt;반대로 $\beta$가 $k-1$개의 Break Point를 가진다는 것을 보이는 것은 쉽습니다. $S^+_2, S^-_2$는 $\mathbf{x}_N$의 값을 제외하면 나머지 $\mathbf{x}_1, \mathbf{x}_2, … \mathbf{x}_N$ 요소가 같은 값이기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전의 두 슬라이드의 내용을 정리해보면 다음과 같습니다.&lt;/p&gt;

\[B(N, k) = \alpha + 2\beta\]

\[\alpha + \beta \leq B(N-1, k)\]

\[\beta \leq B(N-1, k-1)\]

&lt;p&gt;따라서 위 식에서 두 번째 식과 세 번째 식을 양변을 더해주면 아래와 같은 식을 유도할 수 있습니다.&lt;/p&gt;

\[B(N, k) \leq B(N-1, k) + B(N-1, k-1)\]

&lt;p&gt;이를 토대로 임의의 $N$, $k$에 대해서 $B(N, k)$의 값을 Recursive하게 계산할 수 있다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 유도한 식을 토대로 $N$과 $k$에 따라 $B(N, k)$값이 어떻게 변하는지 위 슬라이드의 오른쪽 표에 나와있습니다. 표를 채우려면 먼저 $B(N, 1)$과 $B(1, k)$를 구해야 하는데, $B(N, 1)$은 점이 몇 개가 주어지든 Break Point가 1이란 얘기니까 무조건 딱 한 가지로만 분류가 가능하다는 뜻입니다. 따라서 $B(N, 1)=1$이 됩니다.&lt;/p&gt;

&lt;p&gt;이번엔 $B(1, k)$를 계산해야 하는데, 점이 딱 1개만 있으면 어차피 나눌 수 있는 경우의 수는 +1 또는 -1 밖에 없으니까 Break Point가 아무리 커봤자 $B(1, k)=2$라는 것을 쉽게 알 수 있습니다. (단, $k=1$일 때 제외)&lt;/p&gt;

&lt;p&gt;이렇게 첫 번째 행/열만 채우게 되면 나머지는 이전 슬라이드에서 보였던 부등식을 통해 채울 수 있습니다. 이 표의 값들을 보실 때 주의하실 점은, $B(N, k)$의 정확한 값이 아니라 &lt;strong&gt;상한(Upper Bound)&lt;/strong&gt;이라는 겁니다. 애초에 유도한 재귀식 자체가 부등식이기 때문이죠.&lt;/p&gt;

&lt;p&gt;즉 표의 값을 읽을 때, 예를 들면 표에서 붉은 글씨를 확인해 보시면 $B(3, 2)$이 4라고 나와있습니다. 이 말은 $B(3, 2)$가 정확하게 4라는 뜻이 아니라 아무리 커봤자 최대가 4라는 뜻입니다. 물론 4보다 작을 수도 있습니다. (이건 직접 하나하나 계산해보기 전까지는 모릅니다.)&lt;/p&gt;

&lt;p&gt;그럼 실제로 $B(3, 2)$의 값이 뭔지 궁금합니다. 운이 좋게 이것은 저희가 이미 계산해본 적 있습니다. 5장의 맨 마지막 슬라이드에서 간단한 Puzzle을 풀었었는데, 이때가 $N=3, k=2$의 예제였습니다. 계산했을 때 값이 4가 나왔었는데, 우연히도 $B(3, 2)$의 상한과 같은 값임을 알 수 있습니다. 하지만 이것은 우연일 뿐, 항상 이렇게 같은 값이 나오지는 않음에 유의하셔야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 이렇게 Recursive하게 계산하는 것은 계산 속도도 오래 걸리고 귀찮기 때문에, 이를 한 번에 표현할 수 있는 일반항을 찾아야 합니다. 위 슬라이드에 나온 것처럼, $B(n, k)$에 대한 일반항을 조합(Combination)들의 합으로 제시하고 이를 증명합니다.&lt;/p&gt;

&lt;p&gt;증명 방법은 흔히 사용하는 &lt;strong&gt;수학적 귀납법(Induction)&lt;/strong&gt;으로 증명합니다. 먼저 맨 처음 항이 True임을 보여야 하는데, 이건 그냥 $N=1$과 $k=1$을 각각 넣게 되면 참임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그다음의 과정이 조금 흥미로운데, 여기서는 $B(N-1, k)$의 합과 $B(N-1, k-1)$의 합이 $B(n, k)$가 되는지를 보였습니다. 첫 번째 줄에서 두 번째 줄로 넘어갈 때는 두 항을 합치기 위해 시그마를 똑같이 $i=1$부터 $i=k-1$까지 맞춰눈 것이고, 세 번째 줄에서 네 번째 줄로 넘어간 것은 조합에서 사용하는 파스칼의 삼각형을 사용해서 두 개의 조합을 하나로 합친 것입니다.&lt;/p&gt;

&lt;p&gt;이 외에는 단순한 계산이기 때문에 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적인 결론입니다. 이전 슬라이드까지의 과정을 통해서 결국 $m_{\mathcal{H}}$가 조합들로 이루어진 합보다 작다는 것이 증명되었고, 이 조합은 아무리 커봤자 $N^{k-1}$의 항을 가진 다항함수이므로 결과적으로 그토록 원하던 “&lt;span style=&quot;color:red&quot;&gt;Growth Function이 다항함수(Polynomial)이다&lt;/span&gt;” 라는 결론이 나온 것입니다.&lt;/p&gt;

&lt;p&gt;지금까지 Hoeffding’s Inequality가 무한대에 가까운 $M$으로 고통받았던 것을 생각해보면 Growth Function을 통해 다항함수 꼴로 줄인 것은 매우 큰 의미가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왜 그것이 큰 의미가 있는지 이 슬라이드를 통해 설명할 수 있습니다. 이전 장에서 Growth Function을 설명할 때 사용한 3가지 예제가 기억나실 겁니다. (Positive Ray, Positive Interval, Convex Set)&lt;/p&gt;

&lt;p&gt;이 중에서 Positive Ray와 Positive Interval은 지난 장에서 직접 계산했기 때문에, 방금 유도한 Growth Function의 상한과 비교해 보겠습니다. 운이 좋게 Positive Ray와 Positive Interval은 상한과 똑같이 나왔습니다만, 아까도 말씀드렸듯이 항상 똑같은 게 아니라는 걸 꼭 기억하셔야 합니다.&lt;/p&gt;

&lt;p&gt;그런데 2D Perceptron은 이전에 Break Point가 4인 것은 보였지만, Growth Function을 직접 구할 수는 없었습니다. 하지만 방금 유도한 Growth Function 상한을 이용하면 2D Perceptron도 공식에서 나온 다항함수보다 작다는 것을 보일 수 있습니다.&lt;/p&gt;

&lt;p&gt;결국 이런 방식을 통해 그 어떠한 케이스에서도 Break Point만 찾으면 Growth Function이 다항함수 꼴로 상한이 정해진 다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;proof-that-m_mathcalh-can-replace-m&quot;&gt;Proof that $m_{\mathcal{H}}$ can replace $M$&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 지금까지 $m_{\mathcal{H}}$가 다항함수로 이루어진 식의 상한으로 이루어진 것이 증명되었으니, 이제 정말 중요한 Hoeffding’s Inequality에 있던 $M$ 대신에 $m_{\mathcal{H}}$를 대입하기 위한 증명이 필요합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 우리가 원하는 결과는 Hoeffding’s Inequality에서 $M$ 자리에 그대로 $m_{\mathcal{H}}$가 들어가는 것이지만, 안타깝게도 $M = m_{\mathcal{H}}$는 말이 안 돼니 직접적으로 교체할 수는 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어쨌든 이 $M$ 대신에 $m_{\mathcal{H}}$를 넣어 (변형된) 식을 증명해야겠지만, 강의에서는 이 증명이 너무 복잡하기 때문에 부록에 따로 빼놓았다고 합니다. 제가 확인해보니 6페이지 정도 분량이라 저도 안 읽었습니다. 굳이 그것까지 읽지 않아도 될 거 같았거든요.&lt;/p&gt;

&lt;p&gt;강의에서는 간단하게 증명의 핵심 정도만 언급하고 넘어갑니다. 증명의 핵심은 크게 3가지인데, (1) $m_{\mathcal{H}}$를 어떻게 대입할 것인가, (2) $E_{out}$은 어떻게 되는가, (3) 그리고 이를 합치는 과정입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Hoeffding’s Inequality에서 $M$이 어떻게 나왔는지와 이를 어떻게 바꾸는지에 대한 대략적인 그림입니다.&lt;/p&gt;

&lt;p&gt;가운데 그림이 바로 Hoeffding’s Inequality에서 $M$이 곱해진 이유인데, 각각의 Bad Event를 모두 배반 사건으로 가정하고 Union Bound를 씌웠기 때문입니다. 하지만 직관적으로 생각해 보았을 때, 이 Bad Event들이 모두 배반 사건이라고 놓는다면 너무 비관적인 가정이라고 생각이 듭니다. 아마 맨 오른쪽처럼 대부분의 Event가 겹치게 될 텐데, 이를 보이는 것은 다음 장인 VC Dimension에서 다루게 될 예정입니다.&lt;/p&gt;

&lt;p&gt;지금은 그냥 저런 식으로 해결되겠구나~라고만 생각하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째로는 $E_{out}$에 대해 수행할 작업입니다. 이 그림은 2장에서 다룬 적이 있습니다. 그 때와 마찬가지로 통(Bin)안의 구슬들을 잘못 분류한 것은 Out of Sample Error이고 이 중 몇 개의 구슬을 뽑은 것 중 잘못 분류한 것은 In Sample Error입니다. 증명에서는 기존에 하나만 뽑던 Sample을 독립적으로 한 개 더 뽑는 방법을 사용합니다. (물론 두 Sample은 같은 확률 분포를 사용하여 뽑습니다) 다만 독립적으로 Sample을 뽑았다고 할 지라도 두 Sample은 같은 통 안에서 같은 확률 분포를 사용하여 뽑았기 때문에 서로 관련이 생기게 됩니다. (이 말은 빨간색 구슬의 비율이 비슷하다는 뜻입니다) 강의에서는 통이 1개일 때는 Sample을 한 개 뽑든 두 개 뽑든 차이가 없지만, 여러 개의 통을 가정한 상황에서는 $E_{out}$과 $E_{in}$의 관계가 좀 더 가까워지기 때문에 이런 방법을 사용했다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/6. Theory of Generalization/ML 06-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막에서는 이를 모두 합쳐서 최종적으로 변한 식입니다. 이 식을 &lt;span style=&quot;color:red&quot;&gt;The Vapnic-Chervonenkis (VC) Inequality&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;슬라이드에 나타난 식을 보시면 $M$이 $m_{\mathcal{H}}$가 교체된 것 외에도 이것저것 바뀌었음을 알 수 있습니다. 가령 $N$이 $2N$으로, $m_{\mathcal{H}}$ 앞의 계수가 2에서 4로 바뀌었고, 지수에서 2가 8분의 1로 바뀌었죠. ($N$이 $2N$으로 바뀐 이유는 지난 슬라이드에서 Sample을 두 개 뽑았기 때문입니다.)&lt;/p&gt;

&lt;p&gt;왜 이렇게 바뀌었는지 궁금하시면 책 부록에 첨부된 증명을 보시면 될 것 같습니다…만 사실 저도 증명을 읽어보지 않아서, 그냥 이렇게 바뀌는구나 라고만 이해하시고 그것보다 중요한 VC Bound에 더 중점을 둬야 할 것 같습니다. 이어지는 VC Dimension에서 이 VC Bound에 대해 더 자세히 다뤄질 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Training versus Testing</title><link href="http://localhost:4000/studies/training-versus-testing/" rel="alternate" type="text/html" title="Training versus Testing" /><published>2019-08-15T00:00:00+09:00</published><updated>2019-08-15T00:00:00+09:00</updated><id>http://localhost:4000/studies/training-versus-testing</id><content type="html" xml:base="http://localhost:4000/studies/training-versus-testing/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5장에서는 Training과 Testing이 어떻게 다른지 다루게 됩니다. 핵심은 2장에서 다루었던 Hypothesis의 수인 $M$을 어떻게 효과적으로 줄일 수 있는가 하는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 제목과 같이 Training에서 Testing으로 넘어가면서 생기는 문제에 대해 알아보고, Hypothesis의 수를 줄이기 위해 Dichotomies, Growth function 등의 개념을 설명합니다. 그 다음에는 예제를 통해 Growth function을 계산하는 법을 알아보고, Break Point의 개념을 익히며, 마지막으로 간단한 퍼즐을 풀고 마치게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;from-training-to-testing&quot;&gt;From training to testing&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2장에서 배웠던 Hoeffding’s Inequality를 복습해봅시다. Testing의 경우에는 Final Hypothesis 1개만을 가지고 만든 식이었기 때문에 원래의 Hoeffding’s Inequality와 같았습니다. 그러나 Training의 경우에는 모든 Hypothesis들에 대해 각각 Hoeffding’s Inequality가 적용되었으므로 이들의 개수인 $M$을 오른쪽에 곱해줬었습니다. 여기서 발생하는 문제는, 실질적으로 Hypothesis의 수가 거의 무한하므로 오른쪽 항이 1을 넘기가 너무 쉬워 부등식 자체가 큰 의미를 갖지 못한다고 배웠습니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 이 $M$ 대신에 보다 의미 있는 다른 값을 넣기 위해 알아보는 시간입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 이 $M$이란 값이 어디서 왔는지부터 다시 한번 점검해보겠습니다. Hoeffding’s Inequality에서 확률 $P$ 안에 들어있는 것은 Bad Event였습니다. 이 Bad Event라는 것은 Hypothesis에서 &lt;strong&gt;In Sample Error와 Out of Sample Error의 차이가 $\epsilon$ 값보다 크다&lt;/strong&gt;로 정의되어 있었습니다. 그래서 모든 가능한 $M$개의 Hypothesis에서 이러한 보장이 필요했기에, 최악의 경우(즉, 모든 Bad Event가 서로 배반 사건일 경우)를 감안하여 각각의 Bad Event가 일어날 확률을 그냥 더해줬었습니다.&lt;/p&gt;

&lt;p&gt;그런데 Hypothesis에서 Bad Event가 일어나는 사건들이 배반 사건이 아닐 가능성이 높다는 겁니다. 실제로는 위 슬라이드의 오른쪽 그림처럼 어느 정도 겹쳐 나올 확률이 훨씬 큽니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이해를 돕기 위해 우리가 잘 아는 Perceptron에서 예제를 하나 만들어보겠습니다. 먼저, 위 슬라이드의 오른쪽 그림과 같이 한쪽은 -1, 다른 한쪽은 +1로 깔끔하게 분리되는 Classification 문제가 있다고 가정해봅시다. 이 중 하나의 Hypothesis로, 파란색 선이 존재합니다. 보이시는 대로 정확하게 나누지 않았기 때문에 Error가 존재합니다. 그렇다면 위 예제에서 In Sample Error와 Out of Sample Error를 각각 찾아봅시다. 이 파란색 선의 In Sample Error와 Out of Sample Error는 어떻게 계산할 수 있을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-06-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 Out of Sample Error는 위의 그림과 같이 잘못 나눈 부분의 영역입니다. In Sample Error는 아래처럼 주어진 Data가 있을 때 잘못 판단이 된 Data Point의 수로 정의됩니다. (4장을 참고하세요.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-06-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자, 이렇게 In sample Error와 Out of sample Error를 각각 계산했으니, 이제 다른 Hypothesis를 한번 따져봅시다. 위 슬라이드(5번 슬라이드)에서 파란색 선을 살짝 비튼 초록색 선을 또다른 Hypothesis라 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 두 Hypothesis의 차이는 그림에서의 노란색 면적임을 쉽게 알 수 있습니다. 그런데 전체적으로 보았을 때 이 면적이 매우 미미하므로, 이 두 Hypothesis는 상당히 유사함을 알 수 있습니다. 이와 비슷한 Case가 매우 많다는 것을 직관적으로 알 수 있으므로, 모든 Hypothesis에 대해 Hoeffding’s Inequality를 Union Bound로 잡는 것은 상당히 불합리하다고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 전 슬라이드에서 In Sample Error를 계산했던 방법을 자세히 살펴보겠습니다. 일반적으로, Out of Sample Error를 정확하게 계산할 수 있는 방법은 없습니다. 2장에서도 나왔지만, 기계학습이라는 것은 항상 전체의 데이터를 갖지 못한 상황만을 가정하기 때문입니다. (전체의 데이터가 있다는 것은 굳이 기계학습을 사용할 의미가 없다는 뜻이기 때문입니다.)&lt;/p&gt;

&lt;p&gt;그렇기 때문에 In Sample Error의 상황만을 따져보겠습니다. 전체의 영역은 연속적인 공간입니다. 하지만 이 전체의 공간을 하나하나 따질 수 없으므로, 몇 개의 점만을 가지고 계산하고자 합니다. 즉, 전체의 영역이 어떻게 구분되었나를 따지기보다는 우리가 선택한 해당 점들이 어떻게 분류가 되었나를 확인합니다. 이러한 점들을 &lt;span style=&quot;color:red&quot;&gt;Dichotomies&lt;/span&gt; 라고 합니다. 위 슬라이드에서 오른쪽 그림이 의미하는 것은 첫 번째 그림과 같은 상황일 때, 두 번째 그림과 같이 몇 개의 구멍이 뚫린 불투명한 덮개가 있다고 가정합니다. 그리고 세 번째 그림은 이 덮개를 첫 번째 그림에 덮은 상황입니다. 이 세 번째 그림을 보시면 보라색 선이 보이지 않습니다. 따라서 영역을 어떻게 나누었나(=보라색 선이 어디에 있는가)에 관심을 갖기보다는 우리가 선택한 Data Point들이 어떻게 분류가 되었나를 본다는 것이 핵심입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Classification 문제에서 Dichotomy의 수가 얼마나 되는지 알아봅시다. Hypothesis는 모든 데이터 공간 $\mathcal{X}$를 +1 또는 -1로 분류합니다. 그런데 Dichotomy는 $N$개의 Data Point를 각각 +1 또는 -1로 분류합니다. 따라서 Hypothesis의 숫자는 무한할 수 있지만, Dichotomy의 숫자는 아무리 많아봤자 $2^N$개밖에 나올 수가 없습니다. 그렇다면 Dichotomies는 최소한 유한하다는 보장이 있으니 Hoeffding’s Inequality에서 $M$ 대신에 사용할 수 있음을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 새로운 함수를 하나 배워보겠습니다. 방금과 같이 $N$개의 Data Point가 있을 때, 나올 수 있는 최대의 Dichotomies의 수를 &lt;span style=&quot;color:red&quot;&gt;Growth Function&lt;/span&gt; 이라고 합니다. 이전 슬라이드에서 언급한 것처럼, Dichotomies의 수는 아무리 많아봐야 $2^N$개 이므로, Growth Function의 최댓값도 $2^N$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간단한 예제 몇 가지를 통해 Growth Function을 구하는 방법을 알아보겠습니다. 2D Perceptron 문제에서 Growth Function을 계산해보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 $N=3$일 때를 확인해 봅시다. 첫 번째 그림처럼 Data Point가 놓여있을 경우, 각각의 Data를 어떤 방식으로 +1이나 -1로 정의해도 모두 선 하나로 구분할 수 있습니다. 따라서 $m_{\mathcal{H}}(3)=8$임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;그런데 두 번째 그림과 같은 상황을 생각해서 의문을 가질 수도 있습니다. 두 번째 그림처럼 점 3개를 일렬로 놓으면 절때로 선 하나로 Data Point를 구분할 수가 없기 때문입니다. 하지만 Growth Function은 “최대”의 Dichotomies의 수만을 따지기 때문에, 설사 단 한 가지의 경우만 8개가 나올 수 있다고 해도 Growth Fucntion의 값은 8이 되는 겁니다.&lt;/p&gt;

&lt;p&gt;이번엔 $N=4$인 경우를 보시면, 이 경우에는 어떻게 점을 놓더라도 세 번째 그림처럼 선 하나로는 절대 점들을 구분할 수 없는 경우가 나옵니다. 따라서 이 경우에는 최대치인 16개가 될 수 없고, 저렇게 십자 모양으로 데이터가 분산된 경우(+1과 -1이 뒤바뀌는 경우도 있으니 실제로는 2가지 경우입니다)를 제외한 14개의 경우만 구분이 가능합니다. 따라서 $m_{\mathcal{H}}(4)=14$가 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;illustrative-examples&quot;&gt;Illustrative examples&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Growth Function을 계산하는 방법은 알았지만, 이제 또 다른 문제가 생겼습니다. Growth Function이 “최대”의 Dichotomies의 수를 구해야 하는거면 일일이 해봐야만 알 수 있는 건데, 그러면 $N$이 커질 때는 어떻게 이걸 일일이 구해야 할지 막막합니다. 안타깝게도 일반적인 케이스는 진짜 일일이 해보지 않고는 모릅니다만, 많이 보이는 몇 가지 예제는 간단한 공식으로 계산할 수 있습니다. 다음 슬라이드에서 몇 가지 예제를 통해 Growth Function을 구해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 예제는 Positive Ray입니다. 이것은 데이터가 모두 일직선 위에 놓여있고, 점 $a$를 기준으로 왼쪽은 모두 -1로, 오른쪽은 모두 +1로 분류되는 경우입니다.&lt;/p&gt;

&lt;p&gt;이 상황에서는 나올 수 있는 경우의 수를 따져봅시다. 간단히 이를 계산하려면 점 $a$가 놓일 수 있는 위치가 몇 개나 있을지 세어보는 겁니다. 하나하나 따져보면 $x_1$ 왼쪽에 있는 경우 (1개) + 연속된 두 점 $x_i$, $x_j$ 사이에 있는 경우 ($N-1$개) + $x_N$ 오른쪽에 있는 경우 (1개) 이므로 다 합치면 $N+1$개가 됨을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;즉, Positive Ray에서 Growth Function의 값은 $N+1$이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 예제는 Positive Interval입니다. 이 예제는 방금 전 예제처럼 데이터들이 일직선 위에 놓여있는 상황인데, +1이 되는 조건이 임의의 구간으로 설정되어 있습니다.&lt;/p&gt;

&lt;p&gt;이 상황에서 나올 수 있는 경우의 수를 계산하려면, 이전 예제에서 점 $a$를 두 개 잡아 그 사이를 +1로 설정하면 됩니다. 따라서 총 $N+1$개의 구간에서 두 점을 잡고, 그 순서는 중요하지 않으니 조합(Combination)으로 계산하면 됩니다. 즉, $N+1 \choose 2$가 됩니다.&lt;/p&gt;

&lt;p&gt;마지막으로 놓치지 말아야 하는 경우에 수가 있습니다. 똑같은 구간에서 두 점을 잡게 되면 실질적으로 +1이 되는 점이 한 개도 없으므로, 이것도 경우에 수에 추가해야 합니다. 따라서 Positive Interval에서 Growth Function의 값은 $N+1 \choose 2$ + 1이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 예제는 Convex Set입니다. Convex Set은 임의의 집합에서 두 점을 잡았을 때, 그 두 점을 잇는 선분도 그 집합 내의 영역안에 있는 집합을 말합니다.&lt;/p&gt;

&lt;p&gt;이 예제에서는 오른쪽 그림과 같이 원 위에 임의의 Data Point를 잡은 상황입니다. 이러한 Convex Set에서는 각각의 점이 +1이든 -1이든 상관없이 오른쪽 그림처럼 임의의 다각형을 만들 수 있습니다. 따라서 이때는 모든 경우를 표현 가능하므로, Convex Set에서의 Growth Function의 값은 최댓값인 $2^N$개가 되는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금까지 살펴본 3가지 예제의 Growth Function을 정리하면 위 슬라이드와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이제 이번 강의 초반에서 언급했던 Hoeffding’s Inequality를 다시 따져봅시다.&lt;/p&gt;

&lt;p&gt;우리가 최종적으로 원하던 것은 이 $M$을 다른 것으로 대체하는 것이었는데, 지금 괜찮은 대안으로 Growth Function $m_{\mathcal{H}}(N)$ 이 나왔습니다. 무한대일 수도 있는 $M$보다는 낫지만, Growth Function은 Convex Set 같은 경우처럼 데이터에 지수함수꼴로 비례하는 최악의 경우가 있기 때문에 역시 부등식의 오른쪽 항이 1보다 커질 위험이 있습니다. 만약에 이 Growth Function이 데이터 $N$에 대해 Polynomial 하다는 것만 밝혀낸다면, Hoeffding’s Inequality를 훨씬 괜찮게 바꿔줄 수 있을 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;key-notion--break-point&quot;&gt;Key notion : Break point&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 Break Point라는 것을 배워보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약에 $\mathcal{H}$에서 $k$개의 데이터를 골고루 흩뿌릴 수 없을 때 이 $k$를 $\mathcal{H}$에서의 &lt;span style=&quot;color:red&quot;&gt;Break Point&lt;/span&gt;라고 합니다. 정의가 직관적으로 이해되지 않기 때문에 조금 더 쉽게 설명해 드리겠습니다. “데이터를 골고루 흩뿌릴 수 없다” 라는 말은 데이터를 어떻게 배치해도 최대의 Dichotomies를 만들 수 없는 상황을 말합니다. 즉, $m_{\mathcal{H}}(N)&amp;lt;2^k$ 를 만족하는 k를 말합니다. 예를 들어 아까 보았던 2D Perceptron의 경우, $N=3$ 일 때 $m_{\mathcal{H}}(3)=8$이었지만 $N=4$일 때 $m_{\mathcal{H}}(4)=14&amp;lt;16$ 이었으므로 $k=4$가 됩니다.&lt;/p&gt;

&lt;p&gt;이 Break Point의 개념이 상당히 중요한데, Break Point $k$ 이후로는 절때 최대의 Dichotomies를 만들 수 없기 때문입니다. (즉, 2D Perceptron을 예로 든다면 4 이상인 모든 $N$에 대하여 $m_{\mathcal{H}}(N)&amp;lt;2^N$이 성립한다는 뜻입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 아까 예제로 들었던 3가지 예제에서 Break Point를 구해보겠습니다.&lt;/p&gt;

&lt;p&gt;Positive Ray의 경우에는 $m_{\mathcal{H}}(N)=N+1&amp;lt;2^N$을 만족하는 최소의 $N$이 2이므로, Break Point $k=2$임을 알 수 있습니다. Positive Interval의 경우도 마찬가지로 $m_{\mathcal{H}}(2)=4, m_{\mathcal{H}}(3)=7&amp;lt;9$ 이므로 $k=3$이 Break Point임을 쉽게 계산할 수 있습니다. 그런데 Convex Set의 경우에는, $m_{\mathcal{H}}(N)=2^N$ 이었으므로, 어떤 $k$에 대해서도 Break Point를 찾을 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 재밌는 결과를 도출 할 수 있습니다. 만약에 Break Point가 없다면, Growth Function은 $m_{\mathcal{H}}(N)=2^N$이 되지만, Break Point가 존재하기만 한다면 $m_{\mathcal{H}}(N)$는 $N$에 대해서 Polynomial 하다는 것을 알 수 있습니다. 15번 슬라이드에서의 문제가 해결되는 느낌입니다.&lt;/p&gt;

&lt;h2 id=&quot;puzzle&quot;&gt;Puzzle&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 간단한 퍼즐을 하나 풀어봅시다.&lt;/p&gt;

&lt;p&gt;사실 이 슬라이드에는 문제를 내기도 전에 정답이 이미 화면에 나와있는데, 원래 문제는 “Break Point $k=2$일 때, $N=3$인 경우 가능한 모든 경우의 수를 구하라” 입니다. 이를 해결하기 위해 차근차근 한번 생각해봅시다. 경우의 수를 하나하나 따져보면 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Break Point가 $k=2$ 이므로, $m_{\mathcal{H}}(1)=2$ 임을 알 수 있습니다. 가장 먼저 모든 점이 -1로 분류되는 상황으로 시작해 봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;자, 이 상황은 일단 아무런 문제가 없습니다. 이제 $\mathbf{x}_3$가 +1이 되는 경우를 추가합니다.&lt;/p&gt;

&lt;p&gt;이것은 아무 문제가 없습니다. 왜냐하면 $x_3$이라는 점 하나만 놓고 보았을 때 $N=1$인 경우 $m_{\mathcal{H}}(1)=2$ 라는 조건에 위배되지 않으니까요. (아래 그림 참고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이와 같은 방법으로 $\mathbf{x}_1, \mathbf{x}_2$ 각각에 하나의 점만 +1이 되는 경우를 추가해줍니다. (아래 그림 참고)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이제 새로운 경우를 하나 더 추가해보겠습니다. 아래 그림과 같이 $\mathbf{x}_1, \mathbf{x}_3$이 동시에 +1이 되는 상황을 가정해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 여기에 문제가 있습니다. 왜 문제가 되는지 여기서 $\mathbf{x}_2$를 지워봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/5. Training versus Testing/ML 05-21-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;보시면 첫 번째 경우와 세 번째 경우가 같은 경우이므로 이를 제외하면, 총 4가지의 경우가 나오게 됩니다. 이는 문제에서 제시했던 Break Point가 2라는 조건에 위배됩니다. $m_{\mathcal{H}}(2)&amp;lt;4$이어야 하는데 4개가 나와버렸습니다. 따라서 이 경우는 존재할 수 없다는 것을 알 수 있습니다. 이와 마찬가지로 첫 4가지 경우를 제외하면 어떤 경우도 Break Point가 2라는 조건을 지킬 수 없기 때문에, 정답은 이 4가지 경우만 존재함을 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;제가 쉽게 설명을 못한 것 같은데, 유튜브 강의에서 해당 부분을 확인하시면 훨씬 쉽게 이해하실 수 있을 겁니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Error and Noise</title><link href="http://localhost:4000/studies/error-and-noise/" rel="alternate" type="text/html" title="Error and Noise" /><published>2019-08-09T00:00:00+09:00</published><updated>2019-08-09T00:00:00+09:00</updated><id>http://localhost:4000/studies/error-and-noise</id><content type="html" xml:base="http://localhost:4000/studies/error-and-noise/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;4장에서는 Error와 Noise에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 구성은 총 4개로 나뉩니다.&lt;/p&gt;

&lt;p&gt;먼저 지난 장 마지막에 다루었던 Nonlinear Transformation에 대해 좀 더 이야기해보고, 다음으로 Error를 측정하는 방법, 그리고 Noise가 발생하는 이유와 모델에 적용하는 법, 마지막으로 앞으로 이런 문제를 어떻게 접근할지에 대한 간단한 정리를 하며 마치게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-transformation-continued&quot;&gt;Nonlinear transformation (continued)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지난 장에서 배웠던 Nonlinear transformation의 과정을 한번 정리해봅시다. 이 문제가 시작된 이유는 Orginal data가 선형으로 분리가 불가능하여 우리가 배웠던 Linear Model을 직접적으로 적용할 수 없는 문제들이 존재했기 때문입니다. 그래서 이 문제를 어떻게 해결할지 고민하다가, Linear Model에서 “Linear”가 어디에 Linear 한 것이지를 생각해보았었죠. 여기서 Linear 하다는 것은 Input data $\mathbf{x}$에 Linear 한 것이 아니라 Weight Vector $\mathbf{w}$에 Linear 한 것이라고 배웠습니다. 이를 통해 Input data $\mathbf{x}$는 우리의 입맛에 맞게 Transform을 시켜도 문제가 없겠구나라는 결론에 도달한 것입니다. 따라서 기존의 Input data들이 $\mathcal{X}$라는 공간에 있다고 가정했을 때, 이들을 적절한 함수인 $\Phi$로 Transform 시켜서 선형으로 분리가 가능하게끔 만들어 줬습니다. 이때, Input data들을 $\Phi$로 Transform 시킨 공간을 $\mathcal{Z}$라 부르겠습니다.&lt;/p&gt;

&lt;p&gt;이렇게 선형 독립이 가능한 공간인 $\mathcal{Z}$에서 기존에 우리가 알고있던 Linear Model을 적용해 문제를 해결했습니다. 예를 들면 PLA 같은 방법으로 말입니다. 그렇게 알고리즘을 수행 후 나온 결과는 $\mathcal{Z}$ 공간에서 정의된 Final Hypothesis $\tilde{g}$가 나오게 됩니다. (기존의 Input data가 존재하는 $\mathcal{X}$ 공간에서의 Final Hypothesis와의 차이를 두기 위해 $g$가 아니라 $\tilde{g}$라고 적은 겁니다. $\tilde{g}$는 ‘틸다 g’라고 읽습니다) 이때의 결과를 수식으로 표현하자면 $\tilde{g}(\mathbf{z})=sign(\tilde{\mathbf{w}}^{\sf T}\mathbf{z})$ 라고 쓸 수 있습니다. 하지만 우리가 원하는 결과는 $\tilde{g}$가 아니라 $g$ 입니다.&lt;/p&gt;

&lt;p&gt;따라서 $\tilde{g}$를 초기에 했던 Transform $\Phi$의 역함수인 $\Phi^{-1}$로 Transform을 하게 되면 우리가 원하던 $g$가 나오게 됩니다. 단, $\mathcal{Z}$ 공간에서는 선형으로 $\tilde{g}$를 구하긴 했지만 원래의 $\mathcal{X}$ 공간에서 $g$는 선형으로 나오지 않습니다. (이해를 돕기 위해 $\Phi^{-1}$를 사용해서 $g$를 구하는 것처럼 설명했지만, 실제로는 이렇게 하지 않고 그냥 Input data $\mathbf{x}$를 $\Phi$로 Transform 해서 계산합니다. 왜 그런지는 다음 슬라이드에서 설명드리겠습니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 Transform $\Phi$를 통해서 과연 어떤 값들이 바뀌게 되는지 하나하나 따져보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 Input data $\mathbf{x}$는 당연히 바뀌게 됩니다. $\mathbf{x}$가 바뀐다는 말은 $\mathbf{x}$ 벡터의 모든 값들이 $\Phi$를 거쳐 바뀌게 된다는 겁니다. (ex. $x_0 \to z_0$) 하지만 Output인 $y$는 바뀌지 않습니다. +1로 판단한 것은 그대로 +1, -1로 판단한 것은 그대로 -1로 남아야만 원래의 문제를 그대로 풀 수 있기 때문입니다. 이는 Regression에서도 마찬가지입니다.&lt;/p&gt;

&lt;p&gt;Weight를 구하는 것이 목적이니 이것이 가장 중요한데, $\mathcal{X}$ 공간에서 직접적으로 Weight를 구하지 않기 때문에 $\mathcal{X}$ 공간에서의 Weight Vector는 존재하지 않습니다. Weight를 계산한 것은 $\mathcal{Z}$ 공간이니 $\tilde{\mathbf{w}}$ 만 존재합니다. 하지만 우리가 갖고있는 것은 $\mathbf{x}$이고 $g$를 이용해서 classification/regression을 사용해야 하니 $g(\mathbf{x})$를 아래와 같이 정의합니다.&lt;/p&gt;

\[g(\mathbf{x}) = sign(\tilde{\mathbf{w}}^{\sf T}\mathbf{z}) = sign(\tilde{\mathbf{w}}^{\sf T}\Phi(\mathbf{x}))\]

&lt;p&gt;여기서 “아니 그냥 $\tilde{\mathbf{w}}$에 $\Phi^{-1}$를 취해서 $\mathbf{w}$를 구하면 되는거 아닌가? 왜 이렇게 불편하게 $g$를 계산하지?” 라는 의문을 가질 수도 있습니다. 물론 그 방법이 더 간단합니다만, $\Phi^{-1}$가 존재하지 않을 수 있다는 것이 문제입니다. 예를 들어 만약에 Transform $\Phi$를 통해서 차원이 늘어난다면, 하나의 $\mathcal{Z}$ 공간의 좌표인 $\mathbf{z}$에서 여러 개의 $\mathbf{x}$와 대응할 수 있는 문제가 발생합니다. 따라서 조금 불편하더라도 $g$를 계산할 때 Input data를 $\Phi$로 Transform 할 수밖에 없는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;error-measures&quot;&gt;Error measures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로 Error Measure에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Error Measure를 하기 전에 기존에 배웠던 Learning Diagram을 다시 살펴봅시다. 간단하게만 다시 설명드리면, Input Data $\mathbf{x}$는 어떤 확률 분포에 의해 생성되고, Target Function $f$에 의해 $y$값이 결정되어 Training Example $(\mathbf{x}_i, y_i)$가 생성된다고 가정합니다. 그 후 여러 개의 Hypothesis Set (ex. Perceptron)을 통해 Learning Algorithm (ex. PLA)을 거쳐 $f$와 가장 가까운 Final Hypothesis $g$를 얻는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 $h$와 $f$가 가깝다($h \approx f$)는 어떻게 정의해야 할까요?&lt;/p&gt;

&lt;p&gt;이를 정량적으로 측정하기 위해 Error Measure $E(h, f)$를 정의합니다. 다만 직접적으로 두 함수 $h$와 $f$를 구할 수가 없기 때문에, Input data $\mathbf{x}$를 넣었을 때의 값의 차이로 정의하게 됩니다. 이 때, $\mathbf{x}$는 함수 $h$, $f$의 한 “점”으로 볼 수 있기 때문에 이를 pointwise로 정의한다고 부릅니다.&lt;/p&gt;

&lt;p&gt;그런데, 우리는 이미 Error Measure 방법 중 두 가지를 배웠습니다. Linear Regression에서는 $h$와 $f$의 함수값의 차이의 제곱으로 정의한 Squared Error로 정의했고, Linear Classification에서는 맞았는가/틀렸는가를 비교했기 때문에 Binary Error를 사용하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;점 $\mathbf{x}$에서 각각 에러를 계산하는 것은 이렇게 간단합니다. 하지만 우리가 원하는 것은 각각 점에서의 에러가 아니라, $h$의 전체적인 에러입니다. 즉, 함수 $h$가 함수 $f$와 얼마나 다른가를 알고싶다는 것입니다.&lt;/p&gt;

&lt;p&gt;가장 간단한 방법으로 Error의 평균값을 사용합니다. In sample Error와 Out of sample Error 모두 평균값을 사용합니다만, 여기서도 In sample Error야 우리가 갖고있는 데이터를 사용해서 구하면 되지만, Out of sample Error는 어떻게 구해야 하는지에 대한 의문이 생깁니다. 이 방법은 다음 장에서 다루도록 하고, 우선은 전체적인 Error는 Sample의 평균 Error를 사용한다는 것만 짚고 넘어갑시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Learning Diagram에서 Error를 포함한 그림입니다. Error를 측정할 때 각각 $\mathbf{x}$ 점에서의 $h$와 $f$의 함수값의 차이의 평균으로 계산한다고 말씀드렸습니다. 여기서 $\mathbf{x}$는 특정한 확률 분포로부터 생성된 점이기 때문에 위의 그림과 같이 표현됨을 알 수 있습니다. Final Hypothesis $g$는 이 Error의 평균값이 가장 낮은 Hypothesis로 결정됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이 Error Measure를 어떻게 결정해야 하는지도 중요합니다. 기존의 Error Measure 방법은 Squared Error와 Binary Error를 다루었습니다만, 이 둘은 일부일 뿐 모든 상황에 적용할 수 있는 방법이 아닙니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 기계학습으로 지문을 인식하는 프로그램을 구현했다고 가정해봅시다. 기계학습은 완벽하지 못하기 때문에, 아무리 완벽에 가깝게 구현했다고 할지라도 Error가 발생할 수 있습니다. 이 경우 발생할 수 있는 에러는 2가지입니다. &lt;span style=&quot;color:red&quot;&gt;False Accept Error&lt;/span&gt;는 등록되지 않은 지문을 정상으로 판단하는 오류이고, &lt;span style=&quot;color:red&quot;&gt;False Reject Error&lt;/span&gt;는 정상으로 등록된 지문을 침입자로 판단하는 오류입니다.&lt;/p&gt;

&lt;p&gt;그렇다면 각각의 상황에서 어느정도의 페널티를 주는 것이 적당할까요? 이 문제는 Classification이기 때문에 그냥 Binary Error를 사용하면 된다고 생각하실 수도 있습니다만, 다음 두 가지 예시를 통해 그렇게 간단한 문제가 아님을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 지문 인식 프로그램이 마트에 설치되어 있다고 가정해봅시다. 마트에서는 지문이 등록된 회원들에게 할인해주는 이벤트를 하고 있습니다. 이 상황에서 발생할 수 있는 두 가지 Error에 대해 어떻게 페널티를 주어야 하는지 생각해봅시다.&lt;/p&gt;

&lt;p&gt;만약 마트의 회원이 할인 이벤트로 인해 마트에 갔는데 False Reject가 발생하여 컴퓨터가 지문을 제대로 인식 못해 몇번이나 손을 갖다대야 하는 상황이 온다면 그 고객은 몹시 기분이 나쁠 것입니다. 최악의 경우에는 단골 고객을 잃을 수도 있습니다. 다만 반대로, 우연히 지나가다 마트에 들른 비회원이 심심해서 지문을 갖다댔는데 컴퓨터가 회원으로 인식해서 할인을 해주는 상황(False Accept)은 마트 입장에서 크게 문제가 아닙니다. 마트는 그냥 약간의 금전적인 손해만 보고, 새로운 고객을 유치할 기회를 얻을 수도 있습니다. 이런 상황에서는 False Reject에 큰 패널티를 부여하고, False Accept는 그보다 낮은 페널티를 부여하는 것이 적절할 것입니다. 하지만 다른 상황에서도 마찬가지일까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 이 지문 인식 프로그램이 국가정보원에 설치되어 있다고 가정해봅시다. 국가정보원의 1급 비밀이 담긴 금고는 요원들의 지문을 통해서만 열립니다. 만약 여기서 False Accept가 발생한다면 엄청난 문제가 발생할 수 있음을 알 수 있습니다. 최악의 경우에는 간첩의 지문만으로 금고가 열려서 국가 기밀이 노출되는 상황이 발생하겠죠.&lt;/p&gt;

&lt;p&gt;하지만 False Reject는 그다지 큰 문제가 아닙니다. 요원의 경우 이게 직업이고, 컴퓨터가 몇번 인식 못한다고 해도 툴툴대며 다시 손가락을 갖다댈 것이니까요. 따라서 이 경우에는 False Accept에 큰 패널티를 부여하고, False Reject는 그보다 낮은 페널티를 부여하는 것이 적절할 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 두 가지 상황을 통해, 같은 문제라도 상황에 따라 다른 Error Measure를 적용해야 함을 알 수 있습니다. 그런데 만약에 이렇게 구체적인 Error Measure를 사용할 수 없을 때는 어떻게 해야 할까요?&lt;/p&gt;

&lt;p&gt;크게 두 가지가 있습니다. &lt;span style=&quot;color:red&quot;&gt;Plausible Measure&lt;/span&gt;는 “Error가 특정한 분포를 따를 것이다” 라고 가정하는 것입니다. 가령 어떤 문제에서 Error가 가우시안 분포를 따른다라고 가정하는 것이죠. &lt;span style=&quot;color:red&quot;&gt;Friendly Measure&lt;/span&gt;는 수학적인 방법으로 Error를 계산하는 방법입니다. Closed-form을 통해 답을 구하거나, Convex Optimization을 통해 답을 구하는 방법입니다.&lt;/p&gt;

&lt;p&gt;온라인 강의에서도 이 방법들에 대해서는 그냥 이렇게만 설명만 하고 지나갔었기 때문에, 그냥 이러이러한 것이 있구나라고만 알고 넘어가시면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;방금 배운 Error Measure를 Learning Diagram에 적용한 그림입니다. Learning Algorithm에서 Error Measure를 통해 Fianl Hypothesis를 도출한다는 것을 표현한 것이 추가된 그림입니다.&lt;/p&gt;

&lt;h2 id=&quot;noisy-targets&quot;&gt;Noisy targets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 Noise Target이라는 것이 무엇인지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;지금까지 우리가 구해야하는 목표를 “Target Function”이라고 표현했지만, 사실 이 Target Function은 Function이 아닐 수도 있습니다. (Function이 아니라는 것은 하나의 데이터에 대해서 여러 가지 값이 나올 수도 있다는 말입니다.)&lt;/p&gt;

&lt;p&gt;지난번에 사용했던 카드 발급 문제를 예를 들면, 이 카드 회사에는 지금까지 고객의 정보를 기반으로 카드를 방급해 주었는지/거절했는지를 판단한 데이터가 있습니다. 이 데이터중에서는 우연히 고객의 정보가 동일한 케이스도 있을 것입니다. 그런데 카드 발급 여부를 결정한 사람이 다르다던가, 같은 사람이라도 그날의 기분이 좋고/나쁘고의 이유로 인해 한명에게는 카드를 발급해주고, 다른 한명에게는 거절한 경우가 있을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 이제는 Target “Function” 이라는 표현 대신에 Target “Distribution” 이라고 표현할 것입니다. 즉, $y=f(\mathbf{x})$ 라는 표현 대신에 $P(y \mid \mathbf{x})$라는 표현을 쓰겠다는 얘기입니다. $P(y \mid \mathbf{x})$는 Input data가 $\mathbf{x}$일 때 $y$라는 결과가 나올 확률을 의미합니다. 그런데 Input data $\mathbf{x}$도 어떤 확률 분포에 의해 생성된다고 했으니 (Learning Diagram 참고) Input data $\mathbf{x}$가 생성될 확률을 $P(\mathbf{x})$라 할 수 있습니다. 따라서 종합적으로 $(\mathbf{x}, y)$라는 데이터가 생성될 확률은 $P(\mathbf{x})P(y \mid \mathbf{x})$ 라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이를 토대로 Noisy target를 정의해 보면 deterministic target $f(\mathbf{x})$를 평균적인 값 $\mathbb{E}(y \mid \mathbf{x})$로 정의했을 때 output $y$와 $f(\mathbf{x})$의 차이라고 볼 수 있습니다. Deterministic target이란 Noise가 하나도 없는 target이라는 뜻입니다. (즉, 우리가 원래 알고있던 Target Function과 동일합니다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 Noise를 반영한 Learning Diagram입니다. 자꾸 아까부터 Learning Diagram에 하나씩 추가되어 짜증나실 수도 있는데 다행이 이 그림이 최종판입니다. 왼쪽 위의 Target Function이 Target Distribution으로 바뀌었습니다. Target Distirubution이라는 것은 Target Function에 Noise를 추가한 것으로 보시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번엔 $P(y \mid \mathbf{x})$와 $P(\mathbf{x})$의 차이를 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;$P(y \mid \mathbf{x})$는 “데이터 $\mathbf{x}$가 들어왔을 때 Output $y$가 나올 확률”입니다. 실질적으로 우리가 학습하고 싶은 값이 됩니다. $P(\mathbf{x})$는 데이터 $\mathbf{x}$가 얼마나 자주 등장하는지를 나타냅니다. 특정한 데이터가 너무 자주 나오게 되면 학습 모델이 잘못될 수 있기 때문에 겉으로는 드러나지 않아도 이 값도 무시할 수는 없습니다. 이전 슬라이드에서 $(\mathbf{x}, y)$는 $P(\mathbf{x})P(y \mid \mathbf{x})$로 계산되었으니 이 두 가지 컨셉이 합쳐있음을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;preamble-to-the-theory&quot;&gt;Preamble to the theory&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 이를 통해 앞으로 어떻게 접근할 것인지 간단하게 정리하고 마치겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2장에서 학습이 가능한가를 따졌을 때 In sample Error와 Out of sample Error를 최대한 비슷하게 만드는 것이 좋다고 하였습니다. 그런데 이걸 Learning이라고 말할 수 있을까요? 사실 Learning이라는건 Out of sample Error를 0에 가깝게 만드는건데, 이 두 표현이 같다고 볼 수는 없습니다. 극단적인 예시로 In sample Error와 Out of sample Error가 둘다 1이라고 해도 어쨌든 같아지기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 지금까지 배운 내용을 토대로 Out of sample Error를 0에 가깝에 유도할 수 있는 방법이 무엇인지 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;2장에서 In sample Error와 Out of sample Error를 최대한 비슷하게 만들어야 한다고 언급했었고, 3장에서 In sample Error를 0에 가깝게 만들어야 한다고 했습니다. 그렇다면 $0 \approx E_{in} \approx E_{out}$로 합치게 되면 Out of sample Error를 0에 가깝게 유도할 수 있음을 알 수 있습니다. 따라서 다음 장에서는 이 2가지를 각각 따로 다루게 될 예정입니다. $E_{out} \approx E_{in}$을 구하는 방법은 2개 장에 걸쳐 다룰 예정이고, $E_{in} \approx 0$은 4개 장에 걸쳐 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/4. Error and Noise/ML 04-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체적인 그림은 위 슬라이드의 오른쪽 그림과 같습니다. In Sample Error가 0으로 다가갈수록 Out of Sample Error도 작아진다면 이상적이겠지만, 안타깝게도 In Sample Error를 어느정도까지 낮출 때는 Out of Sample Error도 낮아지지만, In Sample Error를 과도하게 0에 가깝게 맞추게 된다면 오히려 Out of sample Error가 커지는 문제가 있습니다. 왜냐하면 In Sample Error는 낮추려면 Model Complexity가 점점 높아지는데 이것은 Out of Sample Error를 높이기 때문입니다. (이것은 추후 Overfitting에서 더 자세히 다루게 될 예정입니다)&lt;/p&gt;

&lt;p&gt;따라서 Trading off를 정리해보면, Model Complexity가 높아질수록 In Sample Error가 낮아지지만, Out of Sample Error와 In Sample Error의 차이는 늘어난다는 관계가 있습니다. 그러므로 이를 적당히 조절할 수 있는 점을 찾는 것이 중요한데, 이는 VC dimension이라는 것으로 계산할 수 있습니다. VC dimension은 6장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry><entry><title type="html">Linear Models I</title><link href="http://localhost:4000/studies/linear-models-1/" rel="alternate" type="text/html" title="Linear Models I" /><published>2019-08-02T00:00:00+09:00</published><updated>2019-08-02T00:00:00+09:00</updated><id>http://localhost:4000/studies/linear-models-1</id><content type="html" xml:base="http://localhost:4000/studies/linear-models-1/">&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세 번째 장에서는 새로운 이슈인 선형 모델(Linear Model)에 대해서 알아보는 시간입니다.&lt;/p&gt;

&lt;p&gt;Introduction에서도 언급했었지만, 본 교재와 강의자료의 순서가 다른 부분이 조금 있는데, 오늘 할 Linear Models 부분이 바로 그런 부분입니다. 교재에서는 Linear Models 파트가 한 장으로 묶여 있는데, 강의에서는 두 부분으로 나누어 앞부분을 3장에 넣고, 뒷부분을 9장에 배치하였습니다.&lt;/p&gt;

&lt;p&gt;왜 이렇게 만들었는가 궁금했는데, 강의영상에서 말하길 이 장을 여기에 넣는 것이 적절하지 않지만, 이론 설명 후에 구체적인 예시를 주고 싶어서 앞에 끼워넣었다고 합니다. 그런 이유로 이 장은 이전 장과 다음 장과는 직접적인 관련은 없습니다만, 추후에 이어지는 내용이 나오기 때문에 확실히 짚고 넘어가셔야 할 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 순서는 크게 입력값의 표현을 어떻게 할 것인지부터 시작해서 선형 분류(Linear Classification), 선형 회귀(Linear Regression)를 다루고 비선형 문제(Nonlinear)를 해결하기 위한 접근방법을 소개하며 끝나게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;input-representation&quot;&gt;Input Representation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간단한 예제로 사람이 손으로 쓴 숫자를 분류하는 문제를 들어봅시다. (우체국에서 손으로 쓴 주소를 기계가 분류하는 시스템을 구축한다고 생각하시면 이해가 쉬울 것 같습니다) 입력값은 위의 슬라이드에 나온 대로 사람들이 직접 손으로 쓴 숫자들을 따와서 사용하게 됩니다. 첫 번째 문제로, 입력 데이터는 그림인데, 기계학습으로 처리하기 위해서는 이를 수치화 시켜서 표현할 필요가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림 데이터를 수치화시키는 방법은 여러가지가 있지만, 일반적으로 컴퓨터는 그림을 픽셀의 묶음으로 처리하기 때문에 그림의 각 픽셀값을 입력으로 변환하는 방법을 생각해볼 수 있습니다. 이 예제의 경우, 한 숫자 그림이 차지하는 픽셀이 256개라고 가정했습니다. ($x_0$는 Threshold를 위해 만든 Input입니다 - 1장 참고)&lt;/p&gt;

&lt;p&gt;선형 분류의 대표적인 방법이 바로 1장에서 소개되었던 Perceptron Learning Algorithm (PLA)입니다. 이 예제를 PLA로 푼다고 가정했을 때, Input Vector의 크기만큼 Weight Vector가 필요하니까 Weight Vector도 256+1 차원 만큼이 필요함을 알 수 있습니다. 물론 이렇게 놓고 문제를 풀 수도 있지만, 이런 간단한 문제에 이렇게 차원이 큰 벡터를 사용하기엔 비효율적입니다. 데이터를 256차원으로 표현한다고 해봤자 의미없는 입력값이 너무 많고 (예를 들면 귀퉁이부분의 픽셀) 하려고 하는 분류 난이도에 비해 구해야할 Weight의 갯수가 너무 많아 학습이 매우 느리게됩니다.&lt;/p&gt;

&lt;p&gt;그럼 이 데이터를 어떻게 간단하게 바꿀까요? 숫자의 모양을 이용한다면 0, 1, 8 같은 숫자는 대칭적이고 그 외의 숫자는 비대칭이니 대칭 여부를 통해 숫자를 1차적으로 분류하는 것도 문제를 간단하게 만드는 방법이 될 수 있습니다. 또한 1, 7에 비해서 8은 그림에서 차지하는 검은색 픽셀이 더 많습니다. 이것도 숫자를 분류하는 데 사용이 가능해 보입니다. 이 두가지 특징을 사용하여 Input Vector를 간단하게 2+1 차원으로 줄여봅시다.&lt;/p&gt;

&lt;h2 id=&quot;linear-classification&quot;&gt;Linear Classification&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input Vector가 $x_0$, $x_1$, $x_2$ 단 3개로 줄어든 걸 볼 수 있습니다. $x_0$의 역할은 이전과 마찬가지로 Threshold 역할이고, $x_1$은 검은색 픽셀의 밀집도, $x_2$는 대칭 여부를 의미합니다. 간단하게 바꾼 Input Vector로 숫자 1과 5를 분류해 보았습니다. 아무래도 1보다 5가 차지하는 검은색 픽셀이 더 많고, 숫자 1은 대칭적인데 비해 5는 대칭적이지 않으니 이 둘을 구분하는건 크게 어렵지 않아 보입니다.&lt;/p&gt;

&lt;p&gt;다만 슬라이드의 그림을 자세히 보시면 대략적으로는 구분이 가능하지만, 몇몇 지점에서 약간의 Noise가 있음을 알 수 있습니다. 이 Noise 때문에 이 데이터는 &lt;strong&gt;선형 분리(Linearly Seperable)&lt;/strong&gt;가 불가능함을 알 수 있습니다. 1장에서 PLA는 선형 분리가 불가능하면 수렴하지 않는다고 했기 때문에 이 문제 또한 학습 결과가 수렴하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실제로 이전 슬라이드의 데이터를 사용하여 PLA를 수행한 결과입니다. 왼쪽 그림은 데이터의 갯수가 증가할수록 In Sample Error와 Out of Sample Error가 어떻게 변하는지를 나타내고 있습니다. 정상적인 학습이라면 데이터의 갯수가 증가할수록 두 Error 모두 0으로 수렴해야하지만, 이 문제에서는 데이터의 선형 분리가 불가능하니 수렴하지 않고 진동하는 모양의 그래프가 나옴을 알 수 있습니다. ($E_{in}$와 $E_{out}$의 변화를 참고하세요)&lt;/p&gt;

&lt;p&gt;여기서 눈치가 빠르신 분은 어떻게 정확한 $E_{out}$을 표현한 것인지 궁금하실텐데, 물론 실제로는 $E_{out}$를 알지 못하지만, 이 예제에서는 $E_{in}$와 $E_{out}$가 어떤 관계가 있는지 보여주기 위해서 전체 샘플을 제한하여 계산하였습니다.&lt;/p&gt;

&lt;p&gt;다행히도 이 문제에서는 $E_{in}$가 커지면 $E_{out}$도 커지고, 작아질 때도 같이 작아지는 비례 관계임을 알 수 있네요. (모든 문제가 그렇지는 않습니다. 이것은 아주 예외적인 경우입니다.) 즉, 앞으로는 $E_{in}$만 알아도 이를 낮추는 방향으로 설계를 한다면 $E_{out}$도 자연스레 줄어드는 결과가 나온다는 좋은 정보를 알게 되었습니다.&lt;/p&gt;

&lt;p&gt;그런데 왼쪽 그림을 보니 수행 횟수가 1000번일 때의 Error가 250번 정도일 때보다 크게 나타납니다. 하지만 이 알고리즘은 1000번 수행 후의 결과를 출력하다 보니 이전에 더 좋은 성능을 보이는 Weight Vector를 찾았음에도 그보다 못한 값이 최종 값으로 확정되어버렸네요. 이걸 개선한다면 이전보다 좋은(=Error가 낮은) Weight Vector를 찾을 수 있어 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서는 이 문제를 &lt;span style=&quot;color:red&quot;&gt;Pocket Algorithm&lt;/span&gt;으로 해결합니다. Pocket Algorithm은 수행 중에 최고 성능을 보이는 Weight Vector 값을 저장해 두고, Weight Vector가 변경되었을 시 성능이 기존보다 떨어진다면 이를 반영하지 않는 알고리즘입니다. (좋은 것을 찾았을 때 주머니에 넣어두는 것을 떠올리시면 이해가 편할 것 같습니다.) 즉, Pocket Algorithm을 사용한다면 Weight Vector를 학습시킬수록 Error가 높은 (낮은 성능을 보이는) Vector가 나온다고 해도 항상 최선의 결과만 출력해 줄 수 있다는 장점이 있습니다. 왼쪽 그림을 보시면 데이터의 수가 100~400개 근처일 때 가장 성능이 좋고, 이후에는 계속 이보다 나쁜 성능을 가지는 Vector만 나오다 보니 Pocket Algorithm의 경우 100 즈음부터 변화가 없는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 슬라이드는 PLA를 사용한 결과와 Pocket Algorithm을 사용한 결과를 비교한 그림입니다. 검은색 줄이 1로 판단할 것인가 5로 판단할 것인가를 나누는 기준선입니다. 선형 분리가 불가능하기 때문에 두 그림 모두 완벽하게 판별하지는 못하지만, 확실히 Pocket Algorithm으로 나눈 결과가 더 바람직해 보인다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 선형 분류를 하는 방법을 알아보았습니다. 문제 난이도를 조금 올려, 이제 선형 회귀를 하는 방법을 알아보겠습니다. &lt;span style=&quot;color:red&quot;&gt;Regression&lt;/span&gt; (회귀)은 출력 결과가 +1/-1이 나오는 분류와는 다르게, 출력 결과로 실수값으로 나오는 것을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1장에서 다루었던 카드를 발급하는 문제로 돌아와봅시다. 그때는 신청자의 정보를 바탕으로 카드를 발급해 줄 것인가/거부할 것인가의 여부를 다뤘다면, 이제는 카드 발급 여부에 더해 카드의 한도를 어떻게 정할 것인가를 구해야 합니다. 카드의 한도는 사람마다 다를 뿐더러 결과값이 실수(Real Number)로 나오기 때문에 이전보다 문제가 복잡해졌습니다. 지원자의 정보의 형태(Input Vector)는 분류를 할 때와 크게 다르지 않습니다만, 분류를 할 때와 달리 실수값의 출력이 나와야 하므로 Hypothesis $h$의 형태가 $sign()$ 함수를 떼버린 $\mathbf{w}^{\sf T} \mathbf{x}$ 꼴로 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에서 언급했다시피 입력 값은 분류때와 큰 차이점은 없습니다.
다만 분류에서는 $y_n$의 값이 +1/-1이었지만 회귀에서는 실수 값이 들어간다는 것만 주의하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;문제가 다르니 Error를 어떻게 측정할 것인가도 생각해 보아야합니다. 분류에서는 출력이 +1/-1 뿐이다 보니 그냥 정답과 다른 것의 갯수만 세면 충분했습니다. 그래서 전체 중에 몇개나 틀렸는지를 비율로 표시하였습니다.&lt;/p&gt;

&lt;p&gt;하지만 회귀에서는 출력의 값이 무한정 많다보니 단순히 틀렸냐 틀리지 않았냐만을 따지기에는 곤란합니다. 예를 들어 정답 출력이 1000 인 입력값에 대해서, 950 정도 예측한 것과 5000으로 예측한 것은 둘다 틀린 결과값이지만 똑같이 오답으로 처리하기에는 무리가 있습니다. 950정도면 틀려도 감수할만한 값일 수 있지만, 5000은 도저히 용납이 불가능하기 때문입니다. 따라서 각 입력값에 대해 얼마나 “큰 차이”로 틀렸냐를 토대로 Error를 측정하게 됩니다. 이 “차이”를 측정하는 방법은 여러 가지가 있지만, 이 강의에서는 Hypothesis Function과 Target Function의 차이를 제곱한 값을 사용하게 됩니다.이를 &lt;span style=&quot;color:red&quot;&gt;Squared Error&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;이걸 보시면 “단순한 차이를 원한다면 제곱할 필요 없이 그냥 차이의 절대값을 쓰면 되는게 아닌가?” 라는 의문이 드실 수도 있습니다. 제곱을 쓰는 데는 여러가지 이유가 있으나 대표적인 이유로는 절대값을 쓰게 되면 미분이 힘들어지기 때문입니다. 차이가 크면 클수록 패널티를 많이 주기 위함도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 Error Measure를 그림으로 표현한 슬라이드 입니다. 왼쪽 그림은 Input Vector가 1차원일 때의 예시이며, 오른쪽 그림은 Input Vector가 2차원일 때의 예시입니다. 파란색 선(오른쪽 그림에서는 파란색 평면)이 의미하는 것은 Hypothesis Function $h$이고, 빨간색 선이 실제 분류값(Target Fucntion $f$의 값)과 예측한 결과 값의 차이입니다. 헷갈리실수도 있지만, 빨간색 선을 그릴 때 $h$에 대해서 직교하는 선을 그리는 것이 아님을 유의하셔야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 이 Error Measure를 통해서 $E_{in}$을 계산해봅시다. 입력값의 개수가 $N$이므로 모든 입력값에 대해 평균적인 차이를 계산한다면 아래와 같습니다.&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^N (\mathbf{w}^{\sf T} \mathbf{x}_n - y_n)^2\]

&lt;p&gt;식을 좀 더 간단하게 표현하기 위하여, $\mathbf{x}_n$들을 묶어 $\mathbf{X}$라는 큰 벡터로 표현해봅시다. 그리고 출력값인 $y_n$도 하나로 묶어 $\mathbf{y}$로 묶어보겠습니다. 이렇게 바꾸면 $\Sigma$(Sigma) 연산이 사라지고, $\mathbf{X}$ 벡터와 $\mathbf{w}$ 벡터의 곱 연산으로 간단하게 나타낼 수 있습니다. 따라서 아래와 같이 간단한 꼴로 표현이 됩니다.&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} \lVert \mathbf{X}\mathbf{w} - \mathbf{y} \rVert^2\]

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 구하고자 하는 것은 $E_{in}$의 최소값입니다. 어떤 함수의 어디에서 최소값을 갖는지 구하는 방법은 그 함수의 도함수(Derivative)를 구해서 그 값이 0인 점을 찾으면 됩니다. (물론 모든 함수의 최소값이 도함수가 0인 점이라는 것은 아닙니다만, $E_{in}$ 함수는 볼록 함수(Convex Function)이므로 최소값입니다) $E_{in}$을 미분하려면 Vector Calculus를 알아야 하는데, 모르시는 분을 위해서 중간 과정을 조금 적어드리겠습니다. (저도 Vector Calculus를 제대로 배워본적이 없어서 틀린 부분이 있다면 댓글로 지적 부탁드립니다)&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} \lVert \mathbf{X}\mathbf{w} - \mathbf{y} \rVert^2\]

&lt;p&gt;위의 원래의 식에서 제곱을 없애기 위해 $\lVert \cdot \rVert$ 부분을 풀어 써 보겠습니다.&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} (\mathbf{X}\mathbf{w} - \mathbf{y})^{\sf T} (\mathbf{X}\mathbf{w} - \mathbf{y})\]

&lt;p&gt;위 식에서 전치행렬(Transpose Matrix) 기호 $\sf T$가 어디서 튀어나왔나 싶으실텐데 이것은 $\sf T$를 붙이지 않으면 행렬 곱을 할 수 없기 때문에 벡터 계산 중 붙여진 것이라고 생각하시면 됩니다.&lt;/p&gt;

&lt;p&gt;()위에 붙어있는 $\sf T$가 보기 싫으니, $\sf T$를 () 안으로 넣어줍시다.&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} (\mathbf{w}^{\sf T} \mathbf{X}^{\sf T} - \mathbf{y}^{\sf T}) (\mathbf{X}\mathbf{w} - \mathbf{y})\]

&lt;p&gt;이 식을 전개한다면,&lt;/p&gt;

\[E_{in}(\mathbf{w}) = \frac{1}{N} (\mathbf{w}^{\sf T} \mathbf{X}^{\sf T} \mathbf{X} \mathbf{w} - 2 \mathbf{y}^{\sf T} \mathbf{X} \mathbf{w})\]

&lt;p&gt;이렇게 쓸 수 있습니다. $E_{in}$은 $\mathbf{w}$에 대한 함수이므로 $\mathbf{w}$로 미분을 하게 되면($\nabla E_{in}(\mathbf{w})$) 아래와 같이 도함수가 0이 되는 $\mathbf{w}$를 찾는 문제로 바뀌게 됩니다.&lt;/p&gt;

\[\frac{2}{N} \mathbf{X}^{\sf T} (\mathbf{X}\mathbf{w} - \mathbf{y}) = 0\]

&lt;p&gt;여기서 $\frac{2}{N}$은 의미없는 상수이므로 지워주게 되면,&lt;/p&gt;

\[\mathbf{X}^{\sf T} (\mathbf{X}\mathbf{w} - \mathbf{y}) = 0\]

&lt;p&gt;괄호 ()를 없애기 위해 분배법칙을 사용해 전개해줍니다.&lt;/p&gt;

\[\mathbf{X}^{\sf T}\mathbf{X}\mathbf{w} - \mathbf{X}^{\sf T}\mathbf{y} = 0\]

&lt;p&gt;$\mathbf{X}^{\sf T}\mathbf{y}$항을 오른쪽으로 넘겨주면,&lt;/p&gt;

\[\mathbf{X}^{\sf T}\mathbf{X}\mathbf{w} = \mathbf{X}^{\sf T}\mathbf{y}\]

&lt;p&gt;왼쪽 항에 $\mathbf{w}$만 남기기 위해서는 $\mathbf{X}^{\sf T}\mathbf{X}$의 역행렬을 양쪽에 곱해주어야 합니다. 일단 $\mathbf{X}$ 자체는 정사각행렬(Square Matrix)이라는 보장이 없지만, 임의의 행렬에 대해서 그 행렬의 전치행렬을 곱해주면 정사각행렬이 되므로 $\mathbf{X}^{\sf T}\mathbf{X}$는 정사각행렬입니다. (임의의 행렬이 크기가 $n \times m$ 이라 했을 때, 이 행렬의 전치행렬은 $m \times n$이 되고, ($m \times n$ 행렬)($n \times m$ 행렬) 연산을 해주면 $m \times m$ 행렬이 나오므로 $m$과 $n$에 관계없이 무조건 정사각행렬이 됩니다)&lt;/p&gt;

&lt;p&gt;이제 $\mathbf{X}^{\sf T}\mathbf{X}$ 행렬이 역행렬이 존재하는지 확인해야 하는데, 만약에 이 행렬의 역행렬이 존재하지 않는다면 위의 식 자체가 의미가 없어져 버리므로, 여기서는 있다고 가정하고 계산하겠습니다.&lt;/p&gt;

&lt;p&gt;양쪽 항에 $\mathbf{X}^{\sf T}\mathbf{X}$의 역행렬을 곱해주면,&lt;/p&gt;

\[\mathbf{w} = (\mathbf{X}^{\sf T}\mathbf{X})^{-1}\mathbf{X}^{\sf T}\mathbf{y}\]

&lt;p&gt;이 성립하게 됩니다. 여기서 $\mathbf{X}$와 관련된 항인 $(\mathbf{X}^{\sf T}\mathbf{X})^{-1}\mathbf{X}^{\sf T}$를 묶어서 $\mathbf{X}^{\dagger}$이라 한다면 아래처럼 깔끔하게 바뀌게 됩니다. $\dagger$ 기호는 대거(Dagger)라고 읽으시면 됩니다.&lt;/p&gt;

\[\mathbf{w} = \mathbf{X}^{\dagger}\mathbf{y}\]

&lt;p&gt;여기서 $\mathbf{X}^{\dagger}$를 $\mathbf{X}$의 &lt;span style=&quot;color:red&quot;&gt;Pseudo-Inverse&lt;/span&gt; (의사역행렬)라고 합니다. 이런 이름이 붙은 이유는, 이 행렬은 $\mathbf{X}$의 역행렬이 아님에도 불구하고 역행렬처럼 $\mathbf{X}^{\dagger}\mathbf{X} = I$가 나오는 성질을 가지기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이전 슬라이드에 수식이 너무 많아 당황하셨을 수도 있지만, 걱정하지 않으셔도 됩니다. Pseudo-Inverse의 전개 과정을 안다면 더욱 좋겠지만 모르셔도 전체적인 이론을 이해하는데는 큰 무리가 없습니다. “회귀 문제를 풀기 위해서는 Pseudo-Inverse를 계산해야 하는구나” 정도만 아시면 됩니다. 어차피 요즘엔 MATLAB 등과 같은 수학 관련 프로그램이 모두 계산해주기 때문입니다.&lt;/p&gt;

&lt;p&gt;이번에는 Pseudo-Inverse 행렬이 어떤 크기를 갖는지 알아봅시다. $\mathbf{X}$는 $d$차원의 입력이 $N$개 만큼 있으니 $N \times (d+1)$ 크기의 행렬이 될 것입니다. ($d$ 가 아니라 $d+1$ 인 이유는 Threshold를 나타내는 $x_0$가 포함되기 때문입니다) 그럼 $\mathbf{X}^{\sf T}$는 반대로 $(d+1) \times N$ 크기의 행렬이니 $\mathbf{X}^{\sf T} \mathbf{X}$ 를 계산한다면 $(d+1) \times (d+1)$ 크기의 행렬이 됨을 알 수 있습니다. 역행렬을 구한다고 해도 행렬의 크기는 변하지 않으니 $(\mathbf{X}^{\sf T} \mathbf{X})^{-1}$ 또한 $(d+1) \times (d+1)$ 크기의 행렬이 됩니다. $\mathbf{X}^{\sf T}$행렬은 $(d+1) \times N$ 크기라고 했으니 $(\mathbf{X}^{\sf T} \mathbf{X})^{-1} \mathbf{X}^{\sf T}$를 계산하면 최종적으로 Pseudo-Inverse $\mathbf{X}^{\dagger}$는 $(d+1) \times N$ 크기의 행렬이 된다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 선형 회귀를 하는 알고리즘을 정리해보면, 첫째로 입력값과 그 정답을 나타내는 값을 각각 $\mathbf{X}$와 $\mathbf{y}$ 행렬로 묶어주고, 둘째로 Pseudo-Inverse $\mathbf{X}^{\dagger}$를 계산한다음, 마지막으로 Pseudo-Inverse $\mathbf{X}^{\dagger}$와 $\mathbf{y}$를 곱해주면 $\mathbf{w}$를 알 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;이 과정을 보시면 한번만 계산하면 끝나기 때문에 Learning이 아니라고 생각하실 수도 있습니다. 다만 책의 저자이신 Abu-Mostafa 교수님께서는 꼭 PLA처럼 모든 데이터에 대해 하나하나 Weight Vector를 수정하는 것만이 Learning이 아니라고 합니다. 다시 말해서 어떻게 구하는지 그 과정은 별로 중요한 것이 아니라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선형 회귀는 결과 값이 실수로 나온다는 것을 알고 있습니다. 그런데 선형 분류에서 나오는 결과는 +1/-1인데, 이것도 실수니까 회귀와 연관을 지을 수 있지 않을까 하는 생각을 해봅니다. 선형 회귀를 조금 응용한다면 Weight Vector $\mathbf{w}$를 가지고 선형 분류를 할 수 있지 않을까라는 겁니다.&lt;/p&gt;

&lt;p&gt;안타깝게도 회귀와 분류는 그 목적이 다르기 때문에 그대로 사용할 수는 없습니다. 하지만 회귀에 사용한 데이터를 이용해 분류에 도움을 줄 수는 있습니다. 기존에 분류를 할 때 $\mathbf{w}$의 초기값을 무작위 값(혹은 0벡터)으로 정의하였지만, 회귀에서 사용했던 $\mathbf{w}$를 초기값으로 정하게 된다면 분류에 수렴하기까지 시간이 매우 단축되는 결과를 얻을 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선형 회귀에서 구한 Weight Vector $\mathbf{w}$를 초기값으로 설정해 선형 분류 문제의 그림으로 표현한 결과입니다. 완벽하지는 않지만, 정답과 매우 유사한 분류가 이루어졌음을 알 수 있습니다. 이 상태에서 시작해 PLA 등의 선형 분류 알고리즘을 수행한다면, 많은 단계를 거치지 않아도 최적의 $\mathbf{w}$을 쉽게 구할 수 있겠구나라는 것을 예상할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-transformation&quot;&gt;Nonlinear Transformation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 마지막으로 비선형(Non-Linear) 문제는 어떻게 해결하는지를 알아봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-22.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;선형 분류/회귀는 굉장히 편리하고 효과가 좋은 방법이지만, 안타깝게도 모든 문제가 선형으로 해결되지는 않습니다. 간단한 예제로 위 슬라이드의 왼쪽 그림과 같은 데이터의 경우, 어떻게 나누어도 직선으로는 분류할 수 없습니다. 오른쪽 그림처럼 원 모양으로 표현해야 제대로 나누어질텐데, 선형 방법으로는 저렇게 원 형태를 표현할 수 없는 것이 문제입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;잠시 카드 발급 문제로 돌아가서 “years in residence” 라는 요소를 생각해봅시다. 왜 그런지는 모르겠지만, 한 집에서 너무 적게 거주했거나(1년 미만) 너무 오래 거주한 경우(5년 초과) 부정적으로 평가한다고 합니다. 이런 경우 선형 모델을 통해 표현할 수 있을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-24.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제에 답을 하기 위해서는 &lt;strong&gt;“무엇”&lt;/strong&gt;에 대해 선형이냐 라는 것부터 생각해보아야 합니다. 입력 값이 선형이기 때문에 선형 모델이라고 생각하시는 분들이 있는데, 선형 모델이라고 이름이 붙은 이유는 “Weight”에 대해 선형이기 때문에 선형 모델이라고 부르는 겁니다. 그러니까, 다시 말해서 입력값 $\mathbf{x}$은 선형이든 아니든 크게 상관이 없다는 겁니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Machine Learning/3. Linear Models I/ML 03-25.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 입력값이 선형일 필요는 없다고 했으니, 입력 값을 한번 바꾸어봅시다. 임의의 함수 $\Phi$를 정의해서 입력값 $\mathbf{x}$ 제곱하는 연산을 수행하게 만들어 봅시다. 위 슬라이드의 왼쪽 그림은 20번 슬라이드에 나왔던 그 예제입니다. 그런데 모든 데이터에 대해 $\Phi$ 함수를 거친 결과값을 표현하니 오른쪽 그림처럼 변했습니다. 이 바뀐 데이터의 분포는 선형 분리까지 가능해 보인다는 것을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이런 방법을 통해 선형 분리가 도저히 안되는 데이터의 경우 임의의 함수를 정의하여 데이터를 변환시킴으로써 선형 분류가 가능하지만, 여기에는 한 가지 문제가 있습니다. 위의 그림처럼 간단한 예제의 경우에는 적당한 함수 $\Phi$를 쉽게 구했지만, 일반적인 상황에서는 적절한 $\Phi$를 찾는 것이 쉽지 않습니다. 그렇기에 또다른 방법이 필요하지만, 이 문제에 대한 해결 방법은 추후에 다시 언급됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 여기까지입니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="machine learning" /><summary type="html"></summary></entry></feed>